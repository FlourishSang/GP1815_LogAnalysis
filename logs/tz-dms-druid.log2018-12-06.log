[druid] 2018-12-06 00:48:51,642 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 0 time(s); maxRetries=45
   [druid] 2018-12-06 00:49:11,672 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 1 time(s); maxRetries=45
   [druid] 2018-12-06 00:49:31,690 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 2 time(s); maxRetries=45
   [druid] 2018-12-06 00:49:51,709 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 3 time(s); maxRetries=45
   [druid] 2018-12-06 00:50:11,763 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 4 time(s); maxRetries=45
   [druid] 2018-12-06 00:50:31,773 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 5 time(s); maxRetries=45
   [druid] 2018-12-06 00:50:51,775 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 6 time(s); maxRetries=45
   [druid] 2018-12-06 00:51:26,777 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 0 time(s); maxRetries=45
   [druid] 2018-12-06 00:51:46,779 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 1 time(s); maxRetries=45
   [druid] 2018-12-06 00:51:47,168 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 00:51:47,170 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 00:51:48,700 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 00:51:48,907 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 00:51:49,022 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 00:51:49,196 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2092470453_0001
   [druid] 2018-12-06 00:51:49,529 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 00:51:49,533 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 00:51:49,553 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:51:49,553 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 00:51:49,570 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 00:51:49,677 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 00:51:49,678 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2092470453_0001_m_000000_0
   [druid] 2018-12-06 00:51:49,722 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:51:49,733 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:51:50,266 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7e8454e1
   [druid] 2018-12-06 00:51:50,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 00:51:50,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 00:51:50,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 00:51:50,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 00:51:50,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 00:51:50,379 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 00:51:50,385 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 00:51:51,303 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,303 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,304 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,304 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,305 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,305 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,306 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,306 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,306 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:51:51,478 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 00:51:51,483 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 00:51:51,483 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 00:51:51,483 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 00:51:51,483 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 00:51:51,563 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 00:51:51,581 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2092470453_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:51:51,601 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 00:51:51,602 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2092470453_0001_m_000000_0' done.
   [druid] 2018-12-06 00:51:51,602 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2092470453_0001_m_000000_0
   [druid] 2018-12-06 00:51:51,602 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 00:51:51,608 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 00:51:51,608 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2092470453_0001_r_000000_0
   [druid] 2018-12-06 00:51:51,621 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:51:51,622 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:51:54,765 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2ee9e730
   [druid] 2018-12-06 00:51:54,771 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4922751e
   [druid] 2018-12-06 00:51:54,806 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 00:51:54,807 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2092470453_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 00:51:54,860 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2092470453_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 00:51:54,874 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local2092470453_0001_m_000000_0
   [druid] 2018-12-06 00:51:54,875 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 00:51:54,879 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 00:51:54,882 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:51:54,883 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 00:51:54,902 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:51:54,902 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:51:54,906 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 00:51:54,908 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 00:51:54,913 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 00:51:54,913 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:51:54,917 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:51:54,917 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:51:55,584 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 00:51:55,914 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2092470453_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:51:55,915 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 00:51:55,915 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2092470453_0001_r_000000_0' done.
   [druid] 2018-12-06 00:51:55,915 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2092470453_0001_r_000000_0
   [druid] 2018-12-06 00:51:55,915 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 00:51:55,922 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 00:57:03,021 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 00:57:03,025 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 00:57:04,085 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 00:57:04,235 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 00:57:04,282 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 00:57:04,392 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local344067518_0001
   [druid] 2018-12-06 00:57:04,638 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 00:57:04,641 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 00:57:04,651 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:57:04,651 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 00:57:04,663 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 00:57:04,713 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 00:57:04,715 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local344067518_0001_m_000000_0
   [druid] 2018-12-06 00:57:04,752 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:57:04,760 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:57:07,905 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c26ec2
   [druid] 2018-12-06 00:57:07,916 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 00:57:08,006 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 00:57:08,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 00:57:08,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 00:57:08,008 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 00:57:08,008 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 00:57:08,013 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 00:57:09,272 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,272 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,273 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,273 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,273 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,274 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,274 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,274 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,274 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:57:09,322 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 00:57:09,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 00:57:09,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 00:57:09,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 00:57:09,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 00:57:09,381 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 00:57:09,394 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local344067518_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:57:09,411 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 00:57:09,411 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local344067518_0001_m_000000_0' done.
   [druid] 2018-12-06 00:57:09,411 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local344067518_0001_m_000000_0
   [druid] 2018-12-06 00:57:09,412 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 00:57:09,415 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 00:57:09,417 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local344067518_0001_r_000000_0
   [druid] 2018-12-06 00:57:09,428 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:57:09,428 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:57:09,640 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@36502db4
   [druid] 2018-12-06 00:57:09,645 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ac88219
   [druid] 2018-12-06 00:57:09,675 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 00:57:09,678 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local344067518_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 00:57:09,729 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local344067518_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 00:57:09,736 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local344067518_0001_m_000000_0
   [druid] 2018-12-06 00:57:09,739 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 00:57:09,741 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 00:57:09,742 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:57:09,743 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 00:57:09,801 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:57:09,801 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:57:09,805 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 00:57:09,807 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 00:57:09,808 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 00:57:09,808 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:57:09,809 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:57:09,810 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:57:10,329 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 00:57:10,553 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local344067518_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:57:10,555 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 00:57:10,555 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local344067518_0001_r_000000_0' done.
   [druid] 2018-12-06 00:57:10,555 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local344067518_0001_r_000000_0
   [druid] 2018-12-06 00:57:10,556 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 00:57:10,565 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 00:58:39,297 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 00:58:39,300 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 00:58:40,418 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 00:58:40,557 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 00:58:40,614 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 00:58:40,748 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1327036741_0001
   [druid] 2018-12-06 00:58:40,960 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 00:58:40,965 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 00:58:40,973 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:58:40,973 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 00:58:40,986 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 00:58:41,039 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 00:58:41,040 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1327036741_0001_m_000000_0
   [druid] 2018-12-06 00:58:41,088 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:58:41,096 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:58:44,250 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@d25d404
   [druid] 2018-12-06 00:58:44,262 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 00:58:44,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 00:58:44,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 00:58:44,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 00:58:44,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 00:58:44,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 00:58:44,367 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 00:58:45,177 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,178 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,178 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,179 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,179 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,179 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,180 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,180 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,180 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 00:58:45,228 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 00:58:45,230 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 00:58:45,230 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 00:58:45,230 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 00:58:45,230 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 00:58:45,292 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 00:58:45,303 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1327036741_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:58:45,318 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 00:58:45,319 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1327036741_0001_m_000000_0' done.
   [druid] 2018-12-06 00:58:45,319 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1327036741_0001_m_000000_0
   [druid] 2018-12-06 00:58:45,320 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 00:58:45,323 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 00:58:45,324 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1327036741_0001_r_000000_0
   [druid] 2018-12-06 00:58:45,339 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 00:58:45,339 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 00:58:48,479 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7fb6de8d
   [druid] 2018-12-06 00:58:48,483 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3ab180e6
   [druid] 2018-12-06 00:58:48,512 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 00:58:48,515 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1327036741_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 00:58:48,564 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1327036741_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 00:58:48,571 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1327036741_0001_m_000000_0
   [druid] 2018-12-06 00:58:48,573 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 00:58:48,575 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 00:58:48,576 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:58:48,576 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 00:58:48,589 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:58:48,589 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:58:48,595 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 00:58:48,597 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 00:58:48,598 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 00:58:48,598 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 00:58:48,600 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 00:58:48,600 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 00:58:49,059 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 00:58:49,333 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1327036741_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 00:58:49,333 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 00:58:49,334 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1327036741_0001_r_000000_0' done.
   [druid] 2018-12-06 00:58:49,334 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1327036741_0001_r_000000_0
   [druid] 2018-12-06 00:58:49,334 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 00:58:49,344 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:04:02,773 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:04:02,775 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:04:03,808 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:04:03,935 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:04:03,984 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:04:04,101 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1347737079_0001
   [druid] 2018-12-06 01:04:04,314 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:04:04,325 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:04:04,333 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:04,333 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:04:04,342 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:04:04,391 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:04:04,396 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1347737079_0001_m_000000_0
   [druid] 2018-12-06 01:04:04,434 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:04,442 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:04:04,661 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6865c6c8
   [druid] 2018-12-06 01:04:04,673 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:04:04,762 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:04:04,762 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:04:04,762 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:04:04,762 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:04:04,762 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:04:04,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:04:05,563 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,563 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,564 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,564 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,564 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,564 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,565 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,565 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,565 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:05,616 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:04:05,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:04:05,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:04:05,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:04:05,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:04:05,751 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:04:05,760 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1347737079_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:04:05,774 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:04:05,774 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1347737079_0001_m_000000_0' done.
   [druid] 2018-12-06 01:04:05,774 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1347737079_0001_m_000000_0
   [druid] 2018-12-06 01:04:05,775 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:04:05,777 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:04:05,780 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1347737079_0001_r_000000_0
   [druid] 2018-12-06 01:04:05,793 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:05,793 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:04:05,930 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@590699b7
   [druid] 2018-12-06 01:04:05,935 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@662a3480
   [druid] 2018-12-06 01:04:05,968 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:04:05,972 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1347737079_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:04:06,019 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1347737079_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:04:06,026 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1347737079_0001_m_000000_0
   [druid] 2018-12-06 01:04:06,029 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:04:06,032 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:04:06,033 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:04:06,033 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:04:06,050 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:04:06,051 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:04:06,056 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:04:06,058 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:04:06,059 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:04:06,059 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:04:06,060 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:04:06,061 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:04:06,578 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:04:06,807 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1347737079_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:04:06,808 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:04:06,808 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1347737079_0001_r_000000_0' done.
   [druid] 2018-12-06 01:04:06,809 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1347737079_0001_r_000000_0
   [druid] 2018-12-06 01:04:06,809 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:04:06,817 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:04:28,111 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:04:28,113 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:04:29,298 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:04:29,406 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:04:29,468 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:04:29,622 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local584425276_0001
   [druid] 2018-12-06 01:04:29,869 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:04:29,876 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:04:29,892 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:29,892 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:04:29,908 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:04:29,976 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:04:29,981 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local584425276_0001_m_000000_0
   [druid] 2018-12-06 01:04:30,038 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:30,047 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:04:30,193 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3b534e42
   [druid] 2018-12-06 01:04:30,208 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:04:30,303 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:04:30,304 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:04:30,304 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:04:30,304 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:04:30,304 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:04:30,311 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:04:31,103 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,104 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,104 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,104 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,105 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,105 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,105 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,105 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,106 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:04:31,163 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:04:31,168 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:04:31,168 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:04:31,169 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:04:31,169 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:04:31,282 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:04:31,308 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local584425276_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:04:31,328 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:04:31,328 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local584425276_0001_m_000000_0' done.
   [druid] 2018-12-06 01:04:31,328 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local584425276_0001_m_000000_0
   [druid] 2018-12-06 01:04:31,329 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:04:31,333 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:04:31,334 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local584425276_0001_r_000000_0
   [druid] 2018-12-06 01:04:31,348 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:04:31,348 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:04:31,480 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2207fc3d
   [druid] 2018-12-06 01:04:31,487 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@71def2e
   [druid] 2018-12-06 01:04:31,517 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:04:31,523 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local584425276_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:04:31,591 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local584425276_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:04:31,602 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local584425276_0001_m_000000_0
   [druid] 2018-12-06 01:04:31,606 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:04:31,608 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:04:31,609 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:04:31,610 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:04:31,629 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:04:31,629 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:04:31,633 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:04:31,635 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:04:31,636 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:04:31,637 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:04:31,638 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:04:31,639 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:04:32,141 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:04:32,408 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local584425276_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:04:32,410 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:04:32,410 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local584425276_0001_r_000000_0' done.
   [druid] 2018-12-06 01:04:32,410 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local584425276_0001_r_000000_0
   [druid] 2018-12-06 01:04:32,410 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:04:32,420 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:10:35,852 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:10:35,853 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:10:36,904 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:10:37,038 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:10:37,091 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:10:37,219 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1031146433_0001
   [druid] 2018-12-06 01:10:37,424 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:10:37,427 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:10:37,434 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:10:37,435 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:10:37,445 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:10:37,507 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:10:37,509 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1031146433_0001_m_000000_0
   [druid] 2018-12-06 01:10:37,544 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:10:37,549 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:10:37,683 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@887bc32
   [druid] 2018-12-06 01:10:37,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:10:37,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:10:37,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:10:37,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:10:37,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:10:37,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:10:37,792 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:10:38,537 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,537 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,538 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,538 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,538 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,538 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,538 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,539 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,539 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:10:38,594 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:10:38,597 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:10:38,597 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:10:38,597 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:10:38,597 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:10:38,718 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:10:38,728 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1031146433_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:10:38,743 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:10:38,743 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1031146433_0001_m_000000_0' done.
   [druid] 2018-12-06 01:10:38,743 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1031146433_0001_m_000000_0
   [druid] 2018-12-06 01:10:38,743 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:10:38,745 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:10:38,749 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1031146433_0001_r_000000_0
   [druid] 2018-12-06 01:10:38,762 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:10:38,762 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:10:38,898 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@59a515f2
   [druid] 2018-12-06 01:10:38,901 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4e37bf7d
   [druid] 2018-12-06 01:10:38,926 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:10:38,933 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1031146433_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:10:38,981 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1031146433_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:10:38,985 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1031146433_0001_m_000000_0
   [druid] 2018-12-06 01:10:38,990 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:10:38,993 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:10:38,994 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:10:38,995 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:10:39,013 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:10:39,014 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:10:39,019 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:10:39,021 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:10:39,022 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:10:39,022 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:10:39,024 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:10:39,024 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:10:39,517 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:10:39,752 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1031146433_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:10:39,753 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:10:39,753 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1031146433_0001_r_000000_0' done.
   [druid] 2018-12-06 01:10:39,754 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1031146433_0001_r_000000_0
   [druid] 2018-12-06 01:10:39,754 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:10:39,762 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:15:46,444 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:15:46,447 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:15:47,562 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:15:47,688 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:15:47,736 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:15:47,870 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local253787745_0001
   [druid] 2018-12-06 01:15:48,076 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:15:48,080 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:15:48,089 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:15:48,089 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:15:48,102 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:15:48,154 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:15:48,156 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local253787745_0001_m_000000_0
   [druid] 2018-12-06 01:15:48,198 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:15:48,205 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:15:48,338 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1b209d47
   [druid] 2018-12-06 01:15:48,350 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:15:48,440 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:15:48,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:15:48,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:15:48,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:15:48,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:15:48,447 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:15:49,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,300 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,300 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,300 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,300 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,301 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,301 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,301 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,301 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:15:49,344 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:15:49,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:15:49,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:15:49,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:15:49,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:15:49,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:15:49,472 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local253787745_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:15:49,485 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:15:49,486 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local253787745_0001_m_000000_0' done.
   [druid] 2018-12-06 01:15:49,486 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local253787745_0001_m_000000_0
   [druid] 2018-12-06 01:15:49,486 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:15:49,489 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:15:49,491 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local253787745_0001_r_000000_0
   [druid] 2018-12-06 01:15:49,502 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:15:49,502 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:15:49,636 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@48d5600d
   [druid] 2018-12-06 01:15:49,640 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79a3783c
   [druid] 2018-12-06 01:15:49,670 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:15:49,673 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local253787745_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:15:49,720 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local253787745_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:15:49,725 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local253787745_0001_m_000000_0
   [druid] 2018-12-06 01:15:49,731 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:15:49,733 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:15:49,734 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:15:49,735 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:15:49,753 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:15:49,753 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:15:49,758 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:15:49,759 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:15:49,760 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:15:49,760 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:15:49,762 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:15:49,762 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:15:50,244 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:15:50,487 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local253787745_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:15:50,488 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:15:50,488 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local253787745_0001_r_000000_0' done.
   [druid] 2018-12-06 01:15:50,488 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local253787745_0001_r_000000_0
   [druid] 2018-12-06 01:15:50,489 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:15:50,499 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:18:46,978 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:18:46,982 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:18:48,084 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:18:48,204 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:18:48,251 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:18:48,366 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local771033852_0001
   [druid] 2018-12-06 01:18:48,572 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:18:48,575 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:18:48,584 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:18:48,584 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:18:48,592 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:18:48,652 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:18:48,652 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local771033852_0001_m_000000_0
   [druid] 2018-12-06 01:18:48,692 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:18:48,699 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:18:48,834 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6357b4
   [druid] 2018-12-06 01:18:48,843 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:18:48,940 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:18:48,940 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:18:48,940 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:18:48,940 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:18:48,940 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:18:48,946 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:18:49,719 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,719 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,720 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,720 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,720 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,720 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,721 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,721 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,721 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:18:49,771 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:18:49,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:18:49,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:18:49,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:18:49,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:18:49,896 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:18:49,908 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local771033852_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:18:49,922 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:18:49,923 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local771033852_0001_m_000000_0' done.
   [druid] 2018-12-06 01:18:49,923 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local771033852_0001_m_000000_0
   [druid] 2018-12-06 01:18:49,923 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:18:49,924 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:18:49,927 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local771033852_0001_r_000000_0
   [druid] 2018-12-06 01:18:49,941 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:18:49,941 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:18:50,079 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7f5df6f0
   [druid] 2018-12-06 01:18:50,083 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7f728d3b
   [druid] 2018-12-06 01:18:50,113 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:18:50,116 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local771033852_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:18:50,164 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local771033852_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:18:50,169 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local771033852_0001_m_000000_0
   [druid] 2018-12-06 01:18:50,174 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:18:50,176 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:18:50,178 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:18:50,178 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:18:50,194 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:18:50,195 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:18:50,278 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:18:50,279 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:18:50,279 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:18:50,279 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:18:50,283 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:18:50,283 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:18:50,738 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:18:50,960 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local771033852_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:18:50,961 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:18:50,962 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local771033852_0001_r_000000_0' done.
   [druid] 2018-12-06 01:18:50,962 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local771033852_0001_r_000000_0
   [druid] 2018-12-06 01:18:50,963 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:18:50,972 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:20:42,402 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:20:42,402 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:20:43,423 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:20:43,554 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:20:43,611 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:20:43,725 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local595136958_0001
   [druid] 2018-12-06 01:20:43,940 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:20:43,943 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:20:43,951 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:20:43,951 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:20:43,962 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:20:44,006 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:20:44,010 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local595136958_0001_m_000000_0
   [druid] 2018-12-06 01:20:44,048 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:20:44,051 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:20:44,185 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b7f33dc
   [druid] 2018-12-06 01:20:44,196 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:20:44,287 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:20:44,287 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:20:44,287 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:20:44,288 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:20:44,288 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:20:44,293 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:20:45,090 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,091 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,091 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,091 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,092 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,093 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,093 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,093 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,093 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:20:45,143 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:20:45,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:20:45,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:20:45,147 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:20:45,147 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:20:45,168 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:20:45,277 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local595136958_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:20:45,291 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:20:45,291 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local595136958_0001_m_000000_0' done.
   [druid] 2018-12-06 01:20:45,291 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local595136958_0001_m_000000_0
   [druid] 2018-12-06 01:20:45,292 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:20:45,295 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:20:45,295 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local595136958_0001_r_000000_0
   [druid] 2018-12-06 01:20:45,308 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:20:45,308 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:20:45,453 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@49749c3a
   [druid] 2018-12-06 01:20:45,457 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@52c21710
   [druid] 2018-12-06 01:20:45,475 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:20:45,478 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local595136958_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:20:45,545 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local595136958_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:20:45,554 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local595136958_0001_m_000000_0
   [druid] 2018-12-06 01:20:45,558 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:20:45,561 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:20:45,563 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:20:45,563 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:20:45,588 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:20:45,588 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:20:45,593 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:20:45,595 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:20:45,595 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:20:45,596 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:20:45,597 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:20:45,598 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:20:46,074 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:20:46,377 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local595136958_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:20:46,378 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:20:46,378 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local595136958_0001_r_000000_0' done.
   [druid] 2018-12-06 01:20:46,379 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local595136958_0001_r_000000_0
   [druid] 2018-12-06 01:20:46,379 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:20:46,387 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:21:27,169 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 01:21:27,173 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 01:21:28,208 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 01:21:28,337 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 01:21:28,386 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 01:21:28,508 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local704269177_0001
   [druid] 2018-12-06 01:21:28,718 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 01:21:28,723 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local704269177_0001
   [druid] 2018-12-06 01:21:28,726 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 01:21:28,735 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:21:28,735 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 01:21:28,746 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 01:21:28,799 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 01:21:28,801 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local704269177_0001_m_000000_0
   [druid] 2018-12-06 01:21:28,838 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:21:28,844 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:21:28,977 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@20fda614
   [druid] 2018-12-06 01:21:28,990 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 01:21:29,080 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 01:21:29,080 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 01:21:29,080 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 01:21:29,080 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 01:21:29,080 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 01:21:29,087 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 01:21:29,731 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local704269177_0001 running in uber mode : false
   [druid] 2018-12-06 01:21:29,738 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 01:21:29,844 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,844 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,844 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,845 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,845 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,845 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,845 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,845 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,846 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 01:21:29,897 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 01:21:29,901 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 01:21:29,901 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 01:21:29,901 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 01:21:29,901 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 01:21:30,019 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 01:21:30,033 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local704269177_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:21:30,052 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 01:21:30,052 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local704269177_0001_m_000000_0' done.
   [druid] 2018-12-06 01:21:30,052 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local704269177_0001_m_000000_0
   [druid] 2018-12-06 01:21:30,053 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 01:21:30,054 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 01:21:30,057 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local704269177_0001_r_000000_0
   [druid] 2018-12-06 01:21:30,069 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 01:21:30,069 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 01:21:30,213 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@25ba997c
   [druid] 2018-12-06 01:21:30,217 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@524fdd85
   [druid] 2018-12-06 01:21:30,250 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 01:21:30,255 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local704269177_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 01:21:30,309 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local704269177_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 01:21:30,318 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local704269177_0001_m_000000_0
   [druid] 2018-12-06 01:21:30,321 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 01:21:30,324 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 01:21:30,325 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:21:30,325 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 01:21:30,346 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:21:30,346 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:21:30,351 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 01:21:30,353 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 01:21:30,354 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 01:21:30,354 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 01:21:30,355 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 01:21:30,356 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 01:21:30,742 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 01:21:30,824 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 01:21:31,083 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local704269177_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 01:21:31,084 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 01:21:31,084 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local704269177_0001_r_000000_0' done.
   [druid] 2018-12-06 01:21:31,084 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local704269177_0001_r_000000_0
   [druid] 2018-12-06 01:21:31,084 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 01:21:31,093 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 01:21:31,745 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 01:21:31,746 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local704269177_0001 completed successfully
   [druid] 2018-12-06 01:21:31,793 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=17296
		FILE: Number of bytes written=625692
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=8472
		Reduce input records=62
		Reduce output records=6
		Spilled Records=124
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=7
		Total committed heap usage (bytes)=596115456
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:00:30,773 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 0 time(s); maxRetries=45
   [druid] 2018-12-06 12:00:51,082 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 1 time(s); maxRetries=45
   [druid] 2018-12-06 12:01:11,083 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 2 time(s); maxRetries=45
   [druid] 2018-12-06 12:01:31,084 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 3 time(s); maxRetries=45
   [druid] 2018-12-06 12:01:31,165 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:01:31,166 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:01:31,863 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:01:32,028 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:01:32,056 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:01:32,137 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1670484662_0001
   [druid] 2018-12-06 12:01:32,260 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:01:32,260 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1670484662_0001
   [druid] 2018-12-06 12:01:32,262 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:01:32,268 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:01:32,268 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:01:32,273 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:01:32,304 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:01:32,306 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1670484662_0001_m_000000_0
   [druid] 2018-12-06 12:01:32,326 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:01:32,329 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:01:33,241 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@56ae32c0
   [druid] 2018-12-06 12:01:33,263 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1670484662_0001 running in uber mode : false
   [druid] 2018-12-06 12:01:33,264 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 12:01:33,286 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 12:01:33,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:01:33,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:01:33,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:01:33,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:01:33,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:01:33,403 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:01:34,028 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,029 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,030 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:01:34,292 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:01:34,294 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:01:34,294 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:01:34,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 404578; bufvoid = 104857600
   [druid] 2018-12-06 12:01:34,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 12:01:34,384 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:01:34,391 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1670484662_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:01:34,401 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:01:34,401 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1670484662_0001_m_000000_0' done.
   [druid] 2018-12-06 12:01:34,401 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1670484662_0001_m_000000_0
   [druid] 2018-12-06 12:01:34,402 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:01:34,404 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:01:34,405 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1670484662_0001_r_000000_0
   [druid] 2018-12-06 12:01:34,413 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:01:34,413 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:01:34,502 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3914eda7
   [druid] 2018-12-06 12:01:34,524 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2060caf4
   [druid] 2018-12-06 12:01:34,551 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:01:34,553 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1670484662_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:01:34,587 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1670484662_0001_m_000000_0 decomp: 410904 len: 410908 to MEMORY
   [druid] 2018-12-06 12:01:34,592 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 410904 bytes from map-output for attempt_local1670484662_0001_m_000000_0
   [druid] 2018-12-06 12:01:34,594 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 410904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->410904
   [druid] 2018-12-06 12:01:34,599 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:01:34,600 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:01:34,601 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:01:34,610 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:01:34,610 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:01:34,628 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 410904 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:01:34,629 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 410908 bytes from disk
   [druid] 2018-12-06 12:01:34,629 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:01:34,629 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:01:34,630 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:01:34,630 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:01:35,099 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:01:35,137 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1670484662_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:01:35,138 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:01:35,138 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1670484662_0001_r_000000_0' done.
   [druid] 2018-12-06 12:01:35,138 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1670484662_0001_r_000000_0
   [druid] 2018-12-06 12:01:35,138 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:01:35,145 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:01:35,267 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:01:35,267 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1670484662_0001 completed successfully
   [druid] 2018-12-06 12:01:35,285 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=822168
		FILE: Number of bytes written=1836068
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=404578
		Map output materialized bytes=410908
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=410908
		Reduce input records=3162
		Reduce output records=0
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=517996544
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:02:17,005 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:02:17,006 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:02:17,714 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:02:17,799 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:02:17,836 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:02:17,921 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1079402760_0001
   [druid] 2018-12-06 12:02:18,077 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:02:18,078 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1079402760_0001
   [druid] 2018-12-06 12:02:18,079 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:02:18,089 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:02:18,089 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:02:18,097 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:02:18,137 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:02:18,139 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1079402760_0001_m_000000_0
   [druid] 2018-12-06 12:02:18,166 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:02:18,176 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:02:18,251 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@705d0ab2
   [druid] 2018-12-06 12:02:18,257 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 12:02:18,306 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:02:18,306 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:02:18,306 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:02:18,306 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:02:18,306 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:02:18,310 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:02:18,761 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,762 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:02:18,877 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:02:18,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:02:18,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:02:18,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 404578; bufvoid = 104857600
   [druid] 2018-12-06 12:02:18,880 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 12:02:19,022 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:02:19,033 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1079402760_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:02:19,044 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:02:19,044 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1079402760_0001_m_000000_0' done.
   [druid] 2018-12-06 12:02:19,044 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1079402760_0001_m_000000_0
   [druid] 2018-12-06 12:02:19,044 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:02:19,048 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:02:19,048 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1079402760_0001_r_000000_0
   [druid] 2018-12-06 12:02:19,056 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:02:19,056 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:02:19,083 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1079402760_0001 running in uber mode : false
   [druid] 2018-12-06 12:02:19,084 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:02:19,140 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@15be8f15
   [druid] 2018-12-06 12:02:19,143 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6aae0625
   [druid] 2018-12-06 12:02:19,154 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:02:19,156 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1079402760_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:02:19,186 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1079402760_0001_m_000000_0 decomp: 410904 len: 410908 to MEMORY
   [druid] 2018-12-06 12:02:19,191 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 410904 bytes from map-output for attempt_local1079402760_0001_m_000000_0
   [druid] 2018-12-06 12:02:19,192 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 410904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->410904
   [druid] 2018-12-06 12:02:19,194 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:02:19,194 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:02:19,194 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:02:19,205 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:02:19,205 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:02:19,216 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 410904 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:02:19,217 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 410908 bytes from disk
   [druid] 2018-12-06 12:02:19,218 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:02:19,218 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:02:19,220 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:02:19,220 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:02:19,495 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:02:19,530 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1079402760_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:02:19,531 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:02:19,531 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1079402760_0001_r_000000_0' done.
   [druid] 2018-12-06 12:02:19,531 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1079402760_0001_r_000000_0
   [druid] 2018-12-06 12:02:19,531 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:02:19,536 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:02:20,088 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:02:20,088 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1079402760_0001 completed successfully
   [druid] 2018-12-06 12:02:20,101 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=822168
		FILE: Number of bytes written=1836068
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=404578
		Map output materialized bytes=410908
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=410908
		Reduce input records=3162
		Reduce output records=0
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:03:04,265 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:03:04,266 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:03:04,971 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:03:05,054 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:03:05,083 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:03:05,151 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local917523183_0001
   [druid] 2018-12-06 12:03:05,293 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:03:05,294 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local917523183_0001
   [druid] 2018-12-06 12:03:05,295 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:03:05,299 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:03:05,300 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:03:05,305 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:03:05,338 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:03:05,339 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local917523183_0001_m_000000_0
   [druid] 2018-12-06 12:03:05,358 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:03:05,362 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:03:05,435 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ac1d3
   [druid] 2018-12-06 12:03:05,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 12:03:05,488 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:03:05,488 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:03:05,488 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:03:05,488 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:03:05,488 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:03:05,491 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:03:05,951 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:05,952 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:03:06,061 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:03:06,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:03:06,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:03:06,064 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 404578; bufvoid = 104857600
   [druid] 2018-12-06 12:03:06,064 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 12:03:06,213 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:03:06,221 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local917523183_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:03:06,230 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:03:06,230 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local917523183_0001_m_000000_0' done.
   [druid] 2018-12-06 12:03:06,230 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local917523183_0001_m_000000_0
   [druid] 2018-12-06 12:03:06,230 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:03:06,233 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:03:06,233 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local917523183_0001_r_000000_0
   [druid] 2018-12-06 12:03:06,240 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:03:06,240 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:03:06,295 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local917523183_0001 running in uber mode : false
   [druid] 2018-12-06 12:03:06,296 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:03:06,320 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@19e53092
   [druid] 2018-12-06 12:03:06,323 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2ad43e3e
   [druid] 2018-12-06 12:03:06,334 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:03:06,336 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local917523183_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:03:06,366 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local917523183_0001_m_000000_0 decomp: 410904 len: 410908 to MEMORY
   [druid] 2018-12-06 12:03:06,371 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 410904 bytes from map-output for attempt_local917523183_0001_m_000000_0
   [druid] 2018-12-06 12:03:06,372 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 410904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->410904
   [druid] 2018-12-06 12:03:06,373 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:03:06,374 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:03:06,374 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:03:06,384 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:03:06,384 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:03:06,395 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 410904 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:03:06,396 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 410908 bytes from disk
   [druid] 2018-12-06 12:03:06,396 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:03:06,397 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:03:06,398 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:03:06,398 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:03:06,661 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:03:06,692 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local917523183_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:03:06,693 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:03:06,693 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local917523183_0001_r_000000_0' done.
   [druid] 2018-12-06 12:03:06,693 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local917523183_0001_r_000000_0
   [druid] 2018-12-06 12:03:06,694 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:03:06,699 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:03:07,299 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:03:07,299 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local917523183_0001 completed successfully
   [druid] 2018-12-06 12:03:07,311 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=822168
		FILE: Number of bytes written=1832988
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=404578
		Map output materialized bytes=410908
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=410908
		Reduce input records=3162
		Reduce output records=0
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=2
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:04:48,155 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:04:48,157 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:04:49,106 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:04:49,221 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:04:49,274 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:04:49,400 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local789899978_0001
   [druid] 2018-12-06 12:04:49,463 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:04:49,464 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:04:49,874 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:04:49,876 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local789899978_0001
   [druid] 2018-12-06 12:04:49,878 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:04:49,885 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:49,886 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:04:49,894 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:04:49,936 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:04:49,939 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local789899978_0001_m_000000_0
   [druid] 2018-12-06 12:04:49,976 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:49,982 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:04:50,059 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9accd04
   [druid] 2018-12-06 12:04:50,070 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 12:04:50,129 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:04:50,129 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:04:50,129 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:04:50,129 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:04:50,129 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:04:50,134 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:04:50,218 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:04:50,290 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:04:50,320 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:04:50,389 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local972424144_0001
   [druid] 2018-12-06 12:04:50,545 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:04:50,547 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local972424144_0001
   [druid] 2018-12-06 12:04:50,548 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:04:50,554 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:50,554 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:04:50,561 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:04:50,602 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:04:50,604 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local972424144_0001_m_000000_0
   [druid] 2018-12-06 12:04:50,631 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:50,635 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:04:50,696 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,696 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,697 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,698 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,698 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,698 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,698 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:50,714 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77f76243
   [druid] 2018-12-06 12:04:50,723 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 12:04:50,724 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:04:50,727 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:04:50,727 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:04:50,727 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 13180; bufvoid = 104857600
   [druid] 2018-12-06 12:04:50,727 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600
   [druid] 2018-12-06 12:04:50,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:04:50,747 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local789899978_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:04:50,761 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:04:50,761 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local789899978_0001_m_000000_0' done.
   [druid] 2018-12-06 12:04:50,762 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local789899978_0001_m_000000_0
   [druid] 2018-12-06 12:04:50,762 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:04:50,765 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:04:50,765 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local789899978_0001_r_000000_0
   [druid] 2018-12-06 12:04:50,773 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:50,774 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:04:50,779 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:04:50,779 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:04:50,779 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:04:50,779 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:04:50,779 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:04:50,783 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:04:50,858 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5d4e12d0
   [druid] 2018-12-06 12:04:50,862 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4301761c
   [druid] 2018-12-06 12:04:50,880 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local789899978_0001 running in uber mode : false
   [druid] 2018-12-06 12:04:50,881 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:04:50,882 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:04:50,884 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local789899978_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:04:50,922 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local789899978_0001_m_000000_0 decomp: 13386 len: 13390 to MEMORY
   [druid] 2018-12-06 12:04:50,929 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 13386 bytes from map-output for attempt_local789899978_0001_m_000000_0
   [druid] 2018-12-06 12:04:50,932 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 13386, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->13386
   [druid] 2018-12-06 12:04:50,933 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:04:50,934 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:04:50,935 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:04:50,946 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:04:50,946 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:04:50,949 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 13386 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:04:50,950 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 13390 bytes from disk
   [druid] 2018-12-06 12:04:50,950 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:04:50,951 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:04:50,951 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:04:50,952 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:04:51,272 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:04:51,284 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local789899978_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:04:51,285 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:04:51,285 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local789899978_0001_r_000000_0' done.
   [druid] 2018-12-06 12:04:51,285 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local789899978_0001_r_000000_0
   [druid] 2018-12-06 12:04:51,285 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:04:51,289 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,290 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,290 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,291 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,291 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,291 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,292 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,292 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:04:51,292 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,292 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,292 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,292 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,293 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,293 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,293 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:04:51,312 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:04:51,315 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:04:51,315 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:04:51,315 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 13180; bufvoid = 104857600
   [druid] 2018-12-06 12:04:51,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600
   [druid] 2018-12-06 12:04:51,329 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:04:51,335 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local972424144_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:04:51,343 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:04:51,343 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local972424144_0001_m_000000_0' done.
   [druid] 2018-12-06 12:04:51,343 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local972424144_0001_m_000000_0
   [druid] 2018-12-06 12:04:51,343 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:04:51,346 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:04:51,346 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local972424144_0001_r_000000_0
   [druid] 2018-12-06 12:04:51,351 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:04:51,351 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:04:51,419 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@56477af1
   [druid] 2018-12-06 12:04:51,422 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@53868354
   [druid] 2018-12-06 12:04:51,441 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:04:51,444 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local972424144_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:04:51,480 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local972424144_0001_m_000000_0 decomp: 13386 len: 13390 to MEMORY
   [druid] 2018-12-06 12:04:51,487 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 13386 bytes from map-output for attempt_local972424144_0001_m_000000_0
   [druid] 2018-12-06 12:04:51,489 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 13386, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->13386
   [druid] 2018-12-06 12:04:51,490 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:04:51,491 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:04:51,492 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:04:51,504 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:04:51,504 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:04:51,507 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 13386 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:04:51,508 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 13390 bytes from disk
   [druid] 2018-12-06 12:04:51,509 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:04:51,509 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:04:51,510 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:04:51,510 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:04:51,549 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local972424144_0001 running in uber mode : false
   [druid] 2018-12-06 12:04:51,550 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:04:51,777 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:04:51,785 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local972424144_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:04:51,787 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:04:51,787 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local972424144_0001_r_000000_0' done.
   [druid] 2018-12-06 12:04:51,787 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local972424144_0001_r_000000_0
   [druid] 2018-12-06 12:04:51,787 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:04:51,792 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:04:51,884 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:04:51,884 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local789899978_0001 completed successfully
   [druid] 2018-12-06 12:04:51,901 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=27130
		FILE: Number of bytes written=640432
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=102
		Map output bytes=13180
		Map output materialized bytes=13390
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=13390
		Reduce input records=102
		Reduce output records=0
		Spilled Records=204
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:04:52,551 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:04:52,551 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local972424144_0001 completed successfully
   [druid] 2018-12-06 12:04:52,564 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=27130
		FILE: Number of bytes written=640432
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=102
		Map output bytes=13180
		Map output materialized bytes=13390
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=13390
		Reduce input records=102
		Reduce output records=0
		Spilled Records=204
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=597688320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:23:07,178 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:23:07,180 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:23:07,784 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:23:07,864 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:23:07,896 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:23:07,962 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1162731448_0001
   [druid] 2018-12-06 12:23:08,104 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:23:08,105 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1162731448_0001
   [druid] 2018-12-06 12:23:08,107 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:23:08,113 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:08,113 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:23:08,118 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:23:08,170 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:23:08,172 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1162731448_0001_m_000000_0
   [druid] 2018-12-06 12:23:08,205 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:08,210 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:23:08,291 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@37eb1d34
   [druid] 2018-12-06 12:23:08,298 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 12:23:08,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:23:08,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:23:08,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:23:08,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:23:08,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:23:08,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,808 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,809 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:08,830 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:23:08,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:23:08,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:23:08,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 13180; bufvoid = 104857600
   [druid] 2018-12-06 12:23:08,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600
   [druid] 2018-12-06 12:23:08,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:23:08,850 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1162731448_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:23:08,858 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:23:08,858 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1162731448_0001_m_000000_0' done.
   [druid] 2018-12-06 12:23:08,858 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1162731448_0001_m_000000_0
   [druid] 2018-12-06 12:23:08,859 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:23:08,860 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:23:08,861 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1162731448_0001_r_000000_0
   [druid] 2018-12-06 12:23:08,866 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:08,866 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:23:09,074 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4f1537d5
   [druid] 2018-12-06 12:23:09,078 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5e53ed95
   [druid] 2018-12-06 12:23:09,096 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:23:09,098 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1162731448_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:23:09,108 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1162731448_0001 running in uber mode : false
   [druid] 2018-12-06 12:23:09,109 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:23:09,132 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1162731448_0001_m_000000_0 decomp: 13386 len: 13390 to MEMORY
   [druid] 2018-12-06 12:23:09,138 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 13386 bytes from map-output for attempt_local1162731448_0001_m_000000_0
   [druid] 2018-12-06 12:23:09,140 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 13386, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->13386
   [druid] 2018-12-06 12:23:09,141 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:23:09,142 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:23:09,142 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:23:09,201 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:23:09,201 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:23:09,312 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 13386 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:23:09,313 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 13390 bytes from disk
   [druid] 2018-12-06 12:23:09,314 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:23:09,314 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:23:09,315 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 12:23:09,316 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:23:09,584 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:23:09,594 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1162731448_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:23:09,594 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:23:09,594 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1162731448_0001_r_000000_0' done.
   [druid] 2018-12-06 12:23:09,594 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1162731448_0001_r_000000_0
   [druid] 2018-12-06 12:23:09,595 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:23:09,600 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:23:10,110 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:23:10,110 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1162731448_0001 completed successfully
   [druid] 2018-12-06 12:23:10,121 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=27130
		FILE: Number of bytes written=643512
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=102
		Map output bytes=13180
		Map output materialized bytes=13390
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=13390
		Reduce input records=102
		Reduce output records=0
		Spilled Records=204
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=597688320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 12:23:38,394 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 12:23:38,396 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 12:23:39,021 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 12:23:39,106 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 12:23:39,138 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 12:23:39,210 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1089573312_0001
   [druid] 2018-12-06 12:23:39,341 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 12:23:39,342 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1089573312_0001
   [druid] 2018-12-06 12:23:39,343 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 12:23:39,348 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:39,348 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 12:23:39,353 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 12:23:39,382 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 12:23:39,383 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1089573312_0001_m_000000_0
   [druid] 2018-12-06 12:23:39,403 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:39,407 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:23:39,475 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-06 12:23:39,484 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 12:23:39,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 12:23:39,535 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 12:23:39,535 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 12:23:39,535 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 12:23:39,535 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 12:23:39,537 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 12:23:40,018 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,018 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,019 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,020 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 12:23:40,139 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 12:23:40,142 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 12:23:40,142 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 12:23:40,142 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 404578; bufvoid = 104857600
   [druid] 2018-12-06 12:23:40,142 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 12:23:40,189 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 12:23:40,250 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1089573312_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:23:40,259 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 12:23:40,259 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1089573312_0001_m_000000_0' done.
   [druid] 2018-12-06 12:23:40,260 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1089573312_0001_m_000000_0
   [druid] 2018-12-06 12:23:40,260 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 12:23:40,262 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 12:23:40,263 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1089573312_0001_r_000000_0
   [druid] 2018-12-06 12:23:40,270 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 12:23:40,270 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 12:23:40,345 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1089573312_0001 running in uber mode : false
   [druid] 2018-12-06 12:23:40,346 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 12:23:40,352 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@464929c0
   [druid] 2018-12-06 12:23:40,354 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41fce2a4
   [druid] 2018-12-06 12:23:40,367 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 12:23:40,369 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1089573312_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 12:23:40,400 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1089573312_0001_m_000000_0 decomp: 410904 len: 410908 to MEMORY
   [druid] 2018-12-06 12:23:40,405 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 410904 bytes from map-output for attempt_local1089573312_0001_m_000000_0
   [druid] 2018-12-06 12:23:40,407 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 410904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->410904
   [druid] 2018-12-06 12:23:40,408 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 12:23:40,408 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:23:40,409 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 12:23:40,508 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:23:40,509 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:23:40,549 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 410904 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 12:23:40,550 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 410908 bytes from disk
   [druid] 2018-12-06 12:23:40,551 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 12:23:40,551 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 12:23:40,553 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 12:23:40,553 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 12:23:40,837 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 12:23:40,875 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1089573312_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 12:23:40,876 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 12:23:40,876 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1089573312_0001_r_000000_0' done.
   [druid] 2018-12-06 12:23:40,876 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1089573312_0001_r_000000_0
   [druid] 2018-12-06 12:23:40,876 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 12:23:40,883 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 12:23:41,350 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 12:23:41,350 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1089573312_0001 completed successfully
   [druid] 2018-12-06 12:23:41,363 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=822168
		FILE: Number of bytes written=1836068
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=404578
		Map output materialized bytes=410908
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=410908
		Reduce input records=3162
		Reduce output records=0
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 13:09:10,418 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:09:10,419 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:09:11,159 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:09:11,244 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:09:11,272 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:09:11,341 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local391860610_0001
   [druid] 2018-12-06 13:09:11,483 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:09:11,484 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local391860610_0001
   [druid] 2018-12-06 13:09:11,485 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:09:11,491 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:09:11,491 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:09:11,497 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:09:11,531 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:09:11,532 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local391860610_0001_m_000000_0
   [druid] 2018-12-06 13:09:11,554 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:09:11,559 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:09:11,717 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2820564e
   [druid] 2018-12-06 13:09:11,725 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 13:09:11,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:09:11,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:09:11,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:09:11,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:09:11,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:09:11,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:09:12,245 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,245 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,245 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:09:12,369 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 13:09:12,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 13:09:12,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 13:09:12,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 404578; bufvoid = 104857600
   [druid] 2018-12-06 13:09:12,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 13:09:12,487 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local391860610_0001 running in uber mode : false
   [druid] 2018-12-06 13:09:12,489 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 13:09:12,583 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 13:09:12,605 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local391860610_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:09:12,614 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 13:09:12,614 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local391860610_0001_m_000000_0' done.
   [druid] 2018-12-06 13:09:12,614 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local391860610_0001_m_000000_0
   [druid] 2018-12-06 13:09:12,615 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 13:09:12,616 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 13:09:12,616 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local391860610_0001_r_000000_0
   [druid] 2018-12-06 13:09:12,623 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:09:12,623 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:09:12,698 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@34465c5f
   [druid] 2018-12-06 13:09:12,701 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@ef66f8
   [druid] 2018-12-06 13:09:12,714 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 13:09:12,716 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local391860610_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 13:09:12,748 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local391860610_0001_m_000000_0 decomp: 410904 len: 410908 to MEMORY
   [druid] 2018-12-06 13:09:12,753 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 410904 bytes from map-output for attempt_local391860610_0001_m_000000_0
   [druid] 2018-12-06 13:09:12,755 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 410904, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->410904
   [druid] 2018-12-06 13:09:12,758 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 13:09:12,759 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:09:12,759 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 13:09:12,790 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:09:12,791 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 13:09:12,812 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 410904 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 13:09:12,813 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 410908 bytes from disk
   [druid] 2018-12-06 13:09:12,814 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 13:09:12,814 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:09:12,815 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 410834 bytes
   [druid] 2018-12-06 13:09:12,815 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:09:13,093 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 13:09:13,122 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local391860610_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:09:13,123 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 13:09:13,123 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local391860610_0001_r_000000_0' done.
   [druid] 2018-12-06 13:09:13,124 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local391860610_0001_r_000000_0
   [druid] 2018-12-06 13:09:13,124 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 13:09:13,130 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 13:09:13,492 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 13:09:13,492 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local391860610_0001 completed successfully
   [druid] 2018-12-06 13:09:13,504 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=822168
		FILE: Number of bytes written=1832988
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=404578
		Map output materialized bytes=410908
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=410908
		Reduce input records=3162
		Reduce output records=0
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 13:10:52,381 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:10:52,382 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:10:52,980 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:10:53,051 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:10:53,078 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:10:53,141 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1884580350_0001
   [druid] 2018-12-06 13:10:53,263 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:10:53,264 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1884580350_0001
   [druid] 2018-12-06 13:10:53,265 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:10:53,270 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:10:53,270 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:10:53,276 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:10:53,305 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:10:53,307 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1884580350_0001_m_000000_0
   [druid] 2018-12-06 13:10:53,329 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:10:53,334 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:10:53,407 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@159ccee1
   [druid] 2018-12-06 13:10:53,414 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 13:10:53,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:10:53,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:10:53,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:10:53,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:10:53,462 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:10:53,464 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:10:53,914 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,915 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,915 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,915 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,916 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,917 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,917 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:10:53,934 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 13:10:53,937 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 13:10:53,937 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 13:10:53,937 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 13180; bufvoid = 104857600
   [druid] 2018-12-06 13:10:53,937 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600
   [druid] 2018-12-06 13:10:54,100 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 13:10:54,142 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1884580350_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:10:54,149 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 13:10:54,149 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1884580350_0001_m_000000_0' done.
   [druid] 2018-12-06 13:10:54,149 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1884580350_0001_m_000000_0
   [druid] 2018-12-06 13:10:54,149 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 13:10:54,151 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 13:10:54,152 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1884580350_0001_r_000000_0
   [druid] 2018-12-06 13:10:54,159 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:10:54,159 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:10:54,239 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@56477af1
   [druid] 2018-12-06 13:10:54,242 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@53868354
   [druid] 2018-12-06 13:10:54,257 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 13:10:54,259 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1884580350_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 13:10:54,266 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1884580350_0001 running in uber mode : false
   [druid] 2018-12-06 13:10:54,267 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 13:10:54,288 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1884580350_0001_m_000000_0 decomp: 13386 len: 13390 to MEMORY
   [druid] 2018-12-06 13:10:54,292 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 13386 bytes from map-output for attempt_local1884580350_0001_m_000000_0
   [druid] 2018-12-06 13:10:54,294 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 13386, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->13386
   [druid] 2018-12-06 13:10:54,295 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 13:10:54,296 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:10:54,296 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 13:10:54,355 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:10:54,355 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 13:10:54,386 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 13386 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 13:10:54,388 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 13390 bytes from disk
   [druid] 2018-12-06 13:10:54,388 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 13:10:54,389 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:10:54,390 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 13316 bytes
   [druid] 2018-12-06 13:10:54,391 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:10:54,658 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 13:10:54,666 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1884580350_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:10:54,666 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 13:10:54,667 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1884580350_0001_r_000000_0' done.
   [druid] 2018-12-06 13:10:54,667 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1884580350_0001_r_000000_0
   [druid] 2018-12-06 13:10:54,667 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 13:10:54,674 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 13:10:55,268 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 13:10:55,268 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1884580350_0001 completed successfully
   [druid] 2018-12-06 13:10:55,280 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=27130
		FILE: Number of bytes written=643516
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=102
		Map output bytes=13180
		Map output materialized bytes=13390
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=13390
		Reduce input records=102
		Reduce output records=0
		Spilled Records=204
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=600834048
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 13:12:13,789 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:12:13,791 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:12:14,352 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:12:14,424 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:12:14,453 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:12:14,517 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1687084608_0001
   [druid] 2018-12-06 13:12:14,630 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:12:14,631 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1687084608_0001
   [druid] 2018-12-06 13:12:14,632 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:12:14,636 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:12:14,636 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:12:14,641 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:12:14,670 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:12:14,672 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1687084608_0001_m_000000_0
   [druid] 2018-12-06 13:12:14,691 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:12:14,695 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:12:14,762 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@558ef1fc
   [druid] 2018-12-06 13:12:14,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/12/2018-11-12.log:0+14734
   [druid] 2018-12-06 13:12:14,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:12:14,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:12:14,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:12:14,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:12:14,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:12:14,816 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:12:15,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 13:12:15,282 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 13:12:15,283 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 13:12:15,283 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1687084608_0001
   java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 13
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 13
	at com.phone.analysis.mapreduce.SessionAnalysis.SessionMapper.map(SessionMapper.java:55)
	at com.phone.analysis.mapreduce.SessionAnalysis.SessionMapper.map(SessionMapper.java:25)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 13:12:15,634 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1687084608_0001 running in uber mode : false
   [druid] 2018-12-06 13:12:15,635 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 13:12:15,636 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1687084608_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 13:12:15,641 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-06 13:13:12,185 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:13:12,186 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:13:12,753 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:13:12,821 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:13:12,849 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:13:12,913 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1216056412_0001
   [druid] 2018-12-06 13:13:13,093 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:13:13,094 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1216056412_0001
   [druid] 2018-12-06 13:13:13,095 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:13:13,101 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:13:13,101 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:13:13,107 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:13:13,133 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:13:13,135 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1216056412_0001_m_000000_0
   [druid] 2018-12-06 13:13:13,155 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:13:13,159 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:13:13,230 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77802544
   [druid] 2018-12-06 13:13:13,236 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 13:13:13,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:13:13,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:13:13,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:13:13,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:13:13,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:13:13,286 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:13:13,690 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,690 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,690 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,690 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,691 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,692 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,692 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 13:13:13,727 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 13:13:13,728 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 13:13:13,728 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 13:13:13,728 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 13:13:13,729 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 13:13:13,857 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 13:13:13,874 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1216056412_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:13:13,881 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 13:13:13,881 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1216056412_0001_m_000000_0' done.
   [druid] 2018-12-06 13:13:13,881 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1216056412_0001_m_000000_0
   [druid] 2018-12-06 13:13:13,881 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 13:13:13,883 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 13:13:13,883 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1216056412_0001_r_000000_0
   [druid] 2018-12-06 13:13:13,888 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:13:13,888 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:13:13,966 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@54c53ed7
   [druid] 2018-12-06 13:13:13,968 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50e2e381
   [druid] 2018-12-06 13:13:13,984 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 13:13:13,986 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1216056412_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 13:13:14,017 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1216056412_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 13:13:14,021 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local1216056412_0001_m_000000_0
   [druid] 2018-12-06 13:13:14,023 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 13:13:14,024 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 13:13:14,024 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:13:14,024 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 13:13:14,046 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:13:14,046 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 13:13:14,050 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 13:13:14,051 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 13:13:14,052 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 13:13:14,052 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 13:13:14,052 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 13:13:14,053 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 13:13:14,096 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1216056412_0001 running in uber mode : false
   [druid] 2018-12-06 13:13:14,097 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 13:13:14,298 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 13:13:14,311 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1216056412_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 13:13:14,312 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 13:13:14,312 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1216056412_0001_r_000000_0' done.
   [druid] 2018-12-06 13:13:14,312 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1216056412_0001_r_000000_0
   [druid] 2018-12-06 13:13:14,312 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 13:13:14,316 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 13:13:15,098 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 13:13:15,098 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1216056412_0001 completed successfully
   [druid] 2018-12-06 13:13:15,107 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=738650
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=0
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=598736896
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 13:56:55,270 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:56:55,271 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:56:55,912 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:56:55,979 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:56:56,022 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:56:56,106 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2006777949_0001
   [druid] 2018-12-06 13:56:56,270 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:56:56,271 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2006777949_0001
   [druid] 2018-12-06 13:56:56,274 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:56:56,282 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:56:56,282 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:56:56,290 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:56:56,333 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:56:56,336 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2006777949_0001_m_000000_0
   [druid] 2018-12-06 13:56:56,374 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:56:56,382 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:56:56,541 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b17fe72
   [druid] 2018-12-06 13:56:56,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 13:56:56,678 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:56:56,679 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:56:56,679 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:56:56,679 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:56:56,679 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:56:56,683 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:56:57,867 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2006777949_0001 running in uber mode : false
   [druid] 2018-12-06 13:58:54,003 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:58:54,005 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:58:54,807 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:58:54,866 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:58:54,911 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:58:54,982 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local121537319_0001
   [druid] 2018-12-06 13:58:55,119 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:58:55,120 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local121537319_0001
   [druid] 2018-12-06 13:58:55,122 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:58:55,127 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:58:55,127 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:58:55,134 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:58:55,170 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:58:55,172 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local121537319_0001_m_000000_0
   [druid] 2018-12-06 13:58:55,201 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:58:55,207 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:58:55,279 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4dfb8227
   [druid] 2018-12-06 13:58:55,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 13:58:55,346 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:58:55,346 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:58:55,346 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:58:55,346 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:58:55,346 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:58:55,352 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 13:59:17,279 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 13:59:17,280 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 13:59:17,909 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 13:59:17,966 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 13:59:18,116 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 13:59:18,192 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local149057759_0001
   [druid] 2018-12-06 13:59:18,330 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 13:59:18,331 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local149057759_0001
   [druid] 2018-12-06 13:59:18,333 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 13:59:18,340 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:59:18,340 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 13:59:18,348 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 13:59:18,387 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 13:59:18,390 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local149057759_0001_m_000000_0
   [druid] 2018-12-06 13:59:18,419 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 13:59:18,426 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 13:59:18,499 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@352f544b
   [druid] 2018-12-06 13:59:18,514 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 13:59:18,566 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 13:59:18,567 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 13:59:18,567 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 13:59:18,567 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 13:59:18,567 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 13:59:18,571 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:01:21,887 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,888 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local149057759_0001 running in uber mode : false
   [druid] 2018-12-06 14:01:21,888 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,889 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,889 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,889 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,889 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,890 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,890 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,890 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:21,890 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 14:01:34,884 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 14:01:34,885 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 14:01:35,601 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 14:01:35,673 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 14:01:35,789 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 14:01:35,904 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1778314829_0001
   [druid] 2018-12-06 14:01:36,095 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 14:01:36,096 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1778314829_0001
   [druid] 2018-12-06 14:01:36,097 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 14:01:36,101 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:01:36,101 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 14:01:36,106 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 14:01:36,140 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 14:01:36,141 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1778314829_0001_m_000000_0
   [druid] 2018-12-06 14:01:36,163 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:01:36,168 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:01:36,242 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@862d6c6
   [druid] 2018-12-06 14:01:36,249 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 14:01:36,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 14:01:36,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 14:01:36,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 14:01:36,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 14:01:36,295 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 14:01:36,299 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:01:36,743 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,744 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,744 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,744 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,745 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,745 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,745 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,745 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:36,745 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 14:01:37,097 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1778314829_0001 running in uber mode : false
   [druid] 2018-12-06 14:01:37,098 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 14:01:42,172 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 14:01:43,102 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 28% reduce 0%
   [druid] 2018-12-06 14:01:45,173 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 14:01:46,103 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 45% reduce 0%
   [druid] 2018-12-06 14:01:48,174 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 14:01:48,975 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 14:01:48,977 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 14:01:48,977 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 14:01:48,977 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1006; bufvoid = 104857600
   [druid] 2018-12-06 14:01:48,977 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 14:01:49,104 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 63% reduce 0%
   [druid] 2018-12-06 14:01:49,114 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 14:01:49,144 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1778314829_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:01:49,145 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 14:01:49,146 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1778314829_0001_m_000000_0' done.
   [druid] 2018-12-06 14:01:49,146 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1778314829_0001_m_000000_0
   [druid] 2018-12-06 14:01:49,146 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 14:01:49,148 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 14:01:49,148 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1778314829_0001_r_000000_0
   [druid] 2018-12-06 14:01:49,154 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:01:49,154 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:01:49,236 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3566f813
   [druid] 2018-12-06 14:01:49,240 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@29015e0c
   [druid] 2018-12-06 14:01:49,252 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 14:01:49,267 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1778314829_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 14:01:49,294 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1778314829_0001_m_000000_0 decomp: 1024 len: 1028 to MEMORY
   [druid] 2018-12-06 14:01:49,298 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1024 bytes from map-output for attempt_local1778314829_0001_m_000000_0
   [druid] 2018-12-06 14:01:49,299 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1024, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1024
   [druid] 2018-12-06 14:01:49,300 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 14:01:49,301 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:01:49,301 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 14:01:49,357 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:01:49,357 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 14:01:49,388 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1024 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 14:01:49,389 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1028 bytes from disk
   [druid] 2018-12-06 14:01:49,390 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 14:01:49,390 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:01:49,390 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 14:01:49,391 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:01:49,404 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 14:01:49,546 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1778314829_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:01:49,547 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 14:01:49,547 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1778314829_0001_r_000000_0' done.
   [druid] 2018-12-06 14:01:49,547 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1778314829_0001_r_000000_0
   [druid] 2018-12-06 14:01:49,547 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 14:01:49,552 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 14:01:50,104 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 14:01:50,104 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1778314829_0001 completed successfully
   [druid] 2018-12-06 14:01:50,116 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=2408
		FILE: Number of bytes written=606392
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1006
		Map output materialized bytes=1028
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=1028
		Reduce input records=8
		Reduce output records=3
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=123
		Total committed heap usage (bytes)=786432000
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 14:02:23,423 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 14:02:23,424 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 14:02:24,028 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 14:02:24,104 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 14:02:24,131 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 14:02:24,211 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1809097879_0001
   [druid] 2018-12-06 14:02:24,334 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 14:02:24,335 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1809097879_0001
   [druid] 2018-12-06 14:02:24,336 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 14:02:24,341 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:02:24,341 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 14:02:24,346 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 14:02:24,375 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 14:02:24,376 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1809097879_0001_m_000000_0
   [druid] 2018-12-06 14:02:24,395 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:02:24,399 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:02:24,465 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@38522b56
   [druid] 2018-12-06 14:02:24,472 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 14:02:24,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 14:02:24,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 14:02:24,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 14:02:24,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 14:02:24,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 14:02:24,519 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:02:24,934 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,934 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,935 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,936 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:02:24,972 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 14:02:24,974 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 14:02:24,974 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 14:02:24,974 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 14:02:24,974 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 14:02:25,120 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 14:02:25,145 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1809097879_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:02:25,153 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 14:02:25,153 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1809097879_0001_m_000000_0' done.
   [druid] 2018-12-06 14:02:25,153 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1809097879_0001_m_000000_0
   [druid] 2018-12-06 14:02:25,153 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 14:02:25,155 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 14:02:25,156 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1809097879_0001_r_000000_0
   [druid] 2018-12-06 14:02:25,162 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:02:25,163 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:02:25,245 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@67b2fdfd
   [druid] 2018-12-06 14:02:25,247 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3d9e3a8f
   [druid] 2018-12-06 14:02:25,265 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 14:02:25,268 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1809097879_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 14:02:25,298 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1809097879_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 14:02:25,302 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local1809097879_0001_m_000000_0
   [druid] 2018-12-06 14:02:25,304 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 14:02:25,305 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 14:02:25,305 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:02:25,306 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 14:02:25,329 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:02:25,329 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:02:25,335 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 14:02:25,335 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 14:02:25,336 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1809097879_0001 running in uber mode : false
   [druid] 2018-12-06 14:02:25,336 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 14:02:25,336 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:02:25,337 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:02:25,337 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 14:02:25,337 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:02:25,593 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 14:02:25,604 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1809097879_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:02:25,605 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 14:02:25,605 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1809097879_0001_r_000000_0' done.
   [druid] 2018-12-06 14:02:25,605 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1809097879_0001_r_000000_0
   [druid] 2018-12-06 14:02:25,606 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 14:02:25,610 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 14:02:26,337 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 14:02:26,337 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1809097879_0001 completed successfully
   [druid] 2018-12-06 14:02:26,347 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=738650
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=0
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=598212608
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 14:06:25,234 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 14:06:25,235 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 14:06:25,926 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 14:06:26,013 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 14:06:26,040 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 14:06:26,113 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1632204604_0001
   [druid] 2018-12-06 14:06:26,233 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 14:06:26,234 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1632204604_0001
   [druid] 2018-12-06 14:06:26,234 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 14:06:26,239 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:06:26,239 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 14:06:26,244 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 14:06:26,275 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 14:06:26,277 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1632204604_0001_m_000000_0
   [druid] 2018-12-06 14:06:26,298 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:06:26,301 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:06:26,374 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@862d6c6
   [druid] 2018-12-06 14:06:26,381 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 14:06:26,426 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 14:06:26,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 14:06:26,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 14:06:26,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 14:06:26,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 14:06:26,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:06:26,886 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,887 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,888 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:06:26,926 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 14:06:26,928 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 14:06:26,928 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 14:06:26,928 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 14:06:26,928 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 14:06:27,003 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 14:06:27,030 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1632204604_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:06:27,038 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 14:06:27,038 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1632204604_0001_m_000000_0' done.
   [druid] 2018-12-06 14:06:27,038 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1632204604_0001_m_000000_0
   [druid] 2018-12-06 14:06:27,038 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 14:06:27,040 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 14:06:27,040 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1632204604_0001_r_000000_0
   [druid] 2018-12-06 14:06:27,046 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:06:27,046 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:06:27,126 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2567ad3f
   [druid] 2018-12-06 14:06:27,130 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5c747725
   [druid] 2018-12-06 14:06:27,146 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 14:06:27,147 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1632204604_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 14:06:27,179 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1632204604_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 14:06:27,183 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local1632204604_0001_m_000000_0
   [druid] 2018-12-06 14:06:27,185 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 14:06:27,187 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 14:06:27,187 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:06:27,187 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 14:06:27,235 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1632204604_0001 running in uber mode : false
   [druid] 2018-12-06 14:06:27,236 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 14:06:27,243 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:06:27,243 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:06:27,247 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 14:06:27,249 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 14:06:27,249 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 14:06:27,249 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:06:27,250 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:06:27,251 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:06:27,521 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 14:06:27,532 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1632204604_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:06:27,533 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 14:06:27,533 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1632204604_0001_r_000000_0' done.
   [druid] 2018-12-06 14:06:27,533 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1632204604_0001_r_000000_0
   [druid] 2018-12-06 14:06:27,533 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 14:06:27,538 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 14:06:28,237 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 14:06:28,237 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1632204604_0001 completed successfully
   [druid] 2018-12-06 14:06:28,247 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=738650
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=0
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=604504064
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 14:07:31,250 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 14:07:31,251 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 14:07:31,835 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 14:07:31,910 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 14:07:31,938 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 14:07:32,020 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local861191921_0001
   [druid] 2018-12-06 14:07:32,149 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 14:07:32,150 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local861191921_0001
   [druid] 2018-12-06 14:07:32,151 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 14:07:32,155 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:07:32,155 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 14:07:32,161 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 14:07:32,195 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 14:07:32,196 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local861191921_0001_m_000000_0
   [druid] 2018-12-06 14:07:32,218 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:07:32,222 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:07:32,307 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-06 14:07:32,313 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 14:07:32,358 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 14:07:32,359 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 14:07:32,359 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 14:07:32,359 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 14:07:32,359 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 14:07:32,362 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:07:32,844 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,844 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,844 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,844 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,845 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,846 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,846 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:07:32,886 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 14:07:32,889 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 14:07:32,889 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 14:07:32,889 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 14:07:32,889 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 14:07:32,939 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 14:07:32,948 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local861191921_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:07:32,959 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 14:07:32,959 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local861191921_0001_m_000000_0' done.
   [druid] 2018-12-06 14:07:32,959 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local861191921_0001_m_000000_0
   [druid] 2018-12-06 14:07:32,959 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 14:07:32,962 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 14:07:32,962 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local861191921_0001_r_000000_0
   [druid] 2018-12-06 14:07:32,971 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:07:32,971 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:07:33,045 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@464929c0
   [druid] 2018-12-06 14:07:33,048 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41fce2a4
   [druid] 2018-12-06 14:07:33,066 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 14:07:33,069 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local861191921_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 14:07:33,098 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local861191921_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 14:07:33,102 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local861191921_0001_m_000000_0
   [druid] 2018-12-06 14:07:33,104 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 14:07:33,105 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 14:07:33,106 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:07:33,106 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 14:07:33,127 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:07:33,128 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:07:33,132 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 14:07:33,133 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 14:07:33,133 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 14:07:33,133 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:07:33,134 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:07:33,135 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:07:33,152 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local861191921_0001 running in uber mode : false
   [druid] 2018-12-06 14:07:33,153 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 14:07:33,400 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 14:07:33,414 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local861191921_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:07:33,415 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 14:07:33,415 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local861191921_0001_r_000000_0' done.
   [druid] 2018-12-06 14:07:33,415 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local861191921_0001_r_000000_0
   [druid] 2018-12-06 14:07:33,415 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 14:07:33,421 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 14:07:34,154 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 14:07:34,154 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local861191921_0001 completed successfully
   [druid] 2018-12-06 14:07:34,163 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=735570
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=0
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=598212608
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 14:10:38,728 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 14:10:38,730 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 14:10:39,317 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 14:10:39,401 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 14:10:39,429 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 14:10:39,494 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local448629080_0001
   [druid] 2018-12-06 14:10:39,617 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 14:10:39,619 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local448629080_0001
   [druid] 2018-12-06 14:10:39,620 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 14:10:39,625 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:10:39,626 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 14:10:39,631 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 14:10:39,662 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 14:10:39,664 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local448629080_0001_m_000000_0
   [druid] 2018-12-06 14:10:39,683 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:10:39,687 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:10:39,761 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77802544
   [druid] 2018-12-06 14:10:39,767 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 14:10:39,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 14:10:39,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 14:10:39,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 14:10:39,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 14:10:39,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 14:10:39,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,246 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,247 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 14:10:40,282 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 14:10:40,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 14:10:40,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 14:10:40,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 14:10:40,283 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 14:10:40,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 14:10:40,395 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local448629080_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:10:40,402 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 14:10:40,402 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local448629080_0001_m_000000_0' done.
   [druid] 2018-12-06 14:10:40,403 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local448629080_0001_m_000000_0
   [druid] 2018-12-06 14:10:40,403 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 14:10:40,405 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 14:10:40,405 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local448629080_0001_r_000000_0
   [druid] 2018-12-06 14:10:40,410 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 14:10:40,410 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 14:10:40,484 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@54c53ed7
   [druid] 2018-12-06 14:10:40,488 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@50e2e381
   [druid] 2018-12-06 14:10:40,504 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 14:10:40,507 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local448629080_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 14:10:40,537 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local448629080_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 14:10:40,541 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local448629080_0001_m_000000_0
   [druid] 2018-12-06 14:10:40,542 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 14:10:40,543 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 14:10:40,544 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:10:40,544 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 14:10:40,555 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:10:40,555 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:10:40,559 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 14:10:40,559 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 14:10:40,560 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 14:10:40,560 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 14:10:40,561 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 14:10:40,561 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 14:10:40,622 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local448629080_0001 running in uber mode : false
   [druid] 2018-12-06 14:10:40,623 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 14:10:40,826 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 14:10:40,923 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local448629080_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 14:10:40,924 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 14:10:40,924 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local448629080_0001_r_000000_0' done.
   [druid] 2018-12-06 14:10:40,924 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local448629080_0001_r_000000_0
   [druid] 2018-12-06 14:10:40,924 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 14:10:40,928 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 14:10:41,624 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 14:10:41,624 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local448629080_0001 completed successfully
   [druid] 2018-12-06 14:10:41,635 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=735570
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=3
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=597688320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:09:49,822 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:09:49,824 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:09:50,489 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:09:50,560 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:09:50,590 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:09:50,658 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1577780347_0001
   [druid] 2018-12-06 15:09:50,774 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:09:50,775 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1577780347_0001
   [druid] 2018-12-06 15:09:50,776 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:09:50,781 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:09:50,782 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:09:50,787 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:09:50,819 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:09:50,820 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1577780347_0001_m_000000_0
   [druid] 2018-12-06 15:09:50,838 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:09:50,841 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:09:51,104 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@20fda614
   [druid] 2018-12-06 15:09:51,110 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:09:51,156 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:09:51,156 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:09:51,156 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:09:51,156 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:09:51,156 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:09:51,160 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:09:51,683 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,684 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,684 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,684 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,684 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,685 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,685 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,685 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,686 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:09:51,778 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1577780347_0001 running in uber mode : false
   [druid] 2018-12-06 15:09:51,780 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:09:56,848 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:09:57,783 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 25% reduce 0%
   [druid] 2018-12-06 15:09:59,850 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:10:00,784 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 42% reduce 0%
   [druid] 2018-12-06 15:10:02,851 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:10:03,785 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 59% reduce 0%
   [druid] 2018-12-06 15:10:04,349 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:10:04,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:10:04,366 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1577780347_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:10:04,367 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:10:04,368 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1577780347_0001_m_000000_0' done.
   [druid] 2018-12-06 15:10:04,368 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1577780347_0001_m_000000_0
   [druid] 2018-12-06 15:10:04,368 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:10:04,370 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:10:04,370 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1577780347_0001_r_000000_0
   [druid] 2018-12-06 15:10:04,375 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:10:04,375 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:10:04,446 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@61d62d11
   [druid] 2018-12-06 15:10:04,449 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@626368dd
   [druid] 2018-12-06 15:10:04,461 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:10:04,463 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1577780347_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:10:04,487 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1577780347_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 15:10:04,491 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local1577780347_0001_m_000000_0
   [druid] 2018-12-06 15:10:04,492 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 15:10:04,494 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:10:04,494 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:10:04,495 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:10:04,504 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:10:04,504 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:10:04,506 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:10:04,506 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 15:10:04,507 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:10:04,507 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:10:04,509 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:10:04,509 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:10:04,522 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:10:04,526 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1577780347_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:10:04,526 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:10:04,526 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1577780347_0001_r_000000_0' done.
   [druid] 2018-12-06 15:10:04,527 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1577780347_0001_r_000000_0
   [druid] 2018-12-06 15:10:04,527 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:10:04,531 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:10:04,786 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:10:04,786 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1577780347_0001 completed successfully
   [druid] 2018-12-06 15:10:04,797 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=603382
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=117
		Total committed heap usage (bytes)=754974720
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:15:59,831 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:15:59,833 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:16:00,470 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:16:00,550 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:16:00,581 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:16:00,656 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local868481164_0001
   [druid] 2018-12-06 15:16:00,787 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:16:00,788 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local868481164_0001
   [druid] 2018-12-06 15:16:00,789 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:16:00,794 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:16:00,794 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:16:00,800 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:16:00,835 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:16:00,837 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local868481164_0001_m_000000_0
   [druid] 2018-12-06 15:16:00,859 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:16:00,863 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:16:00,931 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@37d62c8f
   [druid] 2018-12-06 15:16:00,938 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:16:00,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:16:00,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:16:00,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:16:00,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:16:00,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:16:00,986 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:16:01,392 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,392 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,393 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,393 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,394 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,394 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,394 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,395 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,395 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:16:01,791 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local868481164_0001 running in uber mode : false
   [druid] 2018-12-06 15:16:01,792 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:16:06,872 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:16:07,796 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 26% reduce 0%
   [druid] 2018-12-06 15:16:09,873 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:16:10,797 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 43% reduce 0%
   [druid] 2018-12-06 15:16:12,874 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:16:13,797 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 59% reduce 0%
   [druid] 2018-12-06 15:16:14,408 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:16:14,410 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:16:14,517 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local868481164_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:16:14,519 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:16:14,520 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local868481164_0001_m_000000_0' done.
   [druid] 2018-12-06 15:16:14,520 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local868481164_0001_m_000000_0
   [druid] 2018-12-06 15:16:14,520 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:16:14,522 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:16:14,523 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local868481164_0001_r_000000_0
   [druid] 2018-12-06 15:16:14,531 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:16:14,531 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:16:14,604 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@61af44db
   [druid] 2018-12-06 15:16:14,607 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3e255deb
   [druid] 2018-12-06 15:16:14,621 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:16:14,629 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local868481164_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:16:14,661 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local868481164_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 15:16:14,665 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local868481164_0001_m_000000_0
   [druid] 2018-12-06 15:16:14,666 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 15:16:14,668 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:16:14,668 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:16:14,668 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:16:14,679 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:16:14,679 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:16:14,681 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:16:14,682 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 15:16:14,683 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:16:14,683 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:16:14,685 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:16:14,685 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:16:14,703 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:16:14,707 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local868481164_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:16:14,708 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:16:14,708 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local868481164_0001_r_000000_0' done.
   [druid] 2018-12-06 15:16:14,708 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local868481164_0001_r_000000_0
   [druid] 2018-12-06 15:16:14,709 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:16:14,714 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:16:14,798 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:16:14,798 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local868481164_0001 completed successfully
   [druid] 2018-12-06 15:16:14,809 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=600302
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=128
		Total committed heap usage (bytes)=785383424
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:30:56,843 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:30:56,844 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:30:57,488 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:30:57,565 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:30:57,592 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:30:57,666 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1497175754_0001
   [druid] 2018-12-06 15:30:57,791 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:30:57,792 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1497175754_0001
   [druid] 2018-12-06 15:30:57,792 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:30:57,796 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:30:57,797 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:30:57,802 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:30:57,833 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:30:57,834 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1497175754_0001_m_000000_0
   [druid] 2018-12-06 15:30:57,854 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:30:57,858 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:30:57,930 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@61ee405d
   [druid] 2018-12-06 15:30:57,937 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:30:57,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:30:57,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:30:57,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:30:57,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:30:57,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:30:57,989 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:30:58,716 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,716 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,717 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,717 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,717 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,718 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,718 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,719 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,719 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:30:58,793 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1497175754_0001 running in uber mode : false
   [druid] 2018-12-06 15:30:58,794 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:31:03,864 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:31:04,799 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 26% reduce 0%
   [druid] 2018-12-06 15:31:06,865 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:31:07,800 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 43% reduce 0%
   [druid] 2018-12-06 15:31:09,866 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:31:10,801 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 59% reduce 0%
   [druid] 2018-12-06 15:31:11,418 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:31:11,420 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:31:11,449 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1497175754_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:31:11,450 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:31:11,451 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1497175754_0001_m_000000_0' done.
   [druid] 2018-12-06 15:31:11,451 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1497175754_0001_m_000000_0
   [druid] 2018-12-06 15:31:11,451 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:31:11,454 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:31:11,454 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1497175754_0001_r_000000_0
   [druid] 2018-12-06 15:31:11,460 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:31:11,460 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:31:11,533 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@15a3dce2
   [druid] 2018-12-06 15:31:11,536 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6876a434
   [druid] 2018-12-06 15:31:11,548 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:31:11,550 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1497175754_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:31:11,573 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1497175754_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 15:31:11,577 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local1497175754_0001_m_000000_0
   [druid] 2018-12-06 15:31:11,579 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 15:31:11,580 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:31:11,581 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:31:11,582 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:31:11,591 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:31:11,591 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:31:11,592 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:31:11,593 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 15:31:11,594 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:31:11,594 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:31:11,596 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:31:11,596 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:31:11,612 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:31:11,615 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1497175754_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:31:11,616 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:31:11,616 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1497175754_0001_r_000000_0' done.
   [druid] 2018-12-06 15:31:11,616 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1497175754_0001_r_000000_0
   [druid] 2018-12-06 15:31:11,616 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:31:11,623 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:31:11,801 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:31:11,801 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1497175754_0001 completed successfully
   [druid] 2018-12-06 15:31:11,812 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=603382
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=120
		Total committed heap usage (bytes)=780140544
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:36:24,705 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:36:24,706 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:36:25,297 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:36:25,373 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:36:25,403 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:36:25,468 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1973775673_0001
   [druid] 2018-12-06 15:36:25,585 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:36:25,586 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1973775673_0001
   [druid] 2018-12-06 15:36:25,587 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:36:25,591 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:36:25,591 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:36:25,597 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:36:25,629 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:36:25,630 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1973775673_0001_m_000000_0
   [druid] 2018-12-06 15:36:25,654 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:36:25,658 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:36:25,729 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@28b7031e
   [druid] 2018-12-06 15:36:25,735 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:36:25,781 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:36:25,781 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:36:25,781 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:36:25,781 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:36:25,781 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:36:25,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:36:26,461 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,462 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,462 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,462 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,462 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,463 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,463 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,463 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,464 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:36:26,587 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1973775673_0001 running in uber mode : false
   [druid] 2018-12-06 15:36:26,588 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:36:31,668 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:36:32,591 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 26% reduce 0%
   [druid] 2018-12-06 15:36:34,668 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:36:35,592 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 42% reduce 0%
   [druid] 2018-12-06 15:36:37,669 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:36:38,593 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 60% reduce 0%
   [druid] 2018-12-06 15:36:39,158 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:36:39,160 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:36:39,267 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1973775673_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:36:39,268 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:36:39,268 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1973775673_0001_m_000000_0' done.
   [druid] 2018-12-06 15:36:39,268 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1973775673_0001_m_000000_0
   [druid] 2018-12-06 15:36:39,269 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:36:39,270 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:36:39,271 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1973775673_0001_r_000000_0
   [druid] 2018-12-06 15:36:39,276 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:36:39,276 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:36:39,352 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@12951735
   [druid] 2018-12-06 15:36:39,356 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@658b2b47
   [druid] 2018-12-06 15:36:39,374 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:36:39,377 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1973775673_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:36:39,407 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1973775673_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 15:36:39,411 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local1973775673_0001_m_000000_0
   [druid] 2018-12-06 15:36:39,412 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 15:36:39,414 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:36:39,415 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:36:39,415 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:36:39,426 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:36:39,427 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:36:39,429 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:36:39,429 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 15:36:39,430 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:36:39,430 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:36:39,432 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:36:39,432 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:36:39,447 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:36:39,450 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1973775673_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:36:39,451 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:36:39,451 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1973775673_0001_r_000000_0' done.
   [druid] 2018-12-06 15:36:39,451 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1973775673_0001_r_000000_0
   [druid] 2018-12-06 15:36:39,451 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:36:39,456 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:36:39,593 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:36:39,593 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1973775673_0001 completed successfully
   [druid] 2018-12-06 15:36:39,608 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=603382
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=127
		Total committed heap usage (bytes)=786432000
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:37:33,601 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:37:33,602 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:37:34,190 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:37:34,264 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:37:34,296 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:37:34,368 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2002555952_0001
   [druid] 2018-12-06 15:37:34,484 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:37:34,485 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2002555952_0001
   [druid] 2018-12-06 15:37:34,486 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:37:34,490 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:37:34,490 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:37:34,495 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:37:34,523 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:37:34,524 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2002555952_0001_m_000000_0
   [druid] 2018-12-06 15:37:34,543 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:37:34,547 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:37:34,618 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@162ccb8f
   [druid] 2018-12-06 15:37:34,624 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:37:34,669 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:37:34,670 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:37:34,670 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:37:34,670 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:37:34,670 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:37:34,673 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:37:35,421 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,421 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,422 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,422 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,423 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,423 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,424 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,424 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,424 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:37:35,488 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2002555952_0001 running in uber mode : false
   [druid] 2018-12-06 15:37:35,489 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:37:40,611 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:37:41,492 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 25% reduce 0%
   [druid] 2018-12-06 15:37:43,613 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:37:44,493 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 41% reduce 0%
   [druid] 2018-12-06 15:37:46,614 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:37:47,494 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 58% reduce 0%
   [druid] 2018-12-06 15:37:48,339 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:37:48,341 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:37:48,341 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:37:48,341 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1006; bufvoid = 104857600
   [druid] 2018-12-06 15:37:48,341 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 15:37:48,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:37:48,365 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2002555952_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:37:48,366 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:37:48,366 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2002555952_0001_m_000000_0' done.
   [druid] 2018-12-06 15:37:48,366 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2002555952_0001_m_000000_0
   [druid] 2018-12-06 15:37:48,367 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:37:48,368 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:37:48,369 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2002555952_0001_r_000000_0
   [druid] 2018-12-06 15:37:48,375 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:37:48,375 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:37:48,455 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2e469a5f
   [druid] 2018-12-06 15:37:48,457 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@16ecfd8e
   [druid] 2018-12-06 15:37:48,468 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:37:48,470 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2002555952_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:37:48,494 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 15:37:48,494 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2002555952_0001_m_000000_0 decomp: 1024 len: 1028 to MEMORY
   [druid] 2018-12-06 15:37:48,498 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1024 bytes from map-output for attempt_local2002555952_0001_m_000000_0
   [druid] 2018-12-06 15:37:48,499 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1024, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1024
   [druid] 2018-12-06 15:37:48,500 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:37:48,501 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:37:48,501 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:37:48,510 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:37:48,510 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 15:37:48,512 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1024 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:37:48,512 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1028 bytes from disk
   [druid] 2018-12-06 15:37:48,513 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:37:48,513 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:37:48,513 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 15:37:48,513 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:37:48,529 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:37:48,566 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:37:48,571 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 15:37:48,571 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local2002555952_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.NewMember.NewMemberOutputWriter.output(NewMemberOutputWriter.java:30)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:101)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalMember.TotalMemberReducer.reduce(TotalMemberReducer.java:63)
	at com.phone.analysis.mapreduce.NewTotalMember.TotalMemberReducer.reduce(TotalMemberReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 15:37:49,494 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2002555952_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 15:37:49,505 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=302710
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1006
		Map output materialized bytes=1028
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=1028
		Reduce input records=0
		Reduce output records=0
		Spilled Records=8
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=119
		Total committed heap usage (bytes)=388497408
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:38:56,293 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:38:56,296 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:38:56,874 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:38:56,951 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:38:56,983 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:38:57,056 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local357357539_0001
   [druid] 2018-12-06 15:38:57,181 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:38:57,182 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local357357539_0001
   [druid] 2018-12-06 15:38:57,183 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:38:57,188 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:38:57,188 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:38:57,194 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:38:57,234 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:38:57,235 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local357357539_0001_m_000000_0
   [druid] 2018-12-06 15:38:57,260 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:38:57,266 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:38:57,339 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@fc48d80
   [druid] 2018-12-06 15:38:57,345 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:38:57,392 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:38:57,392 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:38:57,392 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:38:57,392 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:38:57,392 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:38:57,395 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:38:58,107 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,107 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,108 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,108 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,108 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,109 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,109 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,109 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,109 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:38:58,184 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local357357539_0001 running in uber mode : false
   [druid] 2018-12-06 15:38:58,185 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:39:03,272 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:39:04,189 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 26% reduce 0%
   [druid] 2018-12-06 15:39:06,273 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:39:07,190 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 44% reduce 0%
   [druid] 2018-12-06 15:39:09,274 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:39:10,191 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 61% reduce 0%
   [druid] 2018-12-06 15:39:10,513 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:39:10,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:39:10,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:39:10,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1006; bufvoid = 104857600
   [druid] 2018-12-06 15:39:10,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 15:39:10,630 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:39:10,635 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local357357539_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:39:10,636 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:39:10,636 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local357357539_0001_m_000000_0' done.
   [druid] 2018-12-06 15:39:10,636 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local357357539_0001_m_000000_0
   [druid] 2018-12-06 15:39:10,636 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:39:10,638 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:39:10,638 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local357357539_0001_r_000000_0
   [druid] 2018-12-06 15:39:10,646 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:39:10,646 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:39:10,720 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@357e40dc
   [druid] 2018-12-06 15:39:10,723 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7c47235c
   [druid] 2018-12-06 15:39:10,735 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:39:10,737 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local357357539_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:39:10,767 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local357357539_0001_m_000000_0 decomp: 1024 len: 1028 to MEMORY
   [druid] 2018-12-06 15:39:10,771 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1024 bytes from map-output for attempt_local357357539_0001_m_000000_0
   [druid] 2018-12-06 15:39:10,773 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1024, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1024
   [druid] 2018-12-06 15:39:10,774 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:39:10,775 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:39:10,775 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:39:10,785 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:39:10,785 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 15:39:10,787 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1024 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:39:10,787 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1028 bytes from disk
   [druid] 2018-12-06 15:39:10,788 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:39:10,788 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:39:10,789 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 15:39:10,789 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:39:10,808 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:39:10,825 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:39:10,832 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 15:39:10,832 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local357357539_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.NewMember.NewMemberOutputWriter.output(NewMemberOutputWriter.java:30)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:101)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalMember.TotalMemberReducer.reduce(TotalMemberReducer.java:63)
	at com.phone.analysis.mapreduce.NewTotalMember.TotalMemberReducer.reduce(TotalMemberReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 15:39:11,192 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 15:39:11,192 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local357357539_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 15:39:11,203 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=301170
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1006
		Map output materialized bytes=1028
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=1028
		Reduce input records=0
		Reduce output records=0
		Spilled Records=8
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=120
		Total committed heap usage (bytes)=389021696
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:41:39,435 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:41:39,438 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:41:40,015 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:41:40,082 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:41:40,108 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:41:40,172 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local678573550_0001
   [druid] 2018-12-06 15:41:40,293 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:41:40,293 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local678573550_0001
   [druid] 2018-12-06 15:41:40,294 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:41:40,300 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:41:40,300 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:41:40,306 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:41:40,336 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:41:40,337 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local678573550_0001_m_000000_0
   [druid] 2018-12-06 15:41:40,358 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:41:40,362 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:41:40,513 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@219664a3
   [druid] 2018-12-06 15:41:40,520 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:41:40,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:41:40,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:41:40,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:41:40,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:41:40,570 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:41:40,575 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:41:41,016 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,017 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,017 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,017 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,018 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,018 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,018 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,018 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,019 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:41:41,295 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local678573550_0001 running in uber mode : false
   [druid] 2018-12-06 15:41:41,296 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:41:46,369 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:41:47,300 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 26% reduce 0%
   [druid] 2018-12-06 15:41:49,370 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:41:50,301 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 43% reduce 0%
   [druid] 2018-12-06 15:41:52,371 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:41:53,302 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 60% reduce 0%
   [druid] 2018-12-06 15:41:53,816 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:41:53,818 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:41:53,930 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local678573550_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:41:53,931 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:41:53,931 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local678573550_0001_m_000000_0' done.
   [druid] 2018-12-06 15:41:53,931 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local678573550_0001_m_000000_0
   [druid] 2018-12-06 15:41:53,931 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:41:53,933 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:41:53,933 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local678573550_0001_r_000000_0
   [druid] 2018-12-06 15:41:53,939 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:41:53,939 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:41:54,014 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@37cabfff
   [druid] 2018-12-06 15:41:54,017 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64982404
   [druid] 2018-12-06 15:41:54,030 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:41:54,032 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local678573550_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:41:54,059 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local678573550_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 15:41:54,066 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local678573550_0001_m_000000_0
   [druid] 2018-12-06 15:41:54,067 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 15:41:54,068 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:41:54,069 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:41:54,069 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:41:54,146 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:41:54,146 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:41:54,148 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:41:54,148 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 15:41:54,149 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:41:54,149 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:41:54,150 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 15:41:54,151 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:41:54,165 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:41:54,168 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local678573550_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:41:54,169 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:41:54,170 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local678573550_0001_r_000000_0' done.
   [druid] 2018-12-06 15:41:54,170 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local678573550_0001_r_000000_0
   [druid] 2018-12-06 15:41:54,170 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:41:54,175 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:41:54,303 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:41:54,304 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local678573550_0001 completed successfully
   [druid] 2018-12-06 15:41:54,315 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=600246
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=231
		Total committed heap usage (bytes)=1055916032
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:49:03,678 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:49:03,681 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:49:04,293 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:49:04,358 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:49:04,386 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:49:04,520 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2111465518_0001
   [druid] 2018-12-06 15:49:04,630 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:49:04,631 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2111465518_0001
   [druid] 2018-12-06 15:49:04,631 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:49:04,635 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:49:04,635 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:49:04,641 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:49:04,673 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:49:04,674 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2111465518_0001_m_000000_0
   [druid] 2018-12-06 15:49:04,693 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:49:04,697 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:49:04,768 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4cf200ac
   [druid] 2018-12-06 15:49:04,774 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:49:04,820 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:49:04,820 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:49:04,820 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:49:04,820 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:49:04,821 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:49:04,823 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:49:05,527 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,528 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,528 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,528 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,528 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,529 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,529 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,529 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,530 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:49:05,633 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2111465518_0001 running in uber mode : false
   [druid] 2018-12-06 15:49:05,634 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:49:10,703 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:49:11,637 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 27% reduce 0%
   [druid] 2018-12-06 15:49:13,704 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:49:14,638 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 44% reduce 0%
   [druid] 2018-12-06 15:49:16,705 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:49:17,639 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 61% reduce 0%
   [druid] 2018-12-06 15:49:17,743 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:49:17,744 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:49:17,744 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:49:17,744 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1022; bufvoid = 104857600
   [druid] 2018-12-06 15:49:17,744 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 15:49:17,754 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:49:17,774 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2111465518_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:49:17,775 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:49:17,775 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2111465518_0001_m_000000_0' done.
   [druid] 2018-12-06 15:49:17,775 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2111465518_0001_m_000000_0
   [druid] 2018-12-06 15:49:17,776 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:49:17,778 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:49:17,778 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2111465518_0001_r_000000_0
   [druid] 2018-12-06 15:49:17,786 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:49:17,786 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:49:17,865 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@49a777ec
   [druid] 2018-12-06 15:49:17,868 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6cf4250a
   [druid] 2018-12-06 15:49:17,880 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:49:17,883 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2111465518_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:49:17,915 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2111465518_0001_m_000000_0 decomp: 1040 len: 1044 to MEMORY
   [druid] 2018-12-06 15:49:17,920 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1040 bytes from map-output for attempt_local2111465518_0001_m_000000_0
   [druid] 2018-12-06 15:49:17,921 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1040, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1040
   [druid] 2018-12-06 15:49:17,922 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:49:17,923 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:49:17,923 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:49:17,933 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:49:17,933 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:49:17,935 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1040 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:49:17,935 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1044 bytes from disk
   [druid] 2018-12-06 15:49:17,936 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:49:17,936 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:49:17,937 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:49:17,937 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:49:17,953 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:49:17,961 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2111465518_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:49:17,962 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:49:17,962 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2111465518_0001_r_000000_0' done.
   [druid] 2018-12-06 15:49:17,962 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2111465518_0001_r_000000_0
   [druid] 2018-12-06 15:49:17,963 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:49:17,968 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:49:18,640 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:49:18,640 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2111465518_0001 completed successfully
   [druid] 2018-12-06 15:49:18,651 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=2440
		FILE: Number of bytes written=608392
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1022
		Map output materialized bytes=1044
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=1044
		Reduce input records=8
		Reduce output records=3
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=116
		Total committed heap usage (bytes)=789577728
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:51:47,675 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:51:47,678 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:51:48,274 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:51:48,347 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:51:48,373 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:51:48,438 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local537896890_0001
   [druid] 2018-12-06 15:51:48,554 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:51:48,555 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local537896890_0001
   [druid] 2018-12-06 15:51:48,556 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:51:48,561 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:51:48,561 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:51:48,566 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:51:48,596 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:51:48,597 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local537896890_0001_m_000000_0
   [druid] 2018-12-06 15:51:48,617 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:51:48,620 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:51:48,691 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@61ee405d
   [druid] 2018-12-06 15:51:48,697 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:51:48,741 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:51:48,741 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:51:48,742 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:51:48,742 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:51:48,742 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:51:48,744 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:51:49,466 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,467 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,467 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,467 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,468 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,468 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,468 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,469 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,469 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:51:49,556 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local537896890_0001 running in uber mode : false
   [druid] 2018-12-06 15:51:49,557 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:51:54,627 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:51:55,560 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 27% reduce 0%
   [druid] 2018-12-06 15:51:57,628 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:51:58,561 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 44% reduce 0%
   [druid] 2018-12-06 15:52:00,629 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:52:01,562 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 61% reduce 0%
   [druid] 2018-12-06 15:52:01,874 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:52:01,876 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:52:01,876 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:52:01,876 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1022; bufvoid = 104857600
   [druid] 2018-12-06 15:52:01,876 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 15:52:01,932 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:52:01,937 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local537896890_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:52:01,938 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:52:01,938 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local537896890_0001_m_000000_0' done.
   [druid] 2018-12-06 15:52:01,939 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local537896890_0001_m_000000_0
   [druid] 2018-12-06 15:52:01,939 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:52:01,940 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:52:01,941 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local537896890_0001_r_000000_0
   [druid] 2018-12-06 15:52:01,948 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:52:01,948 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:52:02,025 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@23115441
   [druid] 2018-12-06 15:52:02,028 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1fd13e3a
   [druid] 2018-12-06 15:52:02,039 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:52:02,041 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local537896890_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:52:02,069 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local537896890_0001_m_000000_0 decomp: 1040 len: 1044 to MEMORY
   [druid] 2018-12-06 15:52:02,073 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1040 bytes from map-output for attempt_local537896890_0001_m_000000_0
   [druid] 2018-12-06 15:52:02,075 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1040, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1040
   [druid] 2018-12-06 15:52:02,076 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:52:02,077 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:52:02,077 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:52:02,102 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:52:02,103 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:52:02,105 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1040 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:52:02,105 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1044 bytes from disk
   [druid] 2018-12-06 15:52:02,106 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:52:02,106 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:52:02,107 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:52:02,107 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:52:02,125 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:52:02,219 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local537896890_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:52:02,220 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:52:02,220 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local537896890_0001_r_000000_0' done.
   [druid] 2018-12-06 15:52:02,220 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local537896890_0001_r_000000_0
   [druid] 2018-12-06 15:52:02,220 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:52:02,226 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:52:02,563 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:52:02,563 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local537896890_0001 completed successfully
   [druid] 2018-12-06 15:52:02,573 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=2440
		FILE: Number of bytes written=605304
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1022
		Map output materialized bytes=1044
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=1044
		Reduce input records=8
		Reduce output records=3
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=119
		Total committed heap usage (bytes)=784334848
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:55:55,894 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:55:55,896 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:55:56,521 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:55:56,602 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:55:56,631 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:55:56,700 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2039358544_0001
   [druid] 2018-12-06 15:55:56,837 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:55:56,837 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2039358544_0001
   [druid] 2018-12-06 15:55:56,838 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:55:56,843 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:55:56,844 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:55:56,848 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:55:56,882 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:55:56,883 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2039358544_0001_m_000000_0
   [druid] 2018-12-06 15:55:56,904 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:55:56,908 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:55:56,977 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@59eab707
   [druid] 2018-12-06 15:55:56,982 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 15:55:57,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:55:57,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:55:57,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:55:57,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:55:57,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:55:57,033 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:55:57,720 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,721 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,721 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,722 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,722 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,723 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,723 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,723 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,724 [ask Executor #0] INFO  wTotalMember.TotalMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:55:57,839 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2039358544_0001 running in uber mode : false
   [druid] 2018-12-06 15:55:57,840 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 15:56:02,913 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:56:03,844 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 28% reduce 0%
   [druid] 2018-12-06 15:56:05,913 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:56:06,845 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 45% reduce 0%
   [druid] 2018-12-06 15:56:08,914 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:56:09,817 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 15:56:09,819 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:56:09,819 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:56:09,819 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1022; bufvoid = 104857600
   [druid] 2018-12-06 15:56:09,819 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 15:56:09,868 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 62% reduce 0%
   [druid] 2018-12-06 15:56:09,928 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:56:09,932 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2039358544_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:56:09,934 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:56:09,934 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2039358544_0001_m_000000_0' done.
   [druid] 2018-12-06 15:56:09,934 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2039358544_0001_m_000000_0
   [druid] 2018-12-06 15:56:09,934 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:56:09,936 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:56:09,936 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2039358544_0001_r_000000_0
   [druid] 2018-12-06 15:56:09,942 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:56:09,942 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:56:10,014 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6365842
   [druid] 2018-12-06 15:56:10,017 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7b69f434
   [druid] 2018-12-06 15:56:10,029 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:56:10,031 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2039358544_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:56:10,064 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2039358544_0001_m_000000_0 decomp: 1040 len: 1044 to MEMORY
   [druid] 2018-12-06 15:56:10,070 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1040 bytes from map-output for attempt_local2039358544_0001_m_000000_0
   [druid] 2018-12-06 15:56:10,071 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1040, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1040
   [druid] 2018-12-06 15:56:10,073 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:56:10,073 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:56:10,074 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:56:10,085 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:56:10,085 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:56:10,087 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1040 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:56:10,087 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1044 bytes from disk
   [druid] 2018-12-06 15:56:10,089 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:56:10,089 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:56:10,089 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 958 bytes
   [druid] 2018-12-06 15:56:10,089 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:56:10,108 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:56:10,191 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2039358544_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:56:10,192 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 15:56:10,192 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2039358544_0001_r_000000_0' done.
   [druid] 2018-12-06 15:56:10,192 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2039358544_0001_r_000000_0
   [druid] 2018-12-06 15:56:10,192 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:56:10,197 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 15:56:10,869 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 15:56:10,869 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2039358544_0001 completed successfully
   [druid] 2018-12-06 15:56:10,880 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=2440
		FILE: Number of bytes written=608392
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1022
		Map output materialized bytes=1044
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=1044
		Reduce input records=8
		Reduce output records=3
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=122
		Total committed heap usage (bytes)=785383424
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 15:59:38,959 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 15:59:38,961 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 15:59:39,532 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 15:59:39,606 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 15:59:39,634 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 15:59:39,700 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local749236368_0001
   [druid] 2018-12-06 15:59:39,817 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 15:59:39,818 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local749236368_0001
   [druid] 2018-12-06 15:59:39,819 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 15:59:39,824 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:59:39,824 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 15:59:39,828 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 15:59:39,858 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 15:59:39,860 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local749236368_0001_m_000000_0
   [druid] 2018-12-06 15:59:39,877 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:59:39,880 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:59:39,947 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@637877a6
   [druid] 2018-12-06 15:59:39,954 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 15:59:39,999 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 15:59:39,999 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 15:59:39,999 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 15:59:39,999 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 15:59:40,000 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 15:59:40,002 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,498 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,499 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,499 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,499 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 15:59:40,521 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 15:59:40,523 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 15:59:40,523 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 15:59:40,523 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 15:59:40,523 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 15:59:40,555 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 15:59:40,561 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local749236368_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 15:59:40,568 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 15:59:40,568 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local749236368_0001_m_000000_0' done.
   [druid] 2018-12-06 15:59:40,569 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local749236368_0001_m_000000_0
   [druid] 2018-12-06 15:59:40,569 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 15:59:40,571 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 15:59:40,571 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local749236368_0001_r_000000_0
   [druid] 2018-12-06 15:59:40,576 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 15:59:40,576 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 15:59:40,645 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63007fdb
   [druid] 2018-12-06 15:59:40,648 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5782d6d7
   [druid] 2018-12-06 15:59:40,661 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 15:59:40,663 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local749236368_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 15:59:40,687 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local749236368_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 15:59:40,690 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local749236368_0001_m_000000_0
   [druid] 2018-12-06 15:59:40,692 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 15:59:40,693 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 15:59:40,694 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:59:40,694 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 15:59:40,703 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:59:40,703 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 15:59:40,706 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 15:59:40,707 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 15:59:40,707 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 15:59:40,708 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 15:59:40,709 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 15:59:40,709 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 15:59:40,820 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local749236368_0001 running in uber mode : false
   [druid] 2018-12-06 15:59:40,821 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 15:59:40,980 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 15:59:41,072 [pool-6-thread-1] FATAL ache.hadoop.conf.Configuration {1} - error parsing conf 
   org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 15:59:41,092 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 15:59:41,098 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 15:59:41,098 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local749236368_0001
   java.lang.Exception: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 19 more
[druid] 2018-12-06 15:59:41,822 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local749236368_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 15:59:41,833 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=309554
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91736
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=8472
		Reduce input records=0
		Reduce output records=0
		Spilled Records=62
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=282066944
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:00:32,070 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:00:32,074 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:00:32,652 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:00:32,728 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:00:32,756 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:00:32,821 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local927818039_0001
   [druid] 2018-12-06 16:00:32,953 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:00:32,954 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local927818039_0001
   [druid] 2018-12-06 16:00:32,955 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:00:32,961 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:00:32,961 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:00:32,967 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:00:32,995 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:00:32,996 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local927818039_0001_m_000000_0
   [druid] 2018-12-06 16:00:33,016 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:00:33,020 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:00:33,090 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@162ccb8f
   [druid] 2018-12-06 16:00:33,096 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:00:33,141 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:00:33,141 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:00:33,141 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:00:33,141 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:00:33,141 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:00:33,145 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:00:33,574 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,574 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,575 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,575 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,575 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,575 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,575 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,576 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,576 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:00:33,601 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:00:33,603 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:00:33,603 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:00:33,603 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:00:33,603 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:00:33,633 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:00:33,640 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local927818039_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:00:33,647 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:00:33,647 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local927818039_0001_m_000000_0' done.
   [druid] 2018-12-06 16:00:33,647 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local927818039_0001_m_000000_0
   [druid] 2018-12-06 16:00:33,647 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:00:33,649 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:00:33,650 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local927818039_0001_r_000000_0
   [druid] 2018-12-06 16:00:33,655 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:00:33,655 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:00:33,726 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@23e5aa4f
   [druid] 2018-12-06 16:00:33,728 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2fe09acd
   [druid] 2018-12-06 16:00:33,743 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:00:33,744 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local927818039_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:00:33,773 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local927818039_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:00:33,780 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local927818039_0001_m_000000_0
   [druid] 2018-12-06 16:00:33,782 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:00:33,783 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:00:33,784 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:00:33,784 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:00:33,797 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:00:33,797 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:00:33,828 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:00:33,828 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:00:33,830 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:00:33,830 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:00:33,831 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:00:33,831 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:00:33,956 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local927818039_0001 running in uber mode : false
   [druid] 2018-12-06 16:00:33,957 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:00:34,110 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:00:34,174 [pool-6-thread-1] FATAL ache.hadoop.conf.Configuration {1} - error parsing conf 
   org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 16:00:34,197 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:00:34,202 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 16:00:34,202 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local927818039_0001
   java.lang.Exception: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 19 more
[druid] 2018-12-06 16:00:34,959 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local927818039_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 16:00:34,972 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=309554
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91736
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=8472
		Reduce input records=0
		Reduce output records=0
		Spilled Records=62
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=282066944
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:01:04,299 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,300 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,301 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,301 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,301 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,301 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,302 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,303 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:04,303 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,236 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,236 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local121537319_0001 running in uber mode : false
   [druid] 2018-12-06 16:01:05,237 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,237 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,238 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,238 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,238 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,238 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,238 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,239 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,239 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,239 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,239 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,239 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 16:01:05,239 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:05,240 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:01:12,475 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:01:12,479 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:01:13,059 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:01:13,122 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:01:13,147 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:01:13,208 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1441191074_0001
   [druid] 2018-12-06 16:01:13,318 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:01:13,319 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1441191074_0001
   [druid] 2018-12-06 16:01:13,320 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:01:13,325 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:01:13,325 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:01:13,330 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:01:13,356 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:01:13,358 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1441191074_0001_m_000000_0
   [druid] 2018-12-06 16:01:13,382 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:01:13,387 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:01:13,462 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@76ffd30f
   [druid] 2018-12-06 16:01:13,468 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:01:13,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:01:13,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:01:13,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:01:13,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:01:13,515 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:01:13,517 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:01:13,904 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,904 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,904 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,904 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,904 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,905 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,905 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,905 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,905 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:01:13,929 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:01:13,931 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:01:13,931 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:01:13,931 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:01:13,931 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:01:14,037 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:01:14,043 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1441191074_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:01:14,051 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:01:14,051 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1441191074_0001_m_000000_0' done.
   [druid] 2018-12-06 16:01:14,051 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1441191074_0001_m_000000_0
   [druid] 2018-12-06 16:01:14,051 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:01:14,053 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:01:14,053 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1441191074_0001_r_000000_0
   [druid] 2018-12-06 16:01:14,059 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:01:14,059 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:01:14,134 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3f3d6d76
   [druid] 2018-12-06 16:01:14,137 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@460e351
   [druid] 2018-12-06 16:01:14,154 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:01:14,156 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1441191074_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:01:14,189 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1441191074_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:01:14,196 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1441191074_0001_m_000000_0
   [druid] 2018-12-06 16:01:14,197 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:01:14,199 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:01:14,199 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:01:14,199 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:01:14,209 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:01:14,209 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:01:14,212 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:01:14,212 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:01:14,213 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:01:14,213 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:01:14,214 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:01:14,215 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:01:14,321 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1441191074_0001 running in uber mode : false
   [druid] 2018-12-06 16:01:14,322 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:01:14,471 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:01:14,529 [pool-6-thread-1] FATAL ache.hadoop.conf.Configuration {1} - error parsing conf 
   org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 16:01:14,532 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:01:14,538 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 16:01:14,538 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1441191074_0001
   java.lang.Exception: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 19 more
[druid] 2018-12-06 16:01:15,323 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1441191074_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 16:01:15,335 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=311098
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91736
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=8472
		Reduce input records=0
		Reduce output records=0
		Spilled Records=62
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=282066944
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:02:33,291 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:02:33,294 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:02:33,874 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:02:33,950 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:02:33,979 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:02:34,048 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1293367842_0001
   [druid] 2018-12-06 16:02:34,170 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:02:34,171 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1293367842_0001
   [druid] 2018-12-06 16:02:34,172 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:02:34,178 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:02:34,178 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:02:34,183 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:02:34,214 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:02:34,216 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1293367842_0001_m_000000_0
   [druid] 2018-12-06 16:02:34,237 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:02:34,241 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:02:34,309 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7841c6e5
   [druid] 2018-12-06 16:02:34,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:02:34,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:02:34,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:02:34,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:02:34,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:02:34,360 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:02:34,363 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:02:34,781 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,781 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,781 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,782 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:02:34,808 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:02:34,810 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:02:34,810 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:02:34,810 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:02:34,810 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:02:34,825 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:02:34,831 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1293367842_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:02:34,838 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:02:34,838 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1293367842_0001_m_000000_0' done.
   [druid] 2018-12-06 16:02:34,838 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1293367842_0001_m_000000_0
   [druid] 2018-12-06 16:02:34,838 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:02:34,840 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:02:34,840 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1293367842_0001_r_000000_0
   [druid] 2018-12-06 16:02:34,845 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:02:34,846 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:02:34,916 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4bb1e412
   [druid] 2018-12-06 16:02:34,919 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3f47093
   [druid] 2018-12-06 16:02:34,932 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:02:34,934 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1293367842_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:02:34,959 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1293367842_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:02:34,963 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1293367842_0001_m_000000_0
   [druid] 2018-12-06 16:02:34,963 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:02:34,964 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:02:34,965 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:02:34,965 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:02:34,974 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:02:34,974 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:02:34,978 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:02:34,978 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:02:34,979 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:02:34,979 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:02:34,980 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:02:34,980 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:02:35,173 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1293367842_0001 running in uber mode : false
   [druid] 2018-12-06 16:02:35,174 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:02:35,248 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:02:35,316 [pool-6-thread-1] FATAL ache.hadoop.conf.Configuration {1} - error parsing conf 
   org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 16:02:35,319 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:02:35,325 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 16:02:35,325 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1293367842_0001
   java.lang.Exception: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 19 more
[druid] 2018-12-06 16:02:36,176 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1293367842_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 16:02:36,190 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=311098
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91736
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=8472
		Reduce input records=0
		Reduce output records=0
		Spilled Records=62
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=281018368
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:03:10,681 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:03:10,682 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:03:11,250 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:03:11,330 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:03:11,358 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:03:11,434 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1471152822_0001
   [druid] 2018-12-06 16:03:11,583 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:03:11,584 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1471152822_0001
   [druid] 2018-12-06 16:03:11,586 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:03:11,592 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:03:11,592 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:03:11,598 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:03:11,637 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:03:11,639 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1471152822_0001_m_000000_0
   [druid] 2018-12-06 16:03:11,667 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:03:11,672 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:03:11,757 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@27e72f97
   [druid] 2018-12-06 16:03:11,764 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 16:03:11,811 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:03:11,811 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:03:11,811 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:03:11,811 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:03:11,811 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:03:11,814 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:03:12,234 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,234 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,234 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,235 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,235 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,235 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,236 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,236 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,236 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:03:12,587 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1471152822_0001 running in uber mode : false
   [druid] 2018-12-06 16:03:12,588 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 16:03:17,676 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:03:18,592 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 29% reduce 0%
   [druid] 2018-12-06 16:03:20,677 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:03:21,593 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 47% reduce 0%
   [druid] 2018-12-06 16:03:23,678 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:03:24,219 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:03:24,220 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:03:24,330 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1471152822_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:03:24,332 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:03:24,332 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1471152822_0001_m_000000_0' done.
   [druid] 2018-12-06 16:03:24,332 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1471152822_0001_m_000000_0
   [druid] 2018-12-06 16:03:24,332 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:03:24,334 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:03:24,334 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1471152822_0001_r_000000_0
   [druid] 2018-12-06 16:03:24,339 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:03:24,340 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:03:24,413 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@24ef2367
   [druid] 2018-12-06 16:03:24,416 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@28f63bbb
   [druid] 2018-12-06 16:03:24,430 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:03:24,433 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1471152822_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:03:24,467 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1471152822_0001_m_000000_0 decomp: 2 len: 6 to MEMORY
   [druid] 2018-12-06 16:03:24,473 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 2 bytes from map-output for attempt_local1471152822_0001_m_000000_0
   [druid] 2018-12-06 16:03:24,475 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 2, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->2
   [druid] 2018-12-06 16:03:24,482 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:03:24,483 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:03:24,483 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:03:24,495 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:03:24,495 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 16:03:24,498 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 2 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:03:24,499 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 6 bytes from disk
   [druid] 2018-12-06 16:03:24,500 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:03:24,500 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:03:24,502 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 0 segments left of total size: 0 bytes
   [druid] 2018-12-06 16:03:24,503 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:03:24,520 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:03:24,524 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1471152822_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:03:24,525 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:03:24,525 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1471152822_0001_r_000000_0' done.
   [druid] 2018-12-06 16:03:24,525 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1471152822_0001_r_000000_0
   [druid] 2018-12-06 16:03:24,525 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:03:24,531 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:03:24,594 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:03:24,594 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1471152822_0001 completed successfully
   [druid] 2018-12-06 16:03:24,606 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=364
		FILE: Number of bytes written=605222
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=0
		Map output bytes=0
		Map output materialized bytes=6
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=6
		Reduce input records=0
		Reduce output records=0
		Spilled Records=0
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=119
		Total committed heap usage (bytes)=781189120
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:05:35,780 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:05:35,781 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:05:36,333 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:05:36,399 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:05:36,591 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:05:36,796 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1118846269_0001
   [druid] 2018-12-06 16:05:36,935 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:05:36,935 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1118846269_0001
   [druid] 2018-12-06 16:05:36,936 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:05:36,941 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:05:36,941 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:05:36,947 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:05:36,979 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:05:36,981 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1118846269_0001_m_000000_0
   [druid] 2018-12-06 16:05:37,006 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:05:37,011 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:05:37,089 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6dd757c7
   [druid] 2018-12-06 16:05:37,096 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:05:37,143 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:05:37,143 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:05:37,143 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:05:37,143 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:05:37,143 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:05:37,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:05:37,552 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,553 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:05:37,577 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:05:37,579 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:05:37,579 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:05:37,579 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:05:37,579 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:05:37,591 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:05:37,597 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1118846269_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:05:37,606 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:05:37,606 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1118846269_0001_m_000000_0' done.
   [druid] 2018-12-06 16:05:37,606 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1118846269_0001_m_000000_0
   [druid] 2018-12-06 16:05:37,606 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:05:37,608 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:05:37,608 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1118846269_0001_r_000000_0
   [druid] 2018-12-06 16:05:37,613 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:05:37,613 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:05:37,682 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63007fdb
   [druid] 2018-12-06 16:05:37,684 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5782d6d7
   [druid] 2018-12-06 16:05:37,699 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:05:37,701 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1118846269_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:05:37,726 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1118846269_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:05:37,731 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1118846269_0001_m_000000_0
   [druid] 2018-12-06 16:05:37,733 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:05:37,734 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:05:37,735 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:05:37,735 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:05:37,743 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:05:37,743 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:05:37,745 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:05:37,746 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:05:37,746 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:05:37,746 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:05:37,747 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:05:37,748 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:05:37,938 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1118846269_0001 running in uber mode : false
   [druid] 2018-12-06 16:05:37,939 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:05:37,996 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:05:38,063 [pool-6-thread-1] FATAL ache.hadoop.conf.Configuration {1} - error parsing conf 
   org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 16:05:38,068 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:05:38,072 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 16:05:38,073 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1118846269_0001
   java.lang.Exception: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.RuntimeException: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2645)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2502)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2405)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:981)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:88)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserReducer.reduce(NewTotalUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.xml.sax.SAXParseException; systemId: file:/E:/JavaProjects/GP1815_LogAnalysis/target/classes/; lineNumber: 1; columnNumber: 1; Content is not allowed in prolog.
	at org.apache.xerces.parsers.DOMParser.parse(Unknown Source)
	at org.apache.xerces.jaxp.DocumentBuilderImpl.parse(Unknown Source)
	at javax.xml.parsers.DocumentBuilder.parse(DocumentBuilder.java:150)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2480)
	at org.apache.hadoop.conf.Configuration.parse(Configuration.java:2468)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2539)
	... 19 more
[druid] 2018-12-06 16:05:38,939 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1118846269_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 16:05:38,952 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=311098
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=91736
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=8472
		Reduce input records=0
		Reduce output records=0
		Spilled Records=62
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=0
		Total committed heap usage (bytes)=280494080
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:06:00,204 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:06:00,205 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:06:00,814 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:06:00,871 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:06:00,906 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:06:00,982 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1615384560_0001
   [druid] 2018-12-06 16:06:01,110 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:06:01,111 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1615384560_0001
   [druid] 2018-12-06 16:06:01,112 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:06:01,118 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:06:01,118 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:06:01,124 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:06:01,161 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:06:01,163 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1615384560_0001_m_000000_0
   [druid] 2018-12-06 16:06:01,190 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:06:01,194 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:06:01,262 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@173eaac3
   [druid] 2018-12-06 16:06:01,270 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:06:01,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:06:01,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:06:01,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:06:01,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:06:01,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:06:01,325 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:06:01,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,763 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,763 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,763 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,763 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:06:01,791 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:06:01,794 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:06:01,794 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:06:01,794 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:06:01,794 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:06:01,807 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:06:01,813 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1615384560_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:06:01,825 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:06:01,825 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1615384560_0001_m_000000_0' done.
   [druid] 2018-12-06 16:06:01,825 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1615384560_0001_m_000000_0
   [druid] 2018-12-06 16:06:01,825 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:06:01,828 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:06:01,828 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1615384560_0001_r_000000_0
   [druid] 2018-12-06 16:06:01,836 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:06:01,836 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:06:01,911 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@70086b2a
   [druid] 2018-12-06 16:06:01,914 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@64ed3335
   [druid] 2018-12-06 16:06:01,929 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:06:01,932 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1615384560_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:06:01,963 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1615384560_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:06:01,970 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1615384560_0001_m_000000_0
   [druid] 2018-12-06 16:06:01,972 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:06:01,974 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:06:01,975 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:06:01,975 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:06:01,987 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:06:01,987 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:06:01,990 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:06:01,990 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:06:01,991 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:06:01,991 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:06:01,992 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:06:01,992 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:06:02,113 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1615384560_0001 running in uber mode : false
   [druid] 2018-12-06 16:06:02,115 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:06:02,307 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:09:24,545 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:09:24,547 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:09:25,123 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:09:25,193 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:09:25,313 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:09:25,374 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local151368140_0001
   [druid] 2018-12-06 16:09:25,496 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:09:25,497 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local151368140_0001
   [druid] 2018-12-06 16:09:25,498 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:09:25,503 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:09:25,503 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:09:25,508 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:09:25,537 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:09:25,538 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local151368140_0001_m_000000_0
   [druid] 2018-12-06 16:09:25,558 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:09:25,561 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:09:25,639 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1d319783
   [druid] 2018-12-06 16:09:25,646 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:09:25,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:09:25,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:09:25,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:09:25,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:09:25,694 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:09:25,697 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:09:26,105 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,105 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,105 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,105 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,106 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,107 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,107 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,107 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,107 [ask Executor #0] INFO  .SessionAnalysis.SessionMapper {1} - serverTime或者u_sd不能为空
   [druid] 2018-12-06 16:09:26,143 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:09:26,145 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:09:26,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:09:26,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 44404; bufvoid = 104857600
   [druid] 2018-12-06 16:09:26,146 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213016(104852064); length = 1381/6553600
   [druid] 2018-12-06 16:09:26,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:09:26,168 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local151368140_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:09:26,176 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:09:26,177 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local151368140_0001_m_000000_0' done.
   [druid] 2018-12-06 16:09:26,177 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local151368140_0001_m_000000_0
   [druid] 2018-12-06 16:09:26,177 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:09:26,179 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:09:26,179 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local151368140_0001_r_000000_0
   [druid] 2018-12-06 16:09:26,184 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:09:26,184 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:09:26,256 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16937d04
   [druid] 2018-12-06 16:09:26,260 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f8bcec6
   [druid] 2018-12-06 16:09:26,274 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:09:26,276 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local151368140_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:09:26,304 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local151368140_0001_m_000000_0 decomp: 45098 len: 45102 to MEMORY
   [druid] 2018-12-06 16:09:26,308 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 45098 bytes from map-output for attempt_local151368140_0001_m_000000_0
   [druid] 2018-12-06 16:09:26,309 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 45098, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->45098
   [druid] 2018-12-06 16:09:26,310 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:09:26,311 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:09:26,311 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:09:26,322 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:09:26,322 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 16:09:26,326 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 45098 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:09:26,327 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 45102 bytes from disk
   [druid] 2018-12-06 16:09:26,328 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:09:26,328 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:09:26,328 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 45028 bytes
   [druid] 2018-12-06 16:09:26,329 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:09:26,498 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local151368140_0001 running in uber mode : false
   [druid] 2018-12-06 16:09:26,499 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:09:26,589 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:09:26,786 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local151368140_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:09:26,786 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:09:26,787 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local151368140_0001_r_000000_0' done.
   [druid] 2018-12-06 16:09:26,787 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local151368140_0001_r_000000_0
   [druid] 2018-12-06 16:09:26,787 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:09:26,791 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:09:27,499 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:09:27,499 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local151368140_0001 completed successfully
   [druid] 2018-12-06 16:09:27,510 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=90556
		FILE: Number of bytes written=737458
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=346
		Map output bytes=44404
		Map output materialized bytes=45102
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=45102
		Reduce input records=346
		Reduce output records=3
		Spilled Records=692
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=598212608
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:09:54,107 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:11:25,010 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:11:25,011 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:11:25,664 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:11:25,745 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:11:25,781 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:11:25,858 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local916395774_0001
   [druid] 2018-12-06 16:11:26,009 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:11:26,010 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local916395774_0001
   [druid] 2018-12-06 16:11:26,011 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:11:26,018 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:11:26,018 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:11:26,025 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:11:26,062 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:11:26,064 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local916395774_0001_m_000000_0
   [druid] 2018-12-06 16:11:26,088 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:11:26,093 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:11:26,176 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3c65deb7
   [druid] 2018-12-06 16:11:26,182 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-06 16:11:26,232 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:11:26,232 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:11:26,232 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:11:26,232 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:11:26,232 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:11:26,235 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:11:26,631 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,631 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,631 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,631 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,632 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,632 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,632 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,632 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,632 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:11:26,657 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:11:26,658 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:11:26,658 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:11:26,658 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-06 16:11:26,658 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-06 16:11:26,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:11:26,776 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local916395774_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:11:26,784 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:11:26,784 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local916395774_0001_m_000000_0' done.
   [druid] 2018-12-06 16:11:26,784 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local916395774_0001_m_000000_0
   [druid] 2018-12-06 16:11:26,785 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:11:26,786 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:11:26,787 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local916395774_0001_r_000000_0
   [druid] 2018-12-06 16:11:26,792 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:11:26,792 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:11:26,869 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@45c5e241
   [druid] 2018-12-06 16:11:26,872 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4be3d2c4
   [druid] 2018-12-06 16:11:26,889 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:11:26,890 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local916395774_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:11:26,918 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local916395774_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-06 16:11:26,924 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local916395774_0001_m_000000_0
   [druid] 2018-12-06 16:11:26,925 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-06 16:11:26,926 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:11:26,927 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:11:26,927 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:11:26,999 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:11:26,999 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:11:27,001 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:11:27,002 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-06 16:11:27,003 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:11:27,003 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:11:27,004 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-06 16:11:27,004 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:11:27,013 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local916395774_0001 running in uber mode : false
   [druid] 2018-12-06 16:11:27,014 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:11:27,262 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:11:27,421 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local916395774_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:11:27,421 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:11:27,421 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local916395774_0001_r_000000_0' done.
   [druid] 2018-12-06 16:11:27,421 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local916395774_0001_r_000000_0
   [druid] 2018-12-06 16:11:27,421 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:11:27,426 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:11:28,015 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:11:28,015 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local916395774_0001 completed successfully
   [druid] 2018-12-06 16:11:28,026 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=17296
		FILE: Number of bytes written=627580
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=8472
		Reduce input records=62
		Reduce output records=6
		Spilled Records=124
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=601358336
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:48:32,518 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:48:32,519 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:48:33,114 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:48:33,193 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:48:33,234 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:48:33,301 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local87892091_0001
   [druid] 2018-12-06 16:48:33,420 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:48:33,421 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local87892091_0001
   [druid] 2018-12-06 16:48:33,423 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:48:33,426 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:48:33,426 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:48:33,431 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:48:33,464 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:48:33,466 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local87892091_0001_m_000000_0
   [druid] 2018-12-06 16:48:33,489 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:48:33,493 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:48:33,642 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1d319783
   [druid] 2018-12-06 16:48:33,650 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 16:48:33,700 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:48:33,700 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:48:33,700 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:48:33,701 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:48:33,701 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:48:33,705 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:48:34,171 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,172 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,173 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:48:34,288 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:48:34,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:48:34,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:48:34,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 16:48:34,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 16:48:34,334 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:48:34,342 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local87892091_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:48:34,352 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:48:34,352 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local87892091_0001_m_000000_0' done.
   [druid] 2018-12-06 16:48:34,352 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local87892091_0001_m_000000_0
   [druid] 2018-12-06 16:48:34,352 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:48:34,355 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:48:34,355 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local87892091_0001_r_000000_0
   [druid] 2018-12-06 16:48:34,360 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:48:34,360 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:48:34,423 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local87892091_0001 running in uber mode : false
   [druid] 2018-12-06 16:48:34,424 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:48:34,432 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@42fada95
   [druid] 2018-12-06 16:48:34,434 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33084509
   [druid] 2018-12-06 16:48:34,445 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:48:34,447 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local87892091_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:48:34,473 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local87892091_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 16:48:34,477 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local87892091_0001_m_000000_0
   [druid] 2018-12-06 16:48:34,479 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 16:48:34,480 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:48:34,481 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:48:34,481 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:48:34,492 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:48:34,493 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:48:34,501 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:48:34,502 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 16:48:34,502 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:48:34,503 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:48:34,503 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:48:34,504 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:48:34,770 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:48:34,840 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local87892091_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:48:34,841 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:48:34,841 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local87892091_0001_r_000000_0' done.
   [druid] 2018-12-06 16:48:34,842 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local87892091_0001_r_000000_0
   [druid] 2018-12-06 16:48:34,842 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:48:34,849 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:48:35,426 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:48:35,426 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local87892091_0001 completed successfully
   [druid] 2018-12-06 16:48:35,436 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1918118
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=630194176
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:52:03,518 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:52:03,520 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:52:04,136 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:52:04,213 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:52:04,241 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:52:04,310 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1361984318_0001
   [druid] 2018-12-06 16:52:04,429 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:52:04,430 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1361984318_0001
   [druid] 2018-12-06 16:52:04,431 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:52:04,436 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:52:04,436 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:52:04,442 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:52:04,469 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:52:04,471 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1361984318_0001_m_000000_0
   [druid] 2018-12-06 16:52:04,491 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:52:04,494 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:52:04,570 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@27e72f97
   [druid] 2018-12-06 16:52:04,577 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 16:52:04,622 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:52:04,622 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:52:04,622 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:52:04,622 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:52:04,622 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:52:04,625 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:52:05,060 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,061 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,061 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,061 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,062 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,062 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,062 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,062 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,062 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:52:05,167 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:52:05,169 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:52:05,169 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:52:05,170 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 16:52:05,170 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 16:52:05,303 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:52:05,310 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1361984318_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:52:05,320 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:52:05,320 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1361984318_0001_m_000000_0' done.
   [druid] 2018-12-06 16:52:05,321 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1361984318_0001_m_000000_0
   [druid] 2018-12-06 16:52:05,321 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:52:05,323 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:52:05,323 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1361984318_0001_r_000000_0
   [druid] 2018-12-06 16:52:05,329 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:52:05,329 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:52:05,408 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@977e210
   [druid] 2018-12-06 16:52:05,410 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@10097b0b
   [druid] 2018-12-06 16:52:05,420 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:52:05,422 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1361984318_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:52:05,432 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1361984318_0001 running in uber mode : false
   [druid] 2018-12-06 16:52:05,433 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:52:05,446 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1361984318_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 16:52:05,451 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local1361984318_0001_m_000000_0
   [druid] 2018-12-06 16:52:05,452 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 16:52:05,453 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:52:05,454 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:52:05,454 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:52:05,463 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:52:05,463 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:52:05,474 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:52:05,475 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 16:52:05,476 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:52:05,476 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:52:05,477 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:52:05,477 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:52:05,758 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:52:05,880 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1361984318_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:52:05,880 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:52:05,880 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1361984318_0001_r_000000_0' done.
   [druid] 2018-12-06 16:52:05,880 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1361984318_0001_r_000000_0
   [druid] 2018-12-06 16:52:05,881 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:52:05,886 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:52:06,433 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:52:06,433 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1361984318_0001 completed successfully
   [druid] 2018-12-06 16:52:06,446 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1924294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:54:33,074 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:54:33,075 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:54:33,638 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:54:33,708 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:54:33,735 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:54:33,799 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1772409938_0001
   [druid] 2018-12-06 16:54:33,915 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:54:33,916 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1772409938_0001
   [druid] 2018-12-06 16:54:33,917 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:54:33,922 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:54:33,923 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:54:33,927 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:54:33,960 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:54:33,962 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1772409938_0001_m_000000_0
   [druid] 2018-12-06 16:54:33,981 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:54:33,985 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:54:34,058 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@67731643
   [druid] 2018-12-06 16:54:34,065 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 16:54:34,109 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:54:34,109 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:54:34,110 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:54:34,110 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:54:34,110 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:54:34,112 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:54:34,535 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,535 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,535 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,535 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,536 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,536 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,536 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,536 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,537 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-06 16:54:34,918 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1772409938_0001 running in uber mode : false
   [druid] 2018-12-06 16:54:34,923 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 16:54:39,994 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:54:40,931 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 27% reduce 0%
   [druid] 2018-12-06 16:54:42,995 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:54:43,932 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 45% reduce 0%
   [druid] 2018-12-06 16:54:45,996 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:54:46,855 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-06 16:54:46,857 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:54:46,857 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:54:46,857 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1006; bufvoid = 104857600
   [druid] 2018-12-06 16:54:46,857 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214368(104857472); length = 29/6553600
   [druid] 2018-12-06 16:54:46,933 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 62% reduce 0%
   [druid] 2018-12-06 16:54:46,965 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:54:46,971 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1772409938_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:54:46,972 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:54:46,972 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1772409938_0001_m_000000_0' done.
   [druid] 2018-12-06 16:54:46,972 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1772409938_0001_m_000000_0
   [druid] 2018-12-06 16:54:46,972 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:54:46,974 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:54:46,974 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1772409938_0001_r_000000_0
   [druid] 2018-12-06 16:54:46,980 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:54:46,980 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:54:47,056 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2ea5a956
   [druid] 2018-12-06 16:54:47,060 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7db60429
   [druid] 2018-12-06 16:54:47,073 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:54:47,075 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1772409938_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:54:47,105 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1772409938_0001_m_000000_0 decomp: 1024 len: 1028 to MEMORY
   [druid] 2018-12-06 16:54:47,108 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1024 bytes from map-output for attempt_local1772409938_0001_m_000000_0
   [druid] 2018-12-06 16:54:47,110 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1024, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1024
   [druid] 2018-12-06 16:54:47,111 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:54:47,112 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:54:47,112 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:54:47,122 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:54:47,123 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 16:54:47,124 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1024 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:54:47,125 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1028 bytes from disk
   [druid] 2018-12-06 16:54:47,125 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:54:47,125 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:54:47,126 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 944 bytes
   [druid] 2018-12-06 16:54:47,127 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:54:47,141 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:54:47,215 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1772409938_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:54:47,216 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:54:47,216 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1772409938_0001_r_000000_0' done.
   [druid] 2018-12-06 16:54:47,216 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1772409938_0001_r_000000_0
   [druid] 2018-12-06 16:54:47,216 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:54:47,220 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:54:47,933 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:54:47,933 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1772409938_0001 completed successfully
   [druid] 2018-12-06 16:54:47,945 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=2408
		FILE: Number of bytes written=609356
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=8
		Map output bytes=1006
		Map output materialized bytes=1028
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=3
		Reduce shuffle bytes=1028
		Reduce input records=8
		Reduce output records=3
		Spilled Records=16
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=119
		Total committed heap usage (bytes)=776994816
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 16:56:16,494 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 16:56:16,495 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 16:56:17,080 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 16:56:17,155 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 16:56:17,183 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 16:56:17,249 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1986844215_0001
   [druid] 2018-12-06 16:56:17,369 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 16:56:17,370 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1986844215_0001
   [druid] 2018-12-06 16:56:17,371 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 16:56:17,376 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:56:17,376 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 16:56:17,380 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 16:56:17,408 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 16:56:17,410 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1986844215_0001_m_000000_0
   [druid] 2018-12-06 16:56:17,428 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:56:17,431 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:56:17,500 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-06 16:56:17,507 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 16:56:17,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 16:56:17,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 16:56:17,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 16:56:17,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 16:56:17,551 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 16:56:17,555 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 16:56:17,982 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,982 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,983 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,983 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,984 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,984 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,984 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,985 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:17,985 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 16:56:18,086 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 16:56:18,088 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 16:56:18,088 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 16:56:18,088 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 16:56:18,088 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 16:56:18,230 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 16:56:18,241 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1986844215_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:56:18,252 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 16:56:18,252 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1986844215_0001_m_000000_0' done.
   [druid] 2018-12-06 16:56:18,252 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1986844215_0001_m_000000_0
   [druid] 2018-12-06 16:56:18,252 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 16:56:18,255 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 16:56:18,255 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1986844215_0001_r_000000_0
   [druid] 2018-12-06 16:56:18,263 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 16:56:18,263 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 16:56:18,351 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4473d0e1
   [druid] 2018-12-06 16:56:18,354 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@359c2777
   [druid] 2018-12-06 16:56:18,363 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 16:56:18,365 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1986844215_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 16:56:18,371 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1986844215_0001 running in uber mode : false
   [druid] 2018-12-06 16:56:18,372 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 16:56:18,387 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1986844215_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 16:56:18,391 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local1986844215_0001_m_000000_0
   [druid] 2018-12-06 16:56:18,392 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 16:56:18,393 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 16:56:18,393 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:56:18,394 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 16:56:18,402 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:56:18,403 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:56:18,412 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 16:56:18,413 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 16:56:18,414 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 16:56:18,414 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 16:56:18,415 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 16:56:18,415 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 16:56:18,672 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 16:56:18,785 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1986844215_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 16:56:18,786 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 16:56:18,786 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1986844215_0001_r_000000_0' done.
   [druid] 2018-12-06 16:56:18,786 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1986844215_0001_r_000000_0
   [druid] 2018-12-06 16:56:18,786 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 16:56:18,791 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 16:56:19,373 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 16:56:19,373 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1986844215_0001 completed successfully
   [druid] 2018-12-06 16:56:19,383 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1924294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:01:05,439 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:01:05,440 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:01:06,103 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:01:06,174 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:01:06,202 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:01:06,261 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1702638348_0001
   [druid] 2018-12-06 17:01:06,384 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:01:06,385 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1702638348_0001
   [druid] 2018-12-06 17:01:06,386 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:01:06,391 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:01:06,392 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:01:06,397 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:01:06,427 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:01:06,429 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1702638348_0001_m_000000_0
   [druid] 2018-12-06 17:01:06,449 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:01:06,453 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:01:06,525 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ac1d3
   [druid] 2018-12-06 17:01:06,532 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 17:01:06,582 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:01:06,582 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:01:06,582 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:01:06,582 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:01:06,582 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:01:06,585 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:01:07,036 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,037 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,037 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,038 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,038 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,038 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,039 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,039 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,039 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:01:07,146 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:01:07,147 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:01:07,147 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:01:07,147 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 17:01:07,148 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 17:01:07,196 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:01:07,303 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1702638348_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:01:07,312 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:01:07,312 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1702638348_0001_m_000000_0' done.
   [druid] 2018-12-06 17:01:07,312 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1702638348_0001_m_000000_0
   [druid] 2018-12-06 17:01:07,313 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:01:07,314 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:01:07,314 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1702638348_0001_r_000000_0
   [druid] 2018-12-06 17:01:07,319 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:01:07,319 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:01:07,387 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1702638348_0001 running in uber mode : false
   [druid] 2018-12-06 17:01:07,388 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:01:07,390 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4473d0e1
   [druid] 2018-12-06 17:01:07,392 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@359c2777
   [druid] 2018-12-06 17:01:07,403 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:01:07,404 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1702638348_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:01:07,430 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1702638348_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 17:01:07,436 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local1702638348_0001_m_000000_0
   [druid] 2018-12-06 17:01:07,437 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 17:01:07,438 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:01:07,439 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:01:07,439 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:01:07,448 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:01:07,448 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:01:07,458 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:01:07,460 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 17:01:07,460 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:01:07,460 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:01:07,461 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:01:07,461 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:01:07,745 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:01:07,790 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,791 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,808 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,811 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,812 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,817 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,817 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,822 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,823 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,829 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,830 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,836 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,836 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,840 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:98)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:01:07,844 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1702638348_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:01:07,844 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:01:07,844 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1702638348_0001_r_000000_0' done.
   [druid] 2018-12-06 17:01:07,845 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1702638348_0001_r_000000_0
   [druid] 2018-12-06 17:01:07,845 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:01:07,851 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:01:08,390 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:01:08,390 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1702638348_0001 completed successfully
   [druid] 2018-12-06 17:01:08,406 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1924294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:06:06,598 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:06:06,599 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:06:07,202 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:06:07,260 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:06:07,299 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:06:07,379 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2036812351_0001
   [druid] 2018-12-06 17:06:07,624 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:06:07,626 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2036812351_0001
   [druid] 2018-12-06 17:06:07,629 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:06:07,635 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:06:07,635 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:06:07,642 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:06:07,675 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:06:07,677 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2036812351_0001_m_000000_0
   [druid] 2018-12-06 17:06:07,703 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:06:07,708 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:06:07,777 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@20dc46a
   [druid] 2018-12-06 17:06:07,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 17:06:07,837 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:06:07,838 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:06:07,838 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:06:07,838 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:06:07,838 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:06:07,842 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:06:08,295 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,295 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,296 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,296 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,296 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,297 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,297 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,298 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,298 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:06:08,422 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:06:08,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:06:08,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:06:08,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 17:06:08,427 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 17:06:08,565 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:06:08,574 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2036812351_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:06:08,589 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:06:08,589 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2036812351_0001_m_000000_0' done.
   [druid] 2018-12-06 17:06:08,589 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2036812351_0001_m_000000_0
   [druid] 2018-12-06 17:06:08,589 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:06:08,592 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:06:08,592 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2036812351_0001_r_000000_0
   [druid] 2018-12-06 17:06:08,603 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:06:08,603 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:06:08,629 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2036812351_0001 running in uber mode : false
   [druid] 2018-12-06 17:06:08,631 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:06:08,677 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5d4e12d0
   [druid] 2018-12-06 17:06:08,680 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4301761c
   [druid] 2018-12-06 17:06:08,694 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:06:08,697 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2036812351_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:06:08,727 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2036812351_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 17:06:08,733 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local2036812351_0001_m_000000_0
   [druid] 2018-12-06 17:06:08,735 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 17:06:08,737 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:06:08,738 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:06:08,738 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:06:08,748 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:06:08,748 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:06:08,756 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:06:08,757 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 17:06:08,758 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:06:08,758 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:06:08,759 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:06:08,759 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:06:09,036 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:07:04,630 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:07:07,998 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 82%
   [druid] 2018-12-06 17:09:03,787 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:09:03,788 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:09:04,409 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:09:04,481 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:09:04,509 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:09:04,574 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local524593658_0001
   [druid] 2018-12-06 17:09:04,687 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:09:04,688 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local524593658_0001
   [druid] 2018-12-06 17:09:04,689 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:09:04,694 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:09:04,694 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:09:04,699 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:09:04,730 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:09:04,732 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local524593658_0001_m_000000_0
   [druid] 2018-12-06 17:09:04,749 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:09:04,754 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:09:04,828 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77802544
   [druid] 2018-12-06 17:09:04,834 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 17:09:04,883 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:09:04,883 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:09:04,883 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:09:04,883 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:09:04,883 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:09:04,886 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:09:05,334 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,335 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,335 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,336 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,336 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,336 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,337 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,337 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,338 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:09:05,444 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:09:05,446 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:09:05,446 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:09:05,446 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 17:09:05,446 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 17:09:05,489 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:09:05,494 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local524593658_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:09:05,501 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:09:05,501 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local524593658_0001_m_000000_0' done.
   [druid] 2018-12-06 17:09:05,501 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local524593658_0001_m_000000_0
   [druid] 2018-12-06 17:09:05,501 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:09:05,503 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:09:05,503 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local524593658_0001_r_000000_0
   [druid] 2018-12-06 17:09:05,508 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:09:05,508 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:09:05,578 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6c0996ab
   [druid] 2018-12-06 17:09:05,581 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60255ff
   [druid] 2018-12-06 17:09:05,593 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:09:05,595 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local524593658_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:09:05,622 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local524593658_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 17:09:05,626 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local524593658_0001_m_000000_0
   [druid] 2018-12-06 17:09:05,628 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 17:09:05,629 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:09:05,629 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:09:05,630 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:09:05,641 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:09:05,641 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:09:05,690 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local524593658_0001 running in uber mode : false
   [druid] 2018-12-06 17:09:05,691 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:09:05,748 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:09:05,749 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 17:09:05,749 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:09:05,749 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:09:05,750 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:09:05,750 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:09:06,004 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:09:06,115 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local524593658_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:09:06,116 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:09:06,116 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local524593658_0001_r_000000_0' done.
   [druid] 2018-12-06 17:09:06,116 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local524593658_0001_r_000000_0
   [druid] 2018-12-06 17:09:06,116 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:09:06,121 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:09:06,691 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:09:06,691 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local524593658_0001 completed successfully
   [druid] 2018-12-06 17:09:06,703 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1921202
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:12:28,559 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:12:28,560 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:12:29,137 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:12:29,210 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:12:29,240 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:12:29,305 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1854883568_0001
   [druid] 2018-12-06 17:12:29,423 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:12:29,424 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1854883568_0001
   [druid] 2018-12-06 17:12:29,425 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:12:29,429 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:12:29,429 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:12:29,433 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:12:29,464 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:12:29,465 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1854883568_0001_m_000000_0
   [druid] 2018-12-06 17:12:29,483 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:12:29,487 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:12:29,562 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-06 17:12:29,569 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 17:12:29,616 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:12:29,616 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:12:29,616 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:12:29,617 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:12:29,617 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:12:29,619 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:12:30,069 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,070 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,070 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,070 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,071 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,071 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,071 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,072 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,072 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:12:30,184 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:12:30,186 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:12:30,186 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:12:30,187 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 432972; bufvoid = 104857600
   [druid] 2018-12-06 17:12:30,187 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-06 17:12:30,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:12:30,328 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1854883568_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:12:30,340 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:12:30,340 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1854883568_0001_m_000000_0' done.
   [druid] 2018-12-06 17:12:30,340 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1854883568_0001_m_000000_0
   [druid] 2018-12-06 17:12:30,341 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:12:30,342 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:12:30,343 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1854883568_0001_r_000000_0
   [druid] 2018-12-06 17:12:30,349 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:12:30,349 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:12:30,427 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1854883568_0001 running in uber mode : false
   [druid] 2018-12-06 17:12:30,428 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:12:30,431 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@f2cff5c
   [druid] 2018-12-06 17:12:30,433 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@46ce90e9
   [druid] 2018-12-06 17:12:30,445 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:12:30,446 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1854883568_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:12:30,472 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1854883568_0001_m_000000_0 decomp: 439298 len: 439302 to MEMORY
   [druid] 2018-12-06 17:12:30,477 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 439298 bytes from map-output for attempt_local1854883568_0001_m_000000_0
   [druid] 2018-12-06 17:12:30,477 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 439298, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->439298
   [druid] 2018-12-06 17:12:30,478 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:12:30,479 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:12:30,479 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:12:30,487 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:12:30,488 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:12:30,498 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 439298 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:12:30,499 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 439302 bytes from disk
   [druid] 2018-12-06 17:12:30,500 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:12:30,500 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:12:30,501 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 439219 bytes
   [druid] 2018-12-06 17:12:30,501 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:12:30,756 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:12:30,806 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,807 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,818 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,821 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,824 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,829 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,829 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,835 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,836 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,837 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,839 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,839 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,840 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,842 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:12:30,846 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1854883568_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:12:30,847 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:12:30,847 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1854883568_0001_r_000000_0' done.
   [druid] 2018-12-06 17:12:30,847 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1854883568_0001_r_000000_0
   [druid] 2018-12-06 17:12:30,847 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:12:30,852 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:12:31,430 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:12:31,430 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1854883568_0001 completed successfully
   [druid] 2018-12-06 17:12:31,440 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=878956
		FILE: Number of bytes written=1924294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=432972
		Map output materialized bytes=439302
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=439302
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:17:16,898 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:17:16,899 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:17:17,482 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:17:17,556 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:17:17,592 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:17:17,658 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local356465797_0001
   [druid] 2018-12-06 17:17:17,779 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:17:17,780 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local356465797_0001
   [druid] 2018-12-06 17:17:17,781 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:17:17,786 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:17:17,786 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:17:17,792 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:17:17,820 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:17:17,821 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local356465797_0001_m_000000_0
   [druid] 2018-12-06 17:17:17,841 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:17:17,844 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:17:17,917 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@219664a3
   [druid] 2018-12-06 17:17:17,924 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 17:17:17,971 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:17:17,971 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:17:17,971 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:17:17,971 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:17:17,971 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:17:17,975 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:17:18,407 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,407 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,407 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,407 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,408 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,408 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,408 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,408 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,408 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,409 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,409 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,409 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,409 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,409 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:17:18,426 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:17:18,428 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:17:18,428 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:17:18,428 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 14098; bufvoid = 104857600
   [druid] 2018-12-06 17:17:18,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213992(104855968); length = 405/6553600
   [druid] 2018-12-06 17:17:18,441 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:17:18,461 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local356465797_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:17:18,469 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:17:18,469 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local356465797_0001_m_000000_0' done.
   [druid] 2018-12-06 17:17:18,469 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local356465797_0001_m_000000_0
   [druid] 2018-12-06 17:17:18,469 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:17:18,471 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:17:18,471 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local356465797_0001_r_000000_0
   [druid] 2018-12-06 17:17:18,477 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:17:18,477 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:17:18,548 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63d98ba4
   [druid] 2018-12-06 17:17:18,551 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1cb7f9bb
   [druid] 2018-12-06 17:17:18,567 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:17:18,569 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local356465797_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:17:18,594 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local356465797_0001_m_000000_0 decomp: 14304 len: 14308 to MEMORY
   [druid] 2018-12-06 17:17:18,599 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 14304 bytes from map-output for attempt_local356465797_0001_m_000000_0
   [druid] 2018-12-06 17:17:18,601 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 14304, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->14304
   [druid] 2018-12-06 17:17:18,602 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:17:18,603 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:17:18,603 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:17:18,614 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:17:18,614 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 14225 bytes
   [druid] 2018-12-06 17:17:18,617 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 14304 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:17:18,618 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 14308 bytes from disk
   [druid] 2018-12-06 17:17:18,618 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:17:18,618 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:17:18,619 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 14225 bytes
   [druid] 2018-12-06 17:17:18,620 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:17:18,781 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local356465797_0001 running in uber mode : false
   [druid] 2018-12-06 17:17:18,782 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:17:18,891 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:17:18,932 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,934 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,938 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,938 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,943 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,945 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:17:18,949 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local356465797_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:17:18,949 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:17:18,949 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local356465797_0001_r_000000_0' done.
   [druid] 2018-12-06 17:17:18,950 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local356465797_0001_r_000000_0
   [druid] 2018-12-06 17:17:18,950 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:17:18,955 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:17:19,782 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:17:19,782 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local356465797_0001 completed successfully
   [druid] 2018-12-06 17:17:19,792 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=28966
		FILE: Number of bytes written=646222
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=102
		Map output bytes=14098
		Map output materialized bytes=14308
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=14308
		Reduce input records=102
		Reduce output records=6
		Spilled Records=204
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=598736896
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:18:55,967 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:18:55,969 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:18:56,624 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:18:56,691 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:18:56,728 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:18:56,791 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2133038204_0001
   [druid] 2018-12-06 17:18:56,912 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:18:56,912 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2133038204_0001
   [druid] 2018-12-06 17:18:56,913 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:18:56,918 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:18:56,918 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:18:56,922 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:18:56,954 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:18:56,955 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2133038204_0001_m_000000_0
   [druid] 2018-12-06 17:18:56,979 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:18:56,983 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:18:57,145 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-06 17:18:57,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 17:18:57,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:18:57,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:18:57,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:18:57,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:18:57,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:18:57,201 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:18:57,609 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,609 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,609 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,609 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,610 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,610 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,610 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,610 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,610 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:18:57,621 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:18:57,623 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:18:57,623 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:18:57,623 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3300; bufvoid = 104857600
   [druid] 2018-12-06 17:18:57,623 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-06 17:18:57,726 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:18:57,732 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2133038204_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:18:57,739 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:18:57,739 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2133038204_0001_m_000000_0' done.
   [druid] 2018-12-06 17:18:57,739 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2133038204_0001_m_000000_0
   [druid] 2018-12-06 17:18:57,739 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:18:57,741 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:18:57,741 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2133038204_0001_r_000000_0
   [druid] 2018-12-06 17:18:57,746 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:18:57,746 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:18:57,818 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@43426788
   [druid] 2018-12-06 17:18:57,821 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5ba2e29b
   [druid] 2018-12-06 17:18:57,834 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:18:57,839 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2133038204_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:18:57,870 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2133038204_0001_m_000000_0 decomp: 3350 len: 3354 to MEMORY
   [druid] 2018-12-06 17:18:57,874 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3350 bytes from map-output for attempt_local2133038204_0001_m_000000_0
   [druid] 2018-12-06 17:18:57,876 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3350, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3350
   [druid] 2018-12-06 17:18:57,876 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:18:57,878 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:18:57,878 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:18:57,887 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:18:57,887 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3271 bytes
   [druid] 2018-12-06 17:18:57,889 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3350 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:18:57,890 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3354 bytes from disk
   [druid] 2018-12-06 17:18:57,891 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:18:57,891 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:18:57,892 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3271 bytes
   [druid] 2018-12-06 17:18:57,892 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:18:57,914 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2133038204_0001 running in uber mode : false
   [druid] 2018-12-06 17:18:57,915 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:18:58,139 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:18:58,166 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:18:58,169 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:18:58,172 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:18:58,174 [pool-6-thread-1] WARN  r.HourlyActiveUserOutputWriter {1} - ps赋值失败
   java.sql.SQLException: Parameter index out of range (1 > number of parameters, which is 0).
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1094)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:997)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:983)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:928)
	at com.mysql.jdbc.PreparedStatement.checkBounds(PreparedStatement.java:3688)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3670)
	at com.mysql.jdbc.PreparedStatement.setInternal(PreparedStatement.java:3715)
	at com.mysql.jdbc.PreparedStatement.setInt(PreparedStatement.java:3659)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserOutputWriter.output(HourlyActiveUserOutputWriter.java:29)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:55)
	at com.phone.analysis.mapreduce.HourlyActiveUser.HourlyActiveUserReducer.reduce(HourlyActiveUserReducer.java:25)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 17:18:58,178 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2133038204_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:18:58,179 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:18:58,179 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2133038204_0001_r_000000_0' done.
   [druid] 2018-12-06 17:18:58,179 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2133038204_0001_r_000000_0
   [druid] 2018-12-06 17:18:58,179 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:18:58,185 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:18:58,917 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:18:58,917 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2133038204_0001 completed successfully
   [druid] 2018-12-06 17:18:58,927 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=7058
		FILE: Number of bytes written=616452
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=24
		Map output bytes=3300
		Map output materialized bytes=3354
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3354
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=2
		Total committed heap usage (bytes)=578813952
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:19:54,746 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:19:54,748 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:19:55,327 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:19:55,393 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:19:55,419 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:19:55,480 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local488995902_0001
   [druid] 2018-12-06 17:19:55,597 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:19:55,597 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local488995902_0001
   [druid] 2018-12-06 17:19:55,598 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:19:55,602 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:19:55,602 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:19:55,606 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:19:55,637 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:19:55,638 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local488995902_0001_m_000000_0
   [druid] 2018-12-06 17:19:55,658 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:19:55,662 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:19:55,731 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@46a7fef8
   [druid] 2018-12-06 17:19:55,737 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 17:19:55,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:19:55,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:19:55,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:19:55,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:19:55,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:19:55,787 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:19:56,189 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,189 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,189 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,190 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,190 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,190 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,190 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,190 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,191 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:19:56,202 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:19:56,203 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:19:56,203 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:19:56,203 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3300; bufvoid = 104857600
   [druid] 2018-12-06 17:19:56,203 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-06 17:19:56,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:19:56,319 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local488995902_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:19:56,327 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:19:56,327 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local488995902_0001_m_000000_0' done.
   [druid] 2018-12-06 17:19:56,327 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local488995902_0001_m_000000_0
   [druid] 2018-12-06 17:19:56,328 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:19:56,330 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:19:56,330 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local488995902_0001_r_000000_0
   [druid] 2018-12-06 17:19:56,335 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:19:56,335 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:19:56,405 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3dea0a4a
   [druid] 2018-12-06 17:19:56,409 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@659105a4
   [druid] 2018-12-06 17:19:56,423 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:19:56,429 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local488995902_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:19:56,466 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local488995902_0001_m_000000_0 decomp: 3350 len: 3354 to MEMORY
   [druid] 2018-12-06 17:19:56,472 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3350 bytes from map-output for attempt_local488995902_0001_m_000000_0
   [druid] 2018-12-06 17:19:56,474 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3350, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3350
   [druid] 2018-12-06 17:19:56,475 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:19:56,476 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:19:56,476 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:19:56,488 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:19:56,489 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3271 bytes
   [druid] 2018-12-06 17:19:56,491 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3350 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:19:56,492 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3354 bytes from disk
   [druid] 2018-12-06 17:19:56,492 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:19:56,493 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:19:56,493 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3271 bytes
   [druid] 2018-12-06 17:19:56,494 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:19:56,599 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local488995902_0001 running in uber mode : false
   [druid] 2018-12-06 17:19:56,600 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:19:56,764 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:19:56,835 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local488995902_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:19:56,835 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:19:56,835 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local488995902_0001_r_000000_0' done.
   [druid] 2018-12-06 17:19:56,836 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local488995902_0001_r_000000_0
   [druid] 2018-12-06 17:19:56,836 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:19:56,840 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:19:57,601 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:19:57,601 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local488995902_0001 completed successfully
   [druid] 2018-12-06 17:19:57,611 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=7058
		FILE: Number of bytes written=613360
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=24
		Map output bytes=3300
		Map output materialized bytes=3354
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3354
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=600309760
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 17:21:17,534 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 17:21:17,536 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 17:21:18,125 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 17:21:18,198 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 17:21:18,228 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 17:21:18,291 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1666589926_0001
   [druid] 2018-12-06 17:21:18,405 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 17:21:18,406 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1666589926_0001
   [druid] 2018-12-06 17:21:18,407 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 17:21:18,413 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:21:18,413 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 17:21:18,418 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 17:21:18,449 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 17:21:18,451 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1666589926_0001_m_000000_0
   [druid] 2018-12-06 17:21:18,472 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:21:18,475 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:21:18,549 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@67731643
   [druid] 2018-12-06 17:21:18,555 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/30/part-m-00000:0+27156
   [druid] 2018-12-06 17:21:18,600 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 17:21:18,600 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 17:21:18,601 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 17:21:18,601 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 17:21:18,601 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 17:21:18,603 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 17:21:19,012 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,012 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,013 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,014 [ask Executor #0] INFO  iveUser.HourlyActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-06 17:21:19,024 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 17:21:19,025 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 17:21:19,025 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 17:21:19,025 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1500; bufvoid = 104857600
   [druid] 2018-12-06 17:21:19,025 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214352(104857408); length = 45/6553600
   [druid] 2018-12-06 17:21:19,135 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 17:21:19,144 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1666589926_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:21:19,151 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 17:21:19,151 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1666589926_0001_m_000000_0' done.
   [druid] 2018-12-06 17:21:19,152 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1666589926_0001_m_000000_0
   [druid] 2018-12-06 17:21:19,152 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 17:21:19,153 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 17:21:19,154 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1666589926_0001_r_000000_0
   [druid] 2018-12-06 17:21:19,158 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 17:21:19,158 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 17:21:19,232 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@364d63e3
   [druid] 2018-12-06 17:21:19,236 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60f2a35a
   [druid] 2018-12-06 17:21:19,249 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 17:21:19,251 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1666589926_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 17:21:19,285 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1666589926_0001_m_000000_0 decomp: 1526 len: 1530 to MEMORY
   [druid] 2018-12-06 17:21:19,290 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 1526 bytes from map-output for attempt_local1666589926_0001_m_000000_0
   [druid] 2018-12-06 17:21:19,291 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 1526, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->1526
   [druid] 2018-12-06 17:21:19,292 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 17:21:19,293 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:21:19,293 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 17:21:19,303 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:21:19,303 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 1444 bytes
   [druid] 2018-12-06 17:21:19,306 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 1526 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 17:21:19,307 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 1530 bytes from disk
   [druid] 2018-12-06 17:21:19,307 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 17:21:19,307 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 17:21:19,308 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 1444 bytes
   [druid] 2018-12-06 17:21:19,309 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 17:21:19,409 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1666589926_0001 running in uber mode : false
   [druid] 2018-12-06 17:21:19,410 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 17:21:19,553 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 17:21:19,621 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1666589926_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 17:21:19,621 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 17:21:19,621 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1666589926_0001_r_000000_0' done.
   [druid] 2018-12-06 17:21:19,621 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1666589926_0001_r_000000_0
   [druid] 2018-12-06 17:21:19,621 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 17:21:19,626 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 17:21:20,410 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 17:21:20,410 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1666589926_0001 completed successfully
   [druid] 2018-12-06 17:21:20,420 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=3410
		FILE: Number of bytes written=610980
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=54312
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=65
		Map output records=12
		Map output bytes=1500
		Map output materialized bytes=1530
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=1530
		Reduce input records=12
		Reduce output records=1
		Spilled Records=24
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=597164032
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=27156
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 22:59:55,719 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 0 time(s); maxRetries=45
   [druid] 2018-12-06 23:00:16,383 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 1 time(s); maxRetries=45
   [druid] 2018-12-06 23:00:36,389 [main           ] INFO  org.apache.hadoop.ipc.Client   {1} - Retrying connect to server: Murphy/192.168.44.150:8020. Already tried 2 time(s); maxRetries=45
   [druid] 2018-12-06 23:01:27,287 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:01:27,290 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:01:29,051 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:01:29,253 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:01:29,341 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:01:29,502 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1747291212_0001
   [druid] 2018-12-06 23:01:29,840 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:01:29,841 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1747291212_0001
   [druid] 2018-12-06 23:01:29,844 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:01:29,850 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:01:29,850 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:01:29,866 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:01:29,918 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:01:29,918 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1747291212_0001_m_000000_0
   [druid] 2018-12-06 23:01:29,972 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:01:29,978 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:01:30,847 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1747291212_0001 running in uber mode : false
   [druid] 2018-12-06 23:01:30,853 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:01:33,434 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5b7f33dc
   [druid] 2018-12-06 23:01:33,447 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:01:33,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:01:33,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:01:33,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:01:33,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:01:33,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:01:33,558 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:01:34,586 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:01:34,590 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:01:34,590 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:01:34,590 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:01:34,590 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:01:34,728 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:01:34,739 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1747291212_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:01:34,757 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:01:34,757 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1747291212_0001_m_000000_0' done.
   [druid] 2018-12-06 23:01:34,758 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1747291212_0001_m_000000_0
   [druid] 2018-12-06 23:01:34,758 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:01:34,760 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:01:34,762 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1747291212_0001_r_000000_0
   [druid] 2018-12-06 23:01:34,772 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:01:34,773 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:01:34,861 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:01:35,189 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7f479016
   [druid] 2018-12-06 23:01:35,194 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@a9aadfc
   [druid] 2018-12-06 23:01:35,224 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:01:35,227 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1747291212_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:01:35,277 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1747291212_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:01:35,288 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local1747291212_0001_m_000000_0
   [druid] 2018-12-06 23:01:35,290 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:01:35,292 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:01:35,293 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:01:35,294 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:01:35,306 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:01:35,310 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:01:35,318 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:01:35,320 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:01:35,320 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:01:35,321 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:01:35,323 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:01:35,323 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:01:35,916 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:01:35,957 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1747291212_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:01:35,959 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 23:01:35,959 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1747291212_0001_r_000000_0' done.
   [druid] 2018-12-06 23:01:35,959 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1747291212_0001_r_000000_0
   [druid] 2018-12-06 23:01:35,960 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:01:35,970 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 23:01:36,866 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 23:01:36,866 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1747291212_0001 completed successfully
   [druid] 2018-12-06 23:01:36,891 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=345978
		FILE: Number of bytes written=1124859
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=172813
		Reduce input records=963
		Reduce output records=1
		Spilled Records=1926
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=588251136
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:04:09,779 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:04:09,783 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:04:11,003 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:04:11,157 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:04:11,208 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:04:11,379 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2109010074_0001
   [druid] 2018-12-06 23:04:11,652 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:04:11,653 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2109010074_0001
   [druid] 2018-12-06 23:04:11,657 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:04:11,669 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:04:11,669 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:04:11,683 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:04:11,742 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:04:11,744 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2109010074_0001_m_000000_0
   [druid] 2018-12-06 23:04:11,780 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:04:11,788 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:04:11,924 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3f2b179f
   [druid] 2018-12-06 23:04:11,935 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:04:12,021 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:04:12,021 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:04:12,021 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:04:12,021 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:04:12,021 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:04:12,026 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:04:12,663 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2109010074_0001 running in uber mode : false
   [druid] 2018-12-06 23:04:12,665 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:04:13,042 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:04:13,047 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:04:13,047 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:04:13,047 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:04:13,047 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:04:13,186 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:04:13,199 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2109010074_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:04:13,215 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:04:13,215 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2109010074_0001_m_000000_0' done.
   [druid] 2018-12-06 23:04:13,216 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2109010074_0001_m_000000_0
   [druid] 2018-12-06 23:04:13,216 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:04:13,218 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:04:13,220 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2109010074_0001_r_000000_0
   [druid] 2018-12-06 23:04:13,230 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:04:13,230 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:04:13,376 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1364ecda
   [druid] 2018-12-06 23:04:13,382 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@29958603
   [druid] 2018-12-06 23:04:13,402 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:04:13,409 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2109010074_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:04:13,461 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2109010074_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:04:13,468 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local2109010074_0001_m_000000_0
   [druid] 2018-12-06 23:04:13,470 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:04:13,473 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:04:13,474 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:04:13,474 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:04:13,489 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:04:13,489 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:04:13,500 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:04:13,502 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:04:13,503 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:04:13,503 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:04:13,504 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:04:13,505 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:04:13,670 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:04:13,957 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:04:14,009 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:04:14,018 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:04:14,018 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local2109010074_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:97)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:04:14,674 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2109010074_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:04:14,696 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=476033
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=7
		Total committed heap usage (bytes)=316145664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:05:39,137 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:05:39,138 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:05:40,240 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:05:40,372 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:05:40,420 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:05:40,539 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1880447722_0001
   [druid] 2018-12-06 23:05:40,740 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:05:40,742 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1880447722_0001
   [druid] 2018-12-06 23:05:40,745 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:05:40,754 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:05:40,754 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:05:40,765 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:05:40,815 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:05:40,816 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1880447722_0001_m_000000_0
   [druid] 2018-12-06 23:05:40,853 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:05:40,861 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:05:40,993 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77802544
   [druid] 2018-12-06 23:05:41,008 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:05:41,105 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:05:41,105 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:05:41,105 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:05:41,105 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:05:41,105 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:05:41,112 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:05:41,749 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1880447722_0001 running in uber mode : false
   [druid] 2018-12-06 23:05:41,751 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:05:42,060 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:05:42,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:05:42,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:05:42,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:05:42,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:05:42,186 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:05:42,196 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1880447722_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:05:42,210 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:05:42,210 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1880447722_0001_m_000000_0' done.
   [druid] 2018-12-06 23:05:42,210 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1880447722_0001_m_000000_0
   [druid] 2018-12-06 23:05:42,210 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:05:42,213 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:05:42,215 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1880447722_0001_r_000000_0
   [druid] 2018-12-06 23:05:42,226 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:05:42,226 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:05:42,368 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4dbda9f0
   [druid] 2018-12-06 23:05:42,374 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f0e1702
   [druid] 2018-12-06 23:05:42,396 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:05:42,399 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1880447722_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:05:42,445 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1880447722_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:05:42,453 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local1880447722_0001_m_000000_0
   [druid] 2018-12-06 23:05:42,455 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:05:42,457 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:05:42,458 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:05:42,459 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:05:42,472 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:05:42,472 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:05:42,485 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:05:42,488 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:05:42,488 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:05:42,489 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:05:42,490 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:05:42,490 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:05:42,755 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:05:42,920 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:05:43,154 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:05:43,161 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:05:43,163 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1880447722_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.excuteSql(IDimensionImpl.java:63)
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.getIDimensionImplByDimension(IDimensionImpl.java:54)
	at com.phone.analysis.mapreduce.LocationInfo.LocationOutputWriter.output(LocationOutputWriter.java:37)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:05:43,758 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1880447722_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:05:43,780 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=476075
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=316145664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:07:10,633 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:07:10,635 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:07:11,827 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:07:11,949 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:07:12,017 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:07:12,174 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1659493144_0001
   [druid] 2018-12-06 23:07:12,438 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:07:12,441 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1659493144_0001
   [druid] 2018-12-06 23:07:12,444 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:07:12,455 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:07:12,456 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:07:12,469 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:07:12,538 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:07:12,542 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1659493144_0001_m_000000_0
   [druid] 2018-12-06 23:07:12,594 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:07:12,606 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:07:12,744 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@255764ab
   [druid] 2018-12-06 23:07:12,761 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:07:12,855 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:07:12,855 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:07:12,855 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:07:12,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:07:12,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:07:12,865 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:07:13,446 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1659493144_0001 running in uber mode : false
   [druid] 2018-12-06 23:07:13,449 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:07:13,965 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:07:13,970 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:07:13,970 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:07:13,970 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:07:13,970 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:07:14,008 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:07:14,258 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1659493144_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:07:14,277 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:07:14,277 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1659493144_0001_m_000000_0' done.
   [druid] 2018-12-06 23:07:14,277 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1659493144_0001_m_000000_0
   [druid] 2018-12-06 23:07:14,278 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:07:14,282 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:07:14,283 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1659493144_0001_r_000000_0
   [druid] 2018-12-06 23:07:14,296 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:07:14,296 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:07:14,432 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5158eec3
   [druid] 2018-12-06 23:07:14,439 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49b9fbc6
   [druid] 2018-12-06 23:07:14,460 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:07:14,464 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:07:14,469 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1659493144_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:07:14,529 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1659493144_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:07:14,539 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local1659493144_0001_m_000000_0
   [druid] 2018-12-06 23:07:14,543 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:07:14,546 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:07:14,548 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:07:14,548 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:07:14,567 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:07:14,568 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:07:14,595 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:07:14,597 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:07:14,598 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:07:14,598 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:07:14,600 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:07:14,601 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:07:15,172 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:09:29,242 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:09:52,785 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:09:52,785 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:09:53,942 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:09:54,071 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:09:54,122 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:09:54,236 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local118715276_0001
   [druid] 2018-12-06 23:09:54,448 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:09:54,452 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local118715276_0001
   [druid] 2018-12-06 23:09:54,455 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:09:54,463 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:09:54,463 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:09:54,474 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:09:54,525 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:09:54,527 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local118715276_0001_m_000000_0
   [druid] 2018-12-06 23:09:54,572 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:09:54,578 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:09:54,709 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b26ba3e
   [druid] 2018-12-06 23:09:54,722 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:09:54,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:09:54,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:09:54,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:09:54,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:09:54,815 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:09:54,823 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:09:55,459 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local118715276_0001 running in uber mode : false
   [druid] 2018-12-06 23:09:55,461 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:09:55,801 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:09:55,803 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:09:55,804 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:09:55,804 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:09:55,804 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:09:55,948 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:09:55,960 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local118715276_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:09:55,975 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:09:55,975 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local118715276_0001_m_000000_0' done.
   [druid] 2018-12-06 23:09:55,975 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local118715276_0001_m_000000_0
   [druid] 2018-12-06 23:09:55,976 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:09:55,979 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:09:55,979 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local118715276_0001_r_000000_0
   [druid] 2018-12-06 23:09:55,990 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:09:55,990 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:09:56,133 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2567ad3f
   [druid] 2018-12-06 23:09:56,139 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@5c747725
   [druid] 2018-12-06 23:09:56,158 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:09:56,161 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local118715276_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:09:56,213 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local118715276_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:09:56,218 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local118715276_0001_m_000000_0
   [druid] 2018-12-06 23:09:56,224 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:09:56,227 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:09:56,228 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:09:56,228 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:09:56,245 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:09:56,245 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:09:56,256 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:09:56,257 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:09:56,258 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:09:56,258 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:09:56,260 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:09:56,260 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:09:56,467 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:09:56,705 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:09:56,817 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:09:56,825 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:09:56,826 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local118715276_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.excuteSql(IDimensionImpl.java:63)
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.getIDimensionImplByDimension(IDimensionImpl.java:54)
	at com.phone.analysis.mapreduce.LocationInfo.LocationOutputWriter.output(LocationOutputWriter.java:37)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:09:57,469 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local118715276_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:09:57,488 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=474529
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=316145664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:12:58,104 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:12:58,104 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:12:59,171 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:12:59,308 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:12:59,357 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:12:59,473 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1202133079_0001
   [druid] 2018-12-06 23:12:59,685 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:12:59,692 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1202133079_0001
   [druid] 2018-12-06 23:12:59,692 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:12:59,705 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:12:59,705 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:12:59,721 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:12:59,784 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:12:59,784 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1202133079_0001_m_000000_0
   [druid] 2018-12-06 23:12:59,824 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:12:59,825 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:12:59,965 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-06 23:12:59,978 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:13:00,051 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:13:00,051 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:13:00,051 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:13:00,051 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:13:00,051 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:13:00,074 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:13:00,703 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1202133079_0001 running in uber mode : false
   [druid] 2018-12-06 23:13:00,705 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:13:01,022 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:13:01,027 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:13:01,028 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:13:01,028 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:13:01,028 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:13:01,069 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:13:01,082 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1202133079_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:13:01,100 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:13:01,100 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1202133079_0001_m_000000_0' done.
   [druid] 2018-12-06 23:13:01,100 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1202133079_0001_m_000000_0
   [druid] 2018-12-06 23:13:01,100 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:13:01,103 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:13:01,104 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1202133079_0001_r_000000_0
   [druid] 2018-12-06 23:13:01,118 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:13:01,118 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:13:01,247 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4473d0e1
   [druid] 2018-12-06 23:13:01,252 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@359c2777
   [druid] 2018-12-06 23:13:01,269 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:13:01,275 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1202133079_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:13:01,332 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1202133079_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:13:01,342 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local1202133079_0001_m_000000_0
   [druid] 2018-12-06 23:13:01,344 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:13:01,347 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:13:01,348 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:13:01,349 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:13:01,362 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:13:01,366 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:13:01,374 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:13:01,376 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:13:01,376 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:13:01,376 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:13:01,379 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:13:01,379 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:13:01,709 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:13:01,816 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:13:01,925 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:13:01,936 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:13:01,937 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1202133079_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.excuteSql(IDimensionImpl.java:63)
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.getIDimensionImplByDimension(IDimensionImpl.java:54)
	at com.phone.analysis.mapreduce.LocationInfo.LocationOutputWriter.output(LocationOutputWriter.java:37)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:13:02,715 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1202133079_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:13:02,736 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=476075
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=315621376
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:14:22,739 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:14:22,744 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:14:23,775 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:14:23,896 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:14:23,945 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:14:24,064 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local212119051_0001
   [druid] 2018-12-06 23:14:24,271 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:14:24,274 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local212119051_0001
   [druid] 2018-12-06 23:14:24,275 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:14:24,286 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:14:24,286 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:14:24,297 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:14:24,350 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:14:24,351 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local212119051_0001_m_000000_0
   [druid] 2018-12-06 23:14:24,388 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:14:24,395 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:14:24,525 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4b105acb
   [druid] 2018-12-06 23:14:24,539 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:14:24,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:14:24,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:14:24,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:14:24,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:14:24,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:14:24,634 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:14:25,278 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local212119051_0001 running in uber mode : false
   [druid] 2018-12-06 23:14:25,281 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:14:25,622 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:14:25,627 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:14:25,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:14:25,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:14:25,628 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:14:25,753 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:14:25,768 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local212119051_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:14:25,780 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:14:25,780 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local212119051_0001_m_000000_0' done.
   [druid] 2018-12-06 23:14:25,780 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local212119051_0001_m_000000_0
   [druid] 2018-12-06 23:14:25,783 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:14:25,785 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:14:25,787 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local212119051_0001_r_000000_0
   [druid] 2018-12-06 23:14:25,799 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:14:25,799 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:14:25,938 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7ad67c0
   [druid] 2018-12-06 23:14:25,944 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@e767335
   [druid] 2018-12-06 23:14:25,963 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:14:25,967 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local212119051_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:14:26,015 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local212119051_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:14:26,020 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local212119051_0001_m_000000_0
   [druid] 2018-12-06 23:14:26,026 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:14:26,028 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:14:26,029 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:14:26,030 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:14:26,042 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:14:26,045 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:14:26,088 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:14:26,090 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:14:26,090 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:14:26,090 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:14:26,090 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:14:26,093 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:14:26,287 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:14:26,538 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:14:26,650 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:14:26,659 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:14:26,659 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local212119051_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.excuteSql(IDimensionImpl.java:63)
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.getIDimensionImplByDimension(IDimensionImpl.java:54)
	at com.phone.analysis.mapreduce.LocationInfo.LocationOutputWriter.output(LocationOutputWriter.java:37)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:14:27,287 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local212119051_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:14:27,308 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=474529
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=315621376
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:15:37,916 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:15:37,919 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:15:39,001 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:15:39,143 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:15:39,196 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:15:39,324 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local398592008_0001
   [druid] 2018-12-06 23:15:39,556 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:15:39,557 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local398592008_0001
   [druid] 2018-12-06 23:15:39,557 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:15:39,569 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:15:39,569 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:15:39,580 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:15:39,642 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:15:39,643 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local398592008_0001_m_000000_0
   [druid] 2018-12-06 23:15:39,680 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:15:39,687 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:15:39,818 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@219664a3
   [druid] 2018-12-06 23:15:39,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:15:39,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:15:39,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:15:39,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:15:39,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:15:39,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:15:39,929 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:15:40,566 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local398592008_0001 running in uber mode : false
   [druid] 2018-12-06 23:15:40,568 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:15:40,914 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:15:40,920 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:15:40,920 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:15:40,920 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:15:40,920 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:15:41,037 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:15:41,062 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local398592008_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:15:41,075 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:15:41,075 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local398592008_0001_m_000000_0' done.
   [druid] 2018-12-06 23:15:41,075 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local398592008_0001_m_000000_0
   [druid] 2018-12-06 23:15:41,077 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:15:41,080 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:15:41,080 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local398592008_0001_r_000000_0
   [druid] 2018-12-06 23:15:41,090 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:15:41,091 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:15:41,241 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@11119d62
   [druid] 2018-12-06 23:15:41,245 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@157be9f0
   [druid] 2018-12-06 23:15:41,264 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:15:41,268 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local398592008_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:15:41,322 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local398592008_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:15:41,330 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local398592008_0001_m_000000_0
   [druid] 2018-12-06 23:15:41,332 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:15:41,336 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:15:41,337 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:15:41,337 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:15:41,354 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:15:41,354 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:15:41,367 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:15:41,368 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:15:41,369 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:15:41,369 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:15:41,369 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:15:41,371 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:15:41,568 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:15:41,828 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:15:41,943 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:15:41,952 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-06 23:15:41,953 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local398592008_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.excuteSql(IDimensionImpl.java:63)
	at com.phone.analysis.mapreduce.services.subClass.IDimensionImpl.getIDimensionImplByDimension(IDimensionImpl.java:54)
	at com.phone.analysis.mapreduce.LocationInfo.LocationOutputWriter.output(LocationOutputWriter.java:37)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:99)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:58)
	at com.phone.analysis.mapreduce.LocationInfo.LocationInfoReducer.reduce(LocationInfoReducer.java:22)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-06 23:15:42,573 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local398592008_0001 failed with state FAILED due to: NA
   [druid] 2018-12-06 23:15:42,595 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=474529
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=172813
		Reduce input records=0
		Reduce output records=0
		Spilled Records=963
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=315097088
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:17:26,228 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:17:26,231 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:17:27,421 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:17:27,526 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:17:27,584 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:17:27,728 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1641301004_0001
   [druid] 2018-12-06 23:17:27,960 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:17:27,963 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1641301004_0001
   [druid] 2018-12-06 23:17:27,965 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:17:27,976 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:17:27,976 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:17:27,988 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:17:28,050 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:17:28,053 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1641301004_0001_m_000000_0
   [druid] 2018-12-06 23:17:28,099 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:17:28,109 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:17:28,240 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6f278124
   [druid] 2018-12-06 23:17:28,256 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:17:28,350 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:17:28,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:17:28,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:17:28,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:17:28,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:17:28,357 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:17:28,969 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1641301004_0001 running in uber mode : false
   [druid] 2018-12-06 23:17:28,972 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:17:29,446 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:17:29,451 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:17:29,452 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:17:29,452 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:17:29,452 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:17:29,583 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:17:29,596 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1641301004_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:17:29,615 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:17:29,615 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1641301004_0001_m_000000_0' done.
   [druid] 2018-12-06 23:17:29,615 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1641301004_0001_m_000000_0
   [druid] 2018-12-06 23:17:29,616 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:17:29,621 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:17:29,621 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1641301004_0001_r_000000_0
   [druid] 2018-12-06 23:17:29,635 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:17:29,636 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:17:29,779 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7ffe892c
   [druid] 2018-12-06 23:17:29,784 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@390cc31c
   [druid] 2018-12-06 23:17:29,810 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:17:29,814 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1641301004_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:17:29,868 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1641301004_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:17:29,878 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local1641301004_0001_m_000000_0
   [druid] 2018-12-06 23:17:29,882 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:17:29,884 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:17:29,886 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:17:29,886 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:17:29,918 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:17:29,918 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:17:29,926 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:17:29,929 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:17:29,930 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:17:29,930 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:17:29,931 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:17:29,931 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:17:29,976 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:17:30,418 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:33:35,334 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-06 23:33:35,338 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-06 23:33:36,457 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-06 23:33:36,587 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-06 23:33:36,639 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-06 23:33:36,763 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2118995643_0001
   [druid] 2018-12-06 23:33:36,960 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-06 23:33:36,960 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2118995643_0001
   [druid] 2018-12-06 23:33:36,964 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-06 23:33:36,972 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:33:36,973 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-06 23:33:36,981 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-06 23:33:37,038 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-06 23:33:37,039 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2118995643_0001_m_000000_0
   [druid] 2018-12-06 23:33:37,079 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:33:37,086 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:33:37,326 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@eaa4989
   [druid] 2018-12-06 23:33:37,339 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-06 23:33:37,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-06 23:33:37,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-06 23:33:37,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-06 23:33:37,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-06 23:33:37,430 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-06 23:33:37,436 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-06 23:33:37,968 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2118995643_0001 running in uber mode : false
   [druid] 2018-12-06 23:33:37,970 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-06 23:33:38,380 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-06 23:33:38,384 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-06 23:33:38,384 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-06 23:33:38,384 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 170881; bufvoid = 104857600
   [druid] 2018-12-06 23:33:38,384 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26210548(104842192); length = 3849/6553600
   [druid] 2018-12-06 23:33:38,507 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-06 23:33:38,521 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2118995643_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:33:38,533 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-06 23:33:38,533 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2118995643_0001_m_000000_0' done.
   [druid] 2018-12-06 23:33:38,533 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2118995643_0001_m_000000_0
   [druid] 2018-12-06 23:33:38,535 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-06 23:33:38,537 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-06 23:33:38,539 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2118995643_0001_r_000000_0
   [druid] 2018-12-06 23:33:38,549 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-06 23:33:38,550 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-06 23:33:38,685 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9ed1115
   [druid] 2018-12-06 23:33:38,689 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7e5af669
   [druid] 2018-12-06 23:33:38,714 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-06 23:33:38,717 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2118995643_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-06 23:33:38,763 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2118995643_0001_m_000000_0 decomp: 172809 len: 172813 to MEMORY
   [druid] 2018-12-06 23:33:38,771 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 172809 bytes from map-output for attempt_local2118995643_0001_m_000000_0
   [druid] 2018-12-06 23:33:38,773 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 172809, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->172809
   [druid] 2018-12-06 23:33:38,775 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-06 23:33:38,777 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:33:38,777 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-06 23:33:38,793 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:33:38,794 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:33:38,804 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 172809 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-06 23:33:38,805 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 172813 bytes from disk
   [druid] 2018-12-06 23:33:38,805 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-06 23:33:38,805 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-06 23:33:38,805 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 172707 bytes
   [druid] 2018-12-06 23:33:38,808 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-06 23:33:38,974 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-06 23:33:39,260 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-06 23:33:39,399 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2118995643_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-06 23:33:39,400 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-06 23:33:39,401 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2118995643_0001_r_000000_0' done.
   [druid] 2018-12-06 23:33:39,401 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2118995643_0001_r_000000_0
   [druid] 2018-12-06 23:33:39,401 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-06 23:33:39,411 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-06 23:33:39,975 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-06 23:33:39,976 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2118995643_0001 completed successfully
   [druid] 2018-12-06 23:33:39,997 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=345978
		FILE: Number of bytes written=1124963
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=963
		Map output bytes=170881
		Map output materialized bytes=172813
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=1
		Reduce shuffle bytes=172813
		Reduce input records=963
		Reduce output records=1
		Spilled Records=1926
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=6
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-06 23:35:01,044 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   