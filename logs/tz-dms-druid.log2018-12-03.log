[druid] 2018-12-03 13:16:26,763 [main           ] WARN  apreduce.NewUser.NewUserRunner {1} - NEW_USER TO MYSQL is failed !!!
   java.lang.ArrayIndexOutOfBoundsException: 2
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.handleInputOutput(NewUserRunner.java:122)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.run(NewUserRunner.java:88)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.main(NewUserRunner.java:42)
[druid] 2018-12-03 14:06:12,990 [main           ] WARN  apreduce.NewUser.NewUserRunner {1} - NEW_USER TO MYSQL is failed !!!
   java.lang.ArrayIndexOutOfBoundsException: 2
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.handleInputOutput(NewUserRunner.java:122)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.run(NewUserRunner.java:88)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.main(NewUserRunner.java:42)
[druid] 2018-12-03 14:32:09,204 [main           ] ERROR e.etl.mapreduce.Etl2HdfsDriver {1} - 设置输入输出路径异常
   java.lang.RuntimeException: 输入路径不存在/logs/11/11
	at com.phone.etl.mapreduce.Etl2HdfsDriver.handleInputOutput(Etl2HdfsDriver.java:75)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.run(Etl2HdfsDriver.java:55)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.main(Etl2HdfsDriver.java:31)
[druid] 2018-12-03 14:32:09,253 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 14:32:09,254 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 14:32:09,286 [main           ] ERROR e.etl.mapreduce.Etl2HdfsDriver {1} - 执行etl异常
   org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:138)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.run(Etl2HdfsDriver.java:56)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.main(Etl2HdfsDriver.java:31)
[druid] 2018-12-03 14:36:48,782 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 14:36:48,784 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 14:36:49,673 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 14:36:49,852 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 14:36:49,939 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 14:36:50,041 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local452309737_0001
   [druid] 2018-12-03 14:36:50,207 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 14:36:50,208 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local452309737_0001
   [druid] 2018-12-03 14:36:50,212 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 14:36:50,217 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 14:36:50,218 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 14:36:50,268 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 14:36:50,268 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local452309737_0001_m_000000_0
   [druid] 2018-12-03 14:36:50,290 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 14:36:50,295 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 14:36:50,563 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5c59b2a5
   [druid] 2018-12-03 14:36:50,568 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/logs/2018-11-11/2018-11-11-end.log:0+32947
   [druid] 2018-12-03 14:36:51,211 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local452309737_0001 running in uber mode : false
   [druid] 2018-12-03 14:36:51,216 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 14:36:51,620 [ask Executor #0] INFO  e.etl.mapreduce.Etl2HdfsMapper {1} - 输入：82过滤：0输出：1590
   [druid] 2018-12-03 14:36:51,623 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 14:36:51,685 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local452309737_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 14:36:51,697 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 14:36:51,697 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local452309737_0001_m_000000_0 is allowed to commit now
   [druid] 2018-12-03 14:36:51,704 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local452309737_0001_m_000000_0' to hdfs://Murphy:8020/ods/11/11/_temporary/0/task_local452309737_0001_m_000000
   [druid] 2018-12-03 14:36:51,705 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 14:36:51,705 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local452309737_0001_m_000000_0' done.
   [druid] 2018-12-03 14:36:51,705 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local452309737_0001_m_000000_0
   [druid] 2018-12-03 14:36:51,705 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 14:36:52,219 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 14:36:52,219 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local452309737_0001 completed successfully
   [druid] 2018-12-03 14:36:52,230 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 20
	File System Counters
		FILE: Number of bytes read=171
		FILE: Number of bytes written=277009
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=32947
		HDFS: Number of bytes written=784323
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Map-Reduce Framework
		Map input records=82
		Map output records=1590
		Input split bytes=118
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=188219392
	File Input Format Counters 
		Bytes Read=32947
	File Output Format Counters 
		Bytes Written=784323
   [druid] 2018-12-03 14:42:11,664 [main           ] WARN  apreduce.NewUser.NewUserRunner {1} - NEW_USER TO MYSQL is failed !!!
   java.lang.ArrayIndexOutOfBoundsException: 1
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.handleInputOutput(NewUserRunner.java:122)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.run(NewUserRunner.java:88)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.main(NewUserRunner.java:42)
[druid] 2018-12-03 14:43:38,092 [main           ] WARN  apreduce.NewUser.NewUserRunner {1} - NEW_USER TO MYSQL is failed !!!
   java.lang.RuntimeException: 输入路径不存在inpath/ods/ods/11
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.handleInputOutput(NewUserRunner.java:131)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.run(NewUserRunner.java:88)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.analysis.mapreduce.NewUser.NewUserRunner.main(NewUserRunner.java:42)
[druid] 2018-12-03 14:47:32,165 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 14:47:32,166 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 14:47:32,734 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 14:47:32,812 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 14:47:32,839 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 14:47:32,898 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1898216743_0001
   [druid] 2018-12-03 14:47:33,021 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 14:47:33,022 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1898216743_0001
   [druid] 2018-12-03 14:47:33,023 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 14:47:33,028 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 14:47:33,028 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 14:47:33,035 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 14:47:33,068 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 14:47:33,070 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1898216743_0001_m_000000_0
   [druid] 2018-12-03 14:47:33,089 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 14:47:33,092 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 14:47:33,238 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2820564e
   [druid] 2018-12-03 14:47:33,245 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 14:47:33,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 14:47:33,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 14:47:33,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 14:47:33,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 14:47:33,296 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 14:47:33,299 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 14:47:33,758 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 14:47:33,792 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 14:47:33,792 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 14:47:33,793 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1898216743_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:84)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 14:47:34,025 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1898216743_0001 running in uber mode : false
   [druid] 2018-12-03 14:47:34,026 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 14:47:34,027 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1898216743_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 14:47:34,031 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:19:20,575 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:19:20,577 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:19:21,229 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:19:21,284 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:19:21,316 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:19:21,390 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1896130520_0001
   [druid] 2018-12-03 15:19:21,518 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:19:21,519 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1896130520_0001
   [druid] 2018-12-03 15:19:21,521 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:19:21,527 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:19:21,527 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:19:21,535 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:19:21,571 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:19:21,573 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1896130520_0001_m_000000_0
   [druid] 2018-12-03 15:19:21,601 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:19:21,606 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:19:21,812 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7618f7c6
   [druid] 2018-12-03 15:19:21,824 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:19:21,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:19:21,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:19:21,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:19:21,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:19:21,879 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:19:21,884 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:19:22,397 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:19:22,414 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:19:22,415 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:19:22,416 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1896130520_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:19:22,521 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1896130520_0001 running in uber mode : false
   [druid] 2018-12-03 15:19:22,522 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:19:22,524 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1896130520_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:19:22,530 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:19:38,686 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:19:38,687 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:19:39,288 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:19:39,367 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:19:39,399 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:19:39,466 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1038818508_0001
   [druid] 2018-12-03 15:19:39,588 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:19:39,589 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1038818508_0001
   [druid] 2018-12-03 15:19:39,589 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:19:39,594 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:19:39,594 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:19:39,599 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:19:39,632 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:19:39,633 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1038818508_0001_m_000000_0
   [druid] 2018-12-03 15:19:39,652 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:19:39,656 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:19:39,730 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b26ba3e
   [druid] 2018-12-03 15:19:39,736 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:19:39,782 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:19:39,782 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:19:39,782 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:19:39,782 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:19:39,782 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:19:39,785 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:19:40,265 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:19:40,280 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:19:40,280 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:19:40,280 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1038818508_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:19:40,590 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1038818508_0001 running in uber mode : false
   [druid] 2018-12-03 15:19:40,591 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:19:40,593 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1038818508_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:19:40,597 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:20:09,545 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:20:09,547 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:20:10,183 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:20:10,240 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:20:10,274 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:20:10,354 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local21052052_0001
   [druid] 2018-12-03 15:20:10,496 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:20:10,498 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local21052052_0001
   [druid] 2018-12-03 15:20:10,499 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:20:10,507 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:20:10,507 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:20:10,515 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:20:10,552 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:20:10,554 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local21052052_0001_m_000000_0
   [druid] 2018-12-03 15:20:10,579 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:20:10,583 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:20:10,653 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@86227f5
   [druid] 2018-12-03 15:20:10,750 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:20:10,805 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:20:10,805 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:20:10,805 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:20:10,805 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:20:10,805 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:20:10,809 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:20:24,276 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local21052052_0001 running in uber mode : false
   [druid] 2018-12-03 15:34:42,728 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:34:42,730 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:34:43,315 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:34:43,386 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:34:43,411 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:34:43,477 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local195535207_0001
   [druid] 2018-12-03 15:34:43,594 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:34:43,594 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local195535207_0001
   [druid] 2018-12-03 15:34:43,595 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:34:43,600 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:34:43,600 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:34:43,604 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:34:43,634 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:34:43,636 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local195535207_0001_m_000000_0
   [druid] 2018-12-03 15:34:43,657 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:34:43,662 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:34:43,740 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@109a569c
   [druid] 2018-12-03 15:34:43,746 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:34:43,795 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:34:43,795 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:34:43,795 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:34:43,795 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:34:43,796 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:34:43,799 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:34:44,246 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:34:44,260 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:34:44,260 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:34:44,261 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local195535207_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:34:44,596 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local195535207_0001 running in uber mode : false
   [druid] 2018-12-03 15:34:44,597 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:34:44,599 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local195535207_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:34:44,603 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:35:04,134 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:35:04,135 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:35:04,738 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:35:04,818 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:35:04,848 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:35:04,915 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1591343205_0001
   [druid] 2018-12-03 15:35:05,041 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:35:05,042 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1591343205_0001
   [druid] 2018-12-03 15:35:05,043 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:35:05,047 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:35:05,047 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:35:05,053 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:35:05,081 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:35:05,082 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1591343205_0001_m_000000_0
   [druid] 2018-12-03 15:35:05,104 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:35:05,107 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:35:05,182 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@862d6c6
   [druid] 2018-12-03 15:35:05,189 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:35:05,238 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:35:05,238 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:35:05,238 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:35:05,238 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:35:05,238 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:35:05,241 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:35:05,702 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:35:05,718 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:35:05,718 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:35:05,719 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1591343205_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:35:06,044 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1591343205_0001 running in uber mode : false
   [druid] 2018-12-03 15:35:06,045 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:35:06,047 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1591343205_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:35:06,051 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:35:42,158 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:35:42,159 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:35:42,744 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:35:42,815 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:35:42,848 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:35:42,913 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local649430741_0001
   [druid] 2018-12-03 15:35:43,035 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:35:43,036 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local649430741_0001
   [druid] 2018-12-03 15:35:43,037 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:35:43,041 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:35:43,041 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:35:43,046 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:35:43,076 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:35:43,078 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local649430741_0001_m_000000_0
   [druid] 2018-12-03 15:35:43,098 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:35:43,102 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:35:43,179 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77802544
   [druid] 2018-12-03 15:35:43,185 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:35:43,233 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:35:43,233 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:35:43,233 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:35:43,233 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:35:43,233 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:35:43,236 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:35:43,752 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:35:43,784 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:35:43,784 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:35:43,784 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local649430741_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:35:44,038 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local649430741_0001 running in uber mode : false
   [druid] 2018-12-03 15:35:44,039 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:35:44,041 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local649430741_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:35:44,047 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:37:33,902 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:37:33,904 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:37:34,527 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:37:34,591 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:37:34,620 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:37:34,693 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local685922907_0001
   [druid] 2018-12-03 15:37:34,828 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:37:34,829 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local685922907_0001
   [druid] 2018-12-03 15:37:34,830 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:37:34,835 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:37:34,835 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:37:34,841 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:37:34,874 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:37:34,875 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local685922907_0001_m_000000_0
   [druid] 2018-12-03 15:37:34,895 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:37:34,898 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:37:34,977 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1d319783
   [druid] 2018-12-03 15:37:34,984 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:37:35,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:37:35,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:37:35,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:37:35,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:37:35,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:37:35,038 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:37:35,520 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:37:35,537 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:37:35,537 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:37:35,537 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local685922907_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:37:35,831 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local685922907_0001 running in uber mode : false
   [druid] 2018-12-03 15:37:35,832 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:37:35,833 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local685922907_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:37:35,838 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:37:54,096 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:37:54,097 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:37:54,731 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:37:54,786 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:37:54,818 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:37:54,892 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1064315521_0001
   [druid] 2018-12-03 15:37:55,040 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:37:55,041 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1064315521_0001
   [druid] 2018-12-03 15:37:55,044 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:37:55,052 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:37:55,052 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:37:55,059 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:37:55,097 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:37:55,100 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1064315521_0001_m_000000_0
   [druid] 2018-12-03 15:37:55,128 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:37:55,133 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:37:55,385 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@b510c11
   [druid] 2018-12-03 15:37:55,395 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:37:55,444 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:37:55,445 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:37:55,445 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:37:55,445 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:37:55,445 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:37:55,448 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:37:55,911 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:37:55,930 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:37:55,930 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:37:55,931 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1064315521_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:37:56,044 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1064315521_0001 running in uber mode : false
   [druid] 2018-12-03 15:37:56,045 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:37:56,049 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1064315521_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:37:56,056 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 15:38:20,035 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 15:38:20,037 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 15:38:20,581 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 15:38:20,651 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 15:38:20,676 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 15:38:20,740 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local451240653_0001
   [druid] 2018-12-03 15:38:20,861 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 15:38:20,862 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local451240653_0001
   [druid] 2018-12-03 15:38:20,863 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 15:38:20,868 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:38:20,868 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 15:38:20,872 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 15:38:20,900 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 15:38:20,902 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local451240653_0001_m_000000_0
   [druid] 2018-12-03 15:38:20,926 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 15:38:20,930 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 15:38:21,003 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@eaa4989
   [druid] 2018-12-03 15:38:21,010 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 15:38:21,060 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 15:38:21,060 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 15:38:21,060 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 15:38:21,060 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 15:38:21,060 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 15:38:21,064 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 15:38:21,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 15:38:21,550 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 15:38:21,550 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 15:38:21,551 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local451240653_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 15:38:21,864 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local451240653_0001 running in uber mode : false
   [druid] 2018-12-03 15:38:21,865 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 15:38:21,866 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local451240653_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 15:38:21,870 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:06:55,390 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:06:55,391 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:06:55,407 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-03 16:06:55,411 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:06:55,411 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:06:55,412 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local21052052_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run$$$capture(FutureTask.java:266)
	at java.util.concurrent.FutureTask.run(FutureTask.java)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:07:15,132 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:07:15,133 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:07:15,769 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:07:15,827 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:07:15,859 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:07:15,935 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1742281199_0001
   [druid] 2018-12-03 16:07:16,070 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:07:16,071 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1742281199_0001
   [druid] 2018-12-03 16:07:16,073 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:07:16,080 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:07:16,080 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:07:16,087 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:07:16,125 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:07:16,128 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1742281199_0001_m_000000_0
   [druid] 2018-12-03 16:07:16,157 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:07:16,162 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:07:16,354 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2519a8b0
   [druid] 2018-12-03 16:07:16,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:07:16,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:07:16,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:07:16,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:07:16,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:07:16,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:07:16,429 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:07:17,340 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1742281199_0001 running in uber mode : false
   [druid] 2018-12-03 16:09:47,325 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:10:09,199 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:10:09,214 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-03 16:10:14,884 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:10:14,885 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:10:15,531 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:10:15,592 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:10:15,626 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:10:15,700 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local414151972_0001
   [druid] 2018-12-03 16:10:15,834 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:10:15,835 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local414151972_0001
   [druid] 2018-12-03 16:10:15,837 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:10:15,843 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:10:15,843 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:10:15,850 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:10:15,889 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:10:15,892 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local414151972_0001_m_000000_0
   [druid] 2018-12-03 16:10:15,925 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:10:15,930 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:10:16,005 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5daa1d65
   [druid] 2018-12-03 16:10:16,022 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:10:16,078 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:10:16,078 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:10:16,078 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:10:16,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:10:16,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:10:16,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:10:16,841 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local414151972_0001 running in uber mode : false
   [druid] 2018-12-03 16:10:51,739 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:10:51,740 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:16:11,072 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:16:11,074 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:16:11,628 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:16:11,701 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:16:11,728 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:16:11,789 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1553907213_0001
   [druid] 2018-12-03 16:16:11,902 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:16:11,903 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1553907213_0001
   [druid] 2018-12-03 16:16:11,904 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:16:11,908 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:16:11,908 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:16:11,914 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:16:11,945 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:16:11,946 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1553907213_0001_m_000000_0
   [druid] 2018-12-03 16:16:11,965 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:16:11,969 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:16:12,043 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@862d6c6
   [druid] 2018-12-03 16:16:12,050 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:16:12,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:16:12,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:16:12,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:16:12,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:16:12,101 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:16:12,104 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:16:12,544 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:16:12,559 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:16:12,559 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:16:12,560 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1553907213_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:85)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:16:12,904 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1553907213_0001 running in uber mode : false
   [druid] 2018-12-03 16:16:12,905 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:16:12,906 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1553907213_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:16:12,910 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:27:52,092 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:27:52,093 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:27:52,748 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:27:52,826 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:27:52,858 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:27:52,947 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local885599708_0001
   [druid] 2018-12-03 16:27:53,074 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:27:53,075 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local885599708_0001
   [druid] 2018-12-03 16:27:53,076 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:27:53,082 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:27:53,082 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:27:53,089 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:27:53,121 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:27:53,123 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local885599708_0001_m_000000_0
   [druid] 2018-12-03 16:27:53,146 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:27:53,150 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:27:53,222 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-03 16:27:53,229 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:27:53,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:27:53,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:27:53,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:27:53,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:27:53,277 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:27:53,280 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:27:53,726 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:27:53,741 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:27:53,742 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:27:53,742 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local885599708_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:86)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:27:54,078 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local885599708_0001 running in uber mode : false
   [druid] 2018-12-03 16:27:54,079 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:27:54,081 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local885599708_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:27:54,087 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:29:00,127 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:29:00,128 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:29:00,723 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:29:00,798 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:29:00,825 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:29:00,894 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1682750455_0001
   [druid] 2018-12-03 16:29:01,020 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:29:01,021 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1682750455_0001
   [druid] 2018-12-03 16:29:01,022 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:29:01,028 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:29:01,028 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:29:01,034 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:29:01,065 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:29:01,067 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1682750455_0001_m_000000_0
   [druid] 2018-12-03 16:29:01,088 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:29:01,092 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:29:01,166 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1d319783
   [druid] 2018-12-03 16:29:01,174 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:29:01,222 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:29:01,222 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:29:01,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:29:01,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:29:01,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:29:01,227 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:29:01,687 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:29:01,703 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:29:01,704 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:29:01,705 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1682750455_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:86)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:29:02,025 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1682750455_0001 running in uber mode : false
   [druid] 2018-12-03 16:29:02,026 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:29:02,027 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1682750455_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:29:02,031 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:37:54,216 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:37:54,217 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:37:54,814 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:37:54,889 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:37:54,917 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:37:54,984 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1534845433_0001
   [druid] 2018-12-03 16:37:55,101 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:37:55,102 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1534845433_0001
   [druid] 2018-12-03 16:37:55,102 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:37:55,107 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:37:55,107 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:37:55,111 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:37:55,142 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:37:55,143 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1534845433_0001_m_000000_0
   [druid] 2018-12-03 16:37:55,164 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:37:55,167 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:37:55,364 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@37eb1d34
   [druid] 2018-12-03 16:37:55,371 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:37:55,421 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:37:55,422 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:37:55,422 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:37:55,422 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:37:55,422 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:37:55,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:37:55,867 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:37:55,884 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:37:55,884 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:37:55,884 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1534845433_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:86)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:37:56,103 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1534845433_0001 running in uber mode : false
   [druid] 2018-12-03 16:37:56,104 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:37:56,106 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1534845433_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:37:56,109 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:38:31,490 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:38:31,491 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:38:32,126 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:38:32,191 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:38:32,228 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:38:32,309 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1590604960_0001
   [druid] 2018-12-03 16:38:32,448 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:38:32,449 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1590604960_0001
   [druid] 2018-12-03 16:38:32,451 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:38:32,459 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:38:32,459 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:38:32,465 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:38:32,506 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:38:32,508 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1590604960_0001_m_000000_0
   [druid] 2018-12-03 16:38:32,533 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:38:32,537 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:38:32,612 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@14991056
   [druid] 2018-12-03 16:38:32,633 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:38:32,685 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:38:32,685 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:38:32,685 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:38:32,685 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:38:32,685 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:38:32,689 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:38:48,665 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1590604960_0001 running in uber mode : false
   [druid] 2018-12-03 16:39:08,601 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:39:35,976 [nication thread] INFO  e.hadoop.mapred.LocalJobRunner {1} - map > map
   [druid] 2018-12-03 16:39:37,388 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 2% reduce 0%
   [druid] 2018-12-03 16:43:46,172 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:43:46,194 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:43:46,195 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:44:22,624 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:44:22,626 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:44:23,244 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:44:23,313 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:44:23,341 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:44:23,403 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1262005950_0001
   [druid] 2018-12-03 16:44:23,524 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:44:23,525 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1262005950_0001
   [druid] 2018-12-03 16:44:23,526 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:44:23,531 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:44:23,531 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:44:23,536 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:44:23,568 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:44:23,570 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1262005950_0001_m_000000_0
   [druid] 2018-12-03 16:44:23,590 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:44:23,593 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:44:23,667 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77f76243
   [druid] 2018-12-03 16:44:23,675 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:44:23,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:44:23,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:44:23,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:44:23,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:44:23,740 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:44:23,743 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:44:24,197 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:44:24,212 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:44:24,212 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:44:24,213 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1262005950_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.write(PlatformDimension.java:86)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:49)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:75)
	at com.phone.analysis.mapreduce.NewUser.NewUserMapper.map(NewUserMapper.java:26)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:44:24,528 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1262005950_0001 running in uber mode : false
   [druid] 2018-12-03 16:44:24,529 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 16:44:24,530 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1262005950_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:44:24,535 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-03 16:48:36,065 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:48:36,067 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:48:36,690 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:48:36,761 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:48:36,787 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:48:36,854 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local924879639_0001
   [druid] 2018-12-03 16:48:36,969 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:48:36,970 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local924879639_0001
   [druid] 2018-12-03 16:48:36,971 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:48:36,975 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:48:36,975 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:48:36,981 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:48:37,015 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:48:37,017 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local924879639_0001_m_000000_0
   [druid] 2018-12-03 16:48:37,035 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:48:37,039 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:48:37,110 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ac1d3
   [druid] 2018-12-03 16:48:37,117 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:48:37,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:48:37,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:48:37,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:48:37,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:48:37,162 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:48:37,166 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:48:37,756 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 16:48:37,758 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:48:37,758 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 16:48:37,758 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 16:48:37,758 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 16:48:37,774 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 16:48:37,780 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local924879639_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 16:48:37,789 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 16:48:37,789 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local924879639_0001_m_000000_0' done.
   [druid] 2018-12-03 16:48:37,790 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local924879639_0001_m_000000_0
   [druid] 2018-12-03 16:48:37,790 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:48:37,791 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 16:48:37,792 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local924879639_0001_r_000000_0
   [druid] 2018-12-03 16:48:37,797 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:48:37,797 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:48:37,868 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4dbda9f0
   [druid] 2018-12-03 16:48:37,912 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2f0e1702
   [druid] 2018-12-03 16:48:37,930 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 16:48:37,932 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local924879639_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 16:48:37,964 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local924879639_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 16:48:37,970 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local924879639_0001_m_000000_0
   [druid] 2018-12-03 16:48:37,971 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 16:48:37,972 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local924879639_0001 running in uber mode : false
   [druid] 2018-12-03 16:48:37,973 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 16:48:37,973 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 16:48:37,974 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 16:48:37,974 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 16:48:37,986 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 16:48:37,987 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 16:48:37,989 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 16:48:37,990 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 16:48:37,990 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 16:48:37,990 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 16:48:37,991 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 16:48:37,991 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 16:48:38,317 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 16:48:38,343 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 16:48:38,349 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:48:38,349 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local924879639_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:83)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewUser.NewUserReducer.reduce(NewUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewUser.NewUserReducer.reduce(NewUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:48:38,976 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local924879639_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:48:38,984 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=328498
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=31122
		Reduce input records=0
		Reduce output records=0
		Spilled Records=236
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=316145664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 16:51:41,053 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 16:51:41,055 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 16:51:41,638 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 16:51:41,716 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 16:51:41,746 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 16:51:41,808 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1722531942_0001
   [druid] 2018-12-03 16:51:41,935 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 16:51:41,936 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1722531942_0001
   [druid] 2018-12-03 16:51:41,937 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 16:51:41,943 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:51:41,943 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 16:51:41,950 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 16:51:41,981 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 16:51:41,982 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1722531942_0001_m_000000_0
   [druid] 2018-12-03 16:51:42,007 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:51:42,011 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:51:42,089 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-03 16:51:42,097 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 16:51:42,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 16:51:42,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 16:51:42,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 16:51:42,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 16:51:42,152 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 16:51:42,155 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 16:51:42,670 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 16:51:42,672 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 16:51:42,673 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 16:51:42,673 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 16:51:42,673 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 16:51:42,687 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 16:51:42,694 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1722531942_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 16:51:42,702 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 16:51:42,702 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1722531942_0001_m_000000_0' done.
   [druid] 2018-12-03 16:51:42,702 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1722531942_0001_m_000000_0
   [druid] 2018-12-03 16:51:42,703 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 16:51:42,704 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 16:51:42,705 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1722531942_0001_r_000000_0
   [druid] 2018-12-03 16:51:42,711 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 16:51:42,711 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 16:51:42,784 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@19e53092
   [druid] 2018-12-03 16:51:42,786 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2ad43e3e
   [druid] 2018-12-03 16:51:42,796 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 16:51:42,798 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1722531942_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 16:51:42,824 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1722531942_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 16:51:42,828 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local1722531942_0001_m_000000_0
   [druid] 2018-12-03 16:51:42,829 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 16:51:42,830 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 16:51:42,831 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 16:51:42,831 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 16:51:42,840 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 16:51:42,840 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 16:51:42,843 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 16:51:42,844 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 16:51:42,845 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 16:51:42,845 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 16:51:42,845 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 16:51:42,846 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 16:51:42,939 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1722531942_0001 running in uber mode : false
   [druid] 2018-12-03 16:51:42,940 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 16:51:43,084 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 16:51:43,092 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 16:51:43,099 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-03 16:51:43,100 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1722531942_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)
Caused by: java.lang.NullPointerException
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:83)
	at com.phone.analysis.OutputToMysqlFormat$OutputToMysqlRecordWriter.write(OutputToMysqlFormat.java:47)
	at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)
	at com.phone.analysis.mapreduce.NewUser.NewUserReducer.reduce(NewUserReducer.java:40)
	at com.phone.analysis.mapreduce.NewUser.NewUserReducer.reduce(NewUserReducer.java:21)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)
	at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-03 16:51:43,941 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1722531942_0001 failed with state FAILED due to: NA
   [druid] 2018-12-03 16:51:43,951 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=160
		FILE: Number of bytes written=330030
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=784323
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=4
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=0
		Reduce shuffle bytes=31122
		Reduce input records=0
		Reduce output records=0
		Spilled Records=236
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=316145664
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 18:02:23,122 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 18:02:23,123 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 18:02:23,752 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 18:02:23,825 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 18:02:23,853 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 18:02:23,917 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local189132115_0001
   [druid] 2018-12-03 18:02:24,058 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 18:02:24,059 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local189132115_0001
   [druid] 2018-12-03 18:02:24,060 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 18:02:24,064 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:02:24,064 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 18:02:24,069 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 18:02:24,099 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 18:02:24,101 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local189132115_0001_m_000000_0
   [druid] 2018-12-03 18:02:24,122 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:02:24,126 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:02:24,363 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ac1d3
   [druid] 2018-12-03 18:02:24,370 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 18:02:24,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 18:02:24,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 18:02:24,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 18:02:24,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 18:02:24,423 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 18:02:24,426 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 18:02:24,953 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 18:02:24,955 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 18:02:24,955 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 18:02:24,956 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 18:02:24,956 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 18:02:24,973 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 18:02:24,980 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local189132115_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:02:24,989 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 18:02:24,989 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local189132115_0001_m_000000_0' done.
   [druid] 2018-12-03 18:02:24,989 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local189132115_0001_m_000000_0
   [druid] 2018-12-03 18:02:24,989 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 18:02:24,991 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 18:02:24,991 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local189132115_0001_r_000000_0
   [druid] 2018-12-03 18:02:24,996 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:02:24,997 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:02:25,061 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local189132115_0001 running in uber mode : false
   [druid] 2018-12-03 18:02:25,062 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 18:02:25,073 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3b5479e8
   [druid] 2018-12-03 18:02:25,076 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7127ea1a
   [druid] 2018-12-03 18:02:25,088 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 18:02:25,090 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local189132115_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 18:02:25,120 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local189132115_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 18:02:25,124 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local189132115_0001_m_000000_0
   [druid] 2018-12-03 18:02:25,126 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 18:02:25,127 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 18:02:25,128 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:02:25,128 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 18:02:25,138 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:02:25,138 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:02:25,141 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 18:02:25,142 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 18:02:25,142 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 18:02:25,143 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:02:25,143 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:02:25,144 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:02:29,722 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 18:02:30,583 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local189132115_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:02:30,583 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 18:02:30,584 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local189132115_0001_r_000000_0' done.
   [druid] 2018-12-03 18:02:30,584 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local189132115_0001_r_000000_0
   [druid] 2018-12-03 18:02:30,584 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 18:02:30,589 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 18:02:31,066 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 18:02:31,066 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local189132115_0001 completed successfully
   [druid] 2018-12-03 18:02:31,078 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=688118
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 18:54:14,893 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 18:54:14,894 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 18:54:15,507 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 18:54:15,595 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 18:54:15,632 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 18:54:15,711 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1570624687_0001
   [druid] 2018-12-03 18:54:15,847 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 18:54:15,848 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1570624687_0001
   [druid] 2018-12-03 18:54:15,849 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 18:54:15,854 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:54:15,854 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 18:54:15,859 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 18:54:15,910 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 18:54:15,912 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1570624687_0001_m_000000_0
   [druid] 2018-12-03 18:54:15,946 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:54:15,952 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:54:16,219 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@38522b56
   [druid] 2018-12-03 18:54:16,226 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 18:54:16,285 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 18:54:16,285 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 18:54:16,285 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 18:54:16,285 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 18:54:16,285 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 18:54:16,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 18:54:16,850 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1570624687_0001 running in uber mode : false
   [druid] 2018-12-03 18:54:16,851 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-03 18:54:16,960 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 18:54:16,962 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 18:54:16,962 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 18:54:16,962 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 18:54:16,962 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 18:54:16,981 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 18:54:16,989 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1570624687_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:54:16,999 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 18:54:17,000 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1570624687_0001_m_000000_0' done.
   [druid] 2018-12-03 18:54:17,000 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1570624687_0001_m_000000_0
   [druid] 2018-12-03 18:54:17,000 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 18:54:17,002 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 18:54:17,002 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1570624687_0001_r_000000_0
   [druid] 2018-12-03 18:54:17,009 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:54:17,009 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:54:17,095 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2696feff
   [druid] 2018-12-03 18:54:17,097 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@77e4d27c
   [druid] 2018-12-03 18:54:17,111 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 18:54:17,113 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1570624687_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 18:54:17,146 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1570624687_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 18:54:17,150 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local1570624687_0001_m_000000_0
   [druid] 2018-12-03 18:54:17,152 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 18:54:17,154 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 18:54:17,154 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:54:17,155 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 18:54:17,164 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:54:17,164 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:54:17,169 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 18:54:17,170 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 18:54:17,170 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 18:54:17,171 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:54:17,171 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:54:17,172 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:54:17,676 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 18:54:17,853 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 18:54:17,894 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1570624687_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:54:17,895 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 18:54:17,895 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1570624687_0001_r_000000_0' done.
   [druid] 2018-12-03 18:54:17,895 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1570624687_0001_r_000000_0
   [druid] 2018-12-03 18:54:17,895 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 18:54:17,899 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 18:54:18,853 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 18:54:18,853 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1570624687_0001 completed successfully
   [druid] 2018-12-03 18:54:18,870 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=691182
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 18:56:10,314 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 18:56:10,315 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 18:56:10,928 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 18:56:10,995 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 18:56:11,022 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 18:56:11,084 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local694792402_0001
   [druid] 2018-12-03 18:56:11,194 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 18:56:11,194 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local694792402_0001
   [druid] 2018-12-03 18:56:11,195 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 18:56:11,200 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:56:11,200 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 18:56:11,205 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 18:56:11,235 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 18:56:11,236 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local694792402_0001_m_000000_0
   [druid] 2018-12-03 18:56:11,255 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:56:11,259 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:56:11,331 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@77f76243
   [druid] 2018-12-03 18:56:11,338 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 18:56:11,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 18:56:11,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 18:56:11,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 18:56:11,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 18:56:11,388 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 18:56:11,391 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 18:56:11,900 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 18:56:11,902 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 18:56:11,902 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 18:56:11,902 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 18:56:11,902 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 18:56:11,921 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 18:56:11,928 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local694792402_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:56:11,937 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 18:56:11,937 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local694792402_0001_m_000000_0' done.
   [druid] 2018-12-03 18:56:11,937 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local694792402_0001_m_000000_0
   [druid] 2018-12-03 18:56:11,937 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 18:56:11,939 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 18:56:11,939 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local694792402_0001_r_000000_0
   [druid] 2018-12-03 18:56:11,946 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:56:11,947 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:56:12,022 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16937d04
   [druid] 2018-12-03 18:56:12,024 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f8bcec6
   [druid] 2018-12-03 18:56:12,037 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 18:56:12,040 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local694792402_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 18:56:12,075 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local694792402_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 18:56:12,080 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local694792402_0001_m_000000_0
   [druid] 2018-12-03 18:56:12,082 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 18:56:12,083 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 18:56:12,084 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:56:12,084 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 18:56:12,093 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:56:12,093 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:56:12,096 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 18:56:12,097 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 18:56:12,098 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 18:56:12,098 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:56:12,099 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:56:12,099 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:56:12,197 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local694792402_0001 running in uber mode : false
   [druid] 2018-12-03 18:56:12,198 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 18:56:12,392 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 18:56:12,681 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local694792402_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:56:12,682 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 18:56:12,682 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local694792402_0001_r_000000_0' done.
   [druid] 2018-12-03 18:56:12,682 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local694792402_0001_r_000000_0
   [druid] 2018-12-03 18:56:12,682 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 18:56:12,687 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 18:56:13,199 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 18:56:13,199 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local694792402_0001 completed successfully
   [druid] 2018-12-03 18:56:13,211 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=688118
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 18:58:01,213 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 18:58:01,214 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 18:58:01,824 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 18:58:01,904 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 18:58:01,933 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 18:58:02,002 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1358044700_0001
   [druid] 2018-12-03 18:58:02,136 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 18:58:02,137 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1358044700_0001
   [druid] 2018-12-03 18:58:02,137 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 18:58:02,143 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:02,143 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 18:58:02,149 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 18:58:02,184 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 18:58:02,186 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1358044700_0001_m_000000_0
   [druid] 2018-12-03 18:58:02,207 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:02,210 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:58:02,283 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b26ba3e
   [druid] 2018-12-03 18:58:02,289 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 18:58:02,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 18:58:02,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 18:58:02,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 18:58:02,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 18:58:02,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 18:58:02,344 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 18:58:02,857 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 18:58:02,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 18:58:02,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 18:58:02,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 18:58:02,859 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 18:58:02,877 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 18:58:02,884 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1358044700_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:58:02,894 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 18:58:02,894 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1358044700_0001_m_000000_0' done.
   [druid] 2018-12-03 18:58:02,894 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1358044700_0001_m_000000_0
   [druid] 2018-12-03 18:58:02,894 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 18:58:02,896 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 18:58:02,896 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1358044700_0001_r_000000_0
   [druid] 2018-12-03 18:58:02,901 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:02,901 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:58:02,976 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1364ecda
   [druid] 2018-12-03 18:58:02,978 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@29958603
   [druid] 2018-12-03 18:58:02,990 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 18:58:02,991 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1358044700_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 18:58:03,017 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1358044700_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 18:58:03,021 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local1358044700_0001_m_000000_0
   [druid] 2018-12-03 18:58:03,022 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 18:58:03,024 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 18:58:03,024 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:58:03,024 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 18:58:03,033 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:58:03,034 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:58:03,037 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 18:58:03,038 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 18:58:03,038 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 18:58:03,038 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:58:03,039 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:58:03,040 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:58:03,138 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1358044700_0001 running in uber mode : false
   [druid] 2018-12-03 18:58:03,139 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 18:58:03,403 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 18:58:04,009 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1358044700_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:58:04,009 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 18:58:04,009 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1358044700_0001_r_000000_0' done.
   [druid] 2018-12-03 18:58:04,009 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1358044700_0001_r_000000_0
   [druid] 2018-12-03 18:58:04,009 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 18:58:04,014 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 18:58:04,141 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 18:58:04,141 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1358044700_0001 completed successfully
   [druid] 2018-12-03 18:58:04,152 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=691182
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 18:58:57,412 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 18:58:57,414 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 18:58:58,046 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 18:58:58,124 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 18:58:58,155 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 18:58:58,234 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1690990105_0001
   [druid] 2018-12-03 18:58:58,361 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 18:58:58,362 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1690990105_0001
   [druid] 2018-12-03 18:58:58,362 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 18:58:58,368 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:58,368 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 18:58:58,373 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 18:58:58,403 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 18:58:58,404 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1690990105_0001_m_000000_0
   [druid] 2018-12-03 18:58:58,425 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:58,429 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:58:58,503 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2820564e
   [druid] 2018-12-03 18:58:58,510 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 18:58:58,562 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 18:58:58,562 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 18:58:58,562 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 18:58:58,562 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 18:58:58,562 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 18:58:58,566 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 18:58:59,081 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 18:58:59,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 18:58:59,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 18:58:59,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 18:58:59,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 18:58:59,099 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 18:58:59,105 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1690990105_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:58:59,114 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 18:58:59,114 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1690990105_0001_m_000000_0' done.
   [druid] 2018-12-03 18:58:59,114 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1690990105_0001_m_000000_0
   [druid] 2018-12-03 18:58:59,114 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 18:58:59,116 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 18:58:59,117 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1690990105_0001_r_000000_0
   [druid] 2018-12-03 18:58:59,122 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 18:58:59,122 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 18:58:59,197 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@13343eac
   [druid] 2018-12-03 18:58:59,200 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@41f7b54d
   [druid] 2018-12-03 18:58:59,210 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 18:58:59,212 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1690990105_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 18:58:59,238 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1690990105_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 18:58:59,242 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local1690990105_0001_m_000000_0
   [druid] 2018-12-03 18:58:59,243 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 18:58:59,244 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 18:58:59,245 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:58:59,245 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 18:58:59,253 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:58:59,253 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:58:59,257 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 18:58:59,258 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 18:58:59,258 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 18:58:59,259 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 18:58:59,260 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 18:58:59,260 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 18:58:59,363 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1690990105_0001 running in uber mode : false
   [druid] 2018-12-03 18:58:59,365 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 18:58:59,734 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 18:58:59,867 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1690990105_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 18:58:59,868 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 18:58:59,868 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1690990105_0001_r_000000_0' done.
   [druid] 2018-12-03 18:58:59,868 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1690990105_0001_r_000000_0
   [druid] 2018-12-03 18:58:59,868 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 18:58:59,872 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 18:59:00,367 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 18:59:00,367 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1690990105_0001 completed successfully
   [druid] 2018-12-03 18:59:00,378 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=691182
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 19:00:02,947 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 19:00:02,948 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 19:00:03,585 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 19:00:03,652 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 19:00:03,680 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 19:00:03,742 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local248143733_0001
   [druid] 2018-12-03 19:00:03,878 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 19:00:03,879 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local248143733_0001
   [druid] 2018-12-03 19:00:03,880 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 19:00:03,886 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:00:03,886 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 19:00:03,891 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 19:00:03,924 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 19:00:03,926 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local248143733_0001_m_000000_0
   [druid] 2018-12-03 19:00:03,946 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:00:03,950 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 19:00:04,026 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-03 19:00:04,034 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 19:00:04,084 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 19:00:04,084 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 19:00:04,084 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 19:00:04,084 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 19:00:04,084 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 19:00:04,087 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 19:00:04,663 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 19:00:04,665 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 19:00:04,665 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 19:00:04,665 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 19:00:04,665 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 19:00:04,684 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 19:00:04,692 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local248143733_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 19:00:04,701 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 19:00:04,702 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local248143733_0001_m_000000_0' done.
   [druid] 2018-12-03 19:00:04,702 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local248143733_0001_m_000000_0
   [druid] 2018-12-03 19:00:04,702 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 19:00:04,704 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 19:00:04,704 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local248143733_0001_r_000000_0
   [druid] 2018-12-03 19:00:04,712 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:00:04,712 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 19:00:04,799 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7ad67c0
   [druid] 2018-12-03 19:00:04,801 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@e767335
   [druid] 2018-12-03 19:00:04,812 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 19:00:04,813 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local248143733_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 19:00:04,841 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local248143733_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 19:00:04,845 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local248143733_0001_m_000000_0
   [druid] 2018-12-03 19:00:04,846 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 19:00:04,847 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 19:00:04,848 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 19:00:04,848 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 19:00:04,857 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 19:00:04,857 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 19:00:04,862 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 19:00:04,862 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 19:00:04,863 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 19:00:04,863 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 19:00:04,864 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 19:00:04,864 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 19:00:04,882 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local248143733_0001 running in uber mode : false
   [druid] 2018-12-03 19:00:04,884 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 19:00:05,146 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 19:00:05,356 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local248143733_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 19:00:05,357 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 19:00:05,357 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local248143733_0001_r_000000_0' done.
   [druid] 2018-12-03 19:00:05,357 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local248143733_0001_r_000000_0
   [druid] 2018-12-03 19:00:05,357 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 19:00:05,364 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 19:00:05,885 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 19:00:05,885 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local248143733_0001 completed successfully
   [druid] 2018-12-03 19:00:05,895 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=688118
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 19:01:23,686 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 19:01:23,688 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 19:01:24,286 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 19:01:24,361 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 19:01:24,387 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 19:01:24,448 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local736550122_0001
   [druid] 2018-12-03 19:01:24,571 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 19:01:24,572 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local736550122_0001
   [druid] 2018-12-03 19:01:24,573 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 19:01:24,579 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:01:24,579 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 19:01:24,584 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 19:01:24,613 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 19:01:24,615 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local736550122_0001_m_000000_0
   [druid] 2018-12-03 19:01:24,634 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:01:24,638 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 19:01:24,713 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-03 19:01:24,719 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 19:01:24,766 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 19:01:24,766 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 19:01:24,766 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 19:01:24,766 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 19:01:24,766 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 19:01:24,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 19:01:25,282 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 19:01:25,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 19:01:25,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 19:01:25,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-03 19:01:25,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-03 19:01:25,299 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 19:01:25,305 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local736550122_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 19:01:25,314 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 19:01:25,314 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local736550122_0001_m_000000_0' done.
   [druid] 2018-12-03 19:01:25,314 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local736550122_0001_m_000000_0
   [druid] 2018-12-03 19:01:25,314 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 19:01:25,316 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 19:01:25,317 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local736550122_0001_r_000000_0
   [druid] 2018-12-03 19:01:25,323 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 19:01:25,323 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 19:01:25,400 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@724e72b
   [druid] 2018-12-03 19:01:25,402 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1f1fcae4
   [druid] 2018-12-03 19:01:25,412 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 19:01:25,414 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local736550122_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 19:01:25,441 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local736550122_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-03 19:01:25,445 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local736550122_0001_m_000000_0
   [druid] 2018-12-03 19:01:25,446 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-03 19:01:25,447 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 19:01:25,448 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 19:01:25,448 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 19:01:25,458 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 19:01:25,459 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 19:01:25,461 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 19:01:25,462 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-03 19:01:25,463 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 19:01:25,463 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 19:01:25,464 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-03 19:01:25,464 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 19:01:25,575 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local736550122_0001 running in uber mode : false
   [druid] 2018-12-03 19:01:25,576 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 19:01:25,808 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 19:01:26,039 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local736550122_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 19:01:26,040 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 19:01:26,040 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local736550122_0001_r_000000_0' done.
   [druid] 2018-12-03 19:01:26,040 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local736550122_0001_r_000000_0
   [druid] 2018-12-03 19:01:26,040 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 19:01:26,046 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 19:01:26,577 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 19:01:26,577 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local736550122_0001 completed successfully
   [druid] 2018-12-03 19:01:26,588 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=688118
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 20:31:08,484 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 20:31:08,486 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 20:31:09,088 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 20:31:09,164 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 20:31:09,194 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 20:31:09,258 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local763968028_0001
   [druid] 2018-12-03 20:31:09,380 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 20:31:09,381 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local763968028_0001
   [druid] 2018-12-03 20:31:09,382 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 20:31:09,386 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:31:09,386 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 20:31:09,391 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 20:31:09,422 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 20:31:09,423 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local763968028_0001_m_000000_0
   [druid] 2018-12-03 20:31:09,448 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:31:09,452 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:31:09,666 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@2820564e
   [druid] 2018-12-03 20:31:09,675 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 20:31:09,731 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 20:31:09,731 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 20:31:09,731 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 20:31:09,731 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 20:31:09,731 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 20:31:09,733 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 20:31:10,177 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,178 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,179 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,179 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:31:10,294 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 20:31:10,297 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 20:31:10,297 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 20:31:10,297 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 417162; bufvoid = 104857600
   [druid] 2018-12-03 20:31:10,297 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-03 20:31:10,340 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 20:31:10,346 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local763968028_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:31:10,355 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 20:31:10,356 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local763968028_0001_m_000000_0' done.
   [druid] 2018-12-03 20:31:10,356 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local763968028_0001_m_000000_0
   [druid] 2018-12-03 20:31:10,356 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 20:31:10,358 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 20:31:10,358 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local763968028_0001_r_000000_0
   [druid] 2018-12-03 20:31:10,364 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:31:10,365 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:31:10,382 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local763968028_0001 running in uber mode : false
   [druid] 2018-12-03 20:31:10,383 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 20:31:10,446 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@40dbef79
   [druid] 2018-12-03 20:31:10,449 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79042d9b
   [druid] 2018-12-03 20:31:10,462 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 20:31:10,464 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local763968028_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 20:31:10,494 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local763968028_0001_m_000000_0 decomp: 423488 len: 423492 to MEMORY
   [druid] 2018-12-03 20:31:10,498 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 423488 bytes from map-output for attempt_local763968028_0001_m_000000_0
   [druid] 2018-12-03 20:31:10,499 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 423488, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->423488
   [druid] 2018-12-03 20:31:10,500 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 20:31:10,501 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:31:10,501 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 20:31:10,511 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:31:10,512 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:31:10,520 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 423488 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 20:31:10,521 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 423492 bytes from disk
   [druid] 2018-12-03 20:31:10,522 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 20:31:10,522 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:31:10,523 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:31:10,523 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:31:10,930 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 20:31:11,189 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local763968028_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:31:11,190 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 20:31:11,190 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local763968028_0001_r_000000_0' done.
   [druid] 2018-12-03 20:31:11,190 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local763968028_0001_r_000000_0
   [druid] 2018-12-03 20:31:11,190 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 20:31:11,197 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 20:31:11,385 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 20:31:11,385 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local763968028_0001 completed successfully
   [druid] 2018-12-03 20:31:11,397 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=847336
		FILE: Number of bytes written=1865284
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=417162
		Map output materialized bytes=423492
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=423492
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 20:34:35,968 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 20:34:35,969 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 20:34:36,570 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 20:34:36,647 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 20:34:36,674 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 20:34:36,740 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local2117597000_0001
   [druid] 2018-12-03 20:34:36,867 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 20:34:36,868 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local2117597000_0001
   [druid] 2018-12-03 20:34:36,869 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 20:34:36,874 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:34:36,874 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 20:34:36,878 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 20:34:36,909 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 20:34:36,911 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2117597000_0001_m_000000_0
   [druid] 2018-12-03 20:34:36,934 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:34:36,938 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:34:37,023 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@58c653b4
   [druid] 2018-12-03 20:34:37,030 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 20:34:37,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 20:34:37,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 20:34:37,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 20:34:37,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 20:34:37,083 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 20:34:37,087 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 20:34:37,514 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,515 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,515 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,515 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,515 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,516 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,516 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,516 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,516 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:34:37,625 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 20:34:37,627 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 20:34:37,627 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 20:34:37,627 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 417162; bufvoid = 104857600
   [druid] 2018-12-03 20:34:37,627 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-03 20:34:37,671 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 20:34:37,678 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2117597000_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:34:37,686 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 20:34:37,686 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2117597000_0001_m_000000_0' done.
   [druid] 2018-12-03 20:34:37,686 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2117597000_0001_m_000000_0
   [druid] 2018-12-03 20:34:37,686 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 20:34:37,688 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 20:34:37,689 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local2117597000_0001_r_000000_0
   [druid] 2018-12-03 20:34:37,694 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:34:37,695 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:34:37,771 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ad5ae
   [druid] 2018-12-03 20:34:37,773 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2c759bbf
   [druid] 2018-12-03 20:34:37,786 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 20:34:37,788 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local2117597000_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 20:34:37,822 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local2117597000_0001_m_000000_0 decomp: 423488 len: 423492 to MEMORY
   [druid] 2018-12-03 20:34:37,827 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 423488 bytes from map-output for attempt_local2117597000_0001_m_000000_0
   [druid] 2018-12-03 20:34:37,828 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 423488, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->423488
   [druid] 2018-12-03 20:34:37,830 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 20:34:37,830 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:34:37,831 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 20:34:37,842 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:34:37,842 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:34:37,851 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 423488 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 20:34:37,852 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 423492 bytes from disk
   [druid] 2018-12-03 20:34:37,853 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 20:34:37,853 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:34:37,854 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:34:37,855 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:34:37,870 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2117597000_0001 running in uber mode : false
   [druid] 2018-12-03 20:34:37,871 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 20:34:38,115 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 20:34:39,082 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local2117597000_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:34:39,084 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 20:34:39,084 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local2117597000_0001_r_000000_0' done.
   [druid] 2018-12-03 20:34:39,084 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local2117597000_0001_r_000000_0
   [druid] 2018-12-03 20:34:39,084 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 20:34:39,092 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 20:34:39,873 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 20:34:39,873 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local2117597000_0001 completed successfully
   [druid] 2018-12-03 20:34:39,887 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=847336
		FILE: Number of bytes written=1868348
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=417162
		Map output materialized bytes=423492
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=423492
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-03 20:42:57,431 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-03 20:42:57,432 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-03 20:42:58,037 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-03 20:42:58,112 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-03 20:42:58,142 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-03 20:42:58,209 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local807326673_0001
   [druid] 2018-12-03 20:42:58,335 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-03 20:42:58,336 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local807326673_0001
   [druid] 2018-12-03 20:42:58,337 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-03 20:42:58,342 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:42:58,342 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-03 20:42:58,347 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-03 20:42:58,378 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-03 20:42:58,379 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local807326673_0001_m_000000_0
   [druid] 2018-12-03 20:42:58,402 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:42:58,406 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:42:58,484 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@705d0ab2
   [druid] 2018-12-03 20:42:58,491 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-03 20:42:58,538 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-03 20:42:58,538 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-03 20:42:58,538 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-03 20:42:58,538 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-03 20:42:58,538 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-03 20:42:58,541 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-03 20:42:58,964 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,964 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,964 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,964 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,965 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,965 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,965 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,965 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:58,965 [ask Executor #0] INFO  ce.ActiveUser.ActiveUserMapper {1} - serverTime或者uuid不能为空
   [druid] 2018-12-03 20:42:59,077 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-03 20:42:59,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-03 20:42:59,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-03 20:42:59,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 417162; bufvoid = 104857600
   [druid] 2018-12-03 20:42:59,079 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-03 20:42:59,128 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-03 20:42:59,135 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local807326673_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:42:59,145 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-03 20:42:59,145 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local807326673_0001_m_000000_0' done.
   [druid] 2018-12-03 20:42:59,146 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local807326673_0001_m_000000_0
   [druid] 2018-12-03 20:42:59,146 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-03 20:42:59,147 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-03 20:42:59,148 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local807326673_0001_r_000000_0
   [druid] 2018-12-03 20:42:59,153 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-03 20:42:59,153 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-03 20:42:59,231 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1ec50fb7
   [druid] 2018-12-03 20:42:59,233 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@7764c629
   [druid] 2018-12-03 20:42:59,245 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-03 20:42:59,247 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local807326673_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-03 20:42:59,277 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local807326673_0001_m_000000_0 decomp: 423488 len: 423492 to MEMORY
   [druid] 2018-12-03 20:42:59,282 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 423488 bytes from map-output for attempt_local807326673_0001_m_000000_0
   [druid] 2018-12-03 20:42:59,283 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 423488, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->423488
   [druid] 2018-12-03 20:42:59,285 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-03 20:42:59,285 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:42:59,286 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-03 20:42:59,296 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:42:59,296 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:42:59,308 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 423488 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-03 20:42:59,309 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 423492 bytes from disk
   [druid] 2018-12-03 20:42:59,310 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-03 20:42:59,310 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-03 20:42:59,311 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 423414 bytes
   [druid] 2018-12-03 20:42:59,311 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-03 20:42:59,337 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local807326673_0001 running in uber mode : false
   [druid] 2018-12-03 20:42:59,338 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-03 20:42:59,564 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-03 20:43:00,073 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local807326673_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-03 20:43:00,074 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-03 20:43:00,074 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local807326673_0001_r_000000_0' done.
   [druid] 2018-12-03 20:43:00,074 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local807326673_0001_r_000000_0
   [druid] 2018-12-03 20:43:00,074 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-03 20:43:00,080 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-03 20:43:00,339 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-03 20:43:00,339 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local807326673_0001 completed successfully
   [druid] 2018-12-03 20:43:00,350 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=847336
		FILE: Number of bytes written=1865284
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=417162
		Map output materialized bytes=423492
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=423492
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   