[druid] 2018-12-04 14:32:12,412 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 14:32:12,432 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 14:32:13,362 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 14:32:13,433 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 14:32:13,495 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 14:32:13,601 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1934233952_0001
   [druid] 2018-12-04 14:32:13,822 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 14:32:13,823 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1934233952_0001
   [druid] 2018-12-04 14:32:13,827 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 14:32:13,833 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 14:32:13,833 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 14:32:13,843 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 14:32:13,884 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 14:32:13,886 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1934233952_0001_m_000000_0
   [druid] 2018-12-04 14:32:13,909 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 14:32:13,913 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 14:32:14,551 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@57e74904
   [druid] 2018-12-04 14:32:14,558 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/12/2018-11-12.log:0+14734
   [druid] 2018-12-04 14:32:14,608 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 14:32:14,608 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 14:32:14,608 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 14:32:14,608 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 14:32:14,608 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 14:32:14,612 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 14:32:14,827 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1934233952_0001 running in uber mode : false
   [druid] 2018-12-04 14:32:14,833 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 14:32:15,082 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 14:32:15,134 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 14:32:15,134 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-04 14:32:15,135 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1934233952_0001
   java.lang.Exception: java.lang.ArrayIndexOutOfBoundsException: 2
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
	at com.phone.analysis.mapreduce.ActiveMember.ActiveMemberMapper.map(ActiveMemberMapper.java:56)
	at com.phone.analysis.mapreduce.ActiveMember.ActiveMemberMapper.map(ActiveMemberMapper.java:28)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-04 14:32:15,835 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1934233952_0001 failed with state FAILED due to: NA
   [druid] 2018-12-04 14:32:15,845 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-04 14:36:00,867 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 14:36:00,868 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 14:36:01,481 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 14:36:01,576 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 14:36:01,615 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 14:36:01,690 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1904332242_0001
   [druid] 2018-12-04 14:36:01,801 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 14:36:01,802 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1904332242_0001
   [druid] 2018-12-04 14:36:01,803 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 14:36:01,808 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 14:36:01,808 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 14:36:01,812 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 14:36:01,845 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 14:36:01,846 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1904332242_0001_m_000000_0
   [druid] 2018-12-04 14:36:01,869 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 14:36:01,873 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 14:36:01,950 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-04 14:36:01,958 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 14:36:02,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 14:36:02,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 14:36:02,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 14:36:02,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 14:36:02,007 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 14:36:02,010 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 14:36:02,532 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 14:36:02,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 14:36:02,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 14:36:02,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 14:36:02,534 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 14:36:02,563 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 14:36:02,569 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1904332242_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 14:36:02,583 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 14:36:02,584 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1904332242_0001_m_000000_0' done.
   [druid] 2018-12-04 14:36:02,584 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1904332242_0001_m_000000_0
   [druid] 2018-12-04 14:36:02,584 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 14:36:02,685 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 14:36:02,685 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1904332242_0001_r_000000_0
   [druid] 2018-12-04 14:36:02,692 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 14:36:02,692 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 14:36:02,767 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6c0996ab
   [druid] 2018-12-04 14:36:02,789 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@60255ff
   [druid] 2018-12-04 14:36:02,803 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1904332242_0001 running in uber mode : false
   [druid] 2018-12-04 14:36:02,804 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 14:36:02,818 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 14:36:02,820 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1904332242_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 14:36:02,852 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1904332242_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 14:36:02,861 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local1904332242_0001_m_000000_0
   [druid] 2018-12-04 14:36:02,862 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 14:36:02,864 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 14:36:02,864 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 14:36:02,865 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 14:36:02,873 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 14:36:02,873 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 14:36:02,881 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 14:36:02,882 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 14:36:02,882 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 14:36:02,883 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 14:36:02,884 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 14:36:02,884 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 14:36:03,262 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 14:36:03,686 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1904332242_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 14:36:03,687 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 14:36:03,687 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1904332242_0001_r_000000_0' done.
   [druid] 2018-12-04 14:36:03,687 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1904332242_0001_r_000000_0
   [druid] 2018-12-04 14:36:03,687 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 14:36:03,692 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 14:36:03,814 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 14:36:03,814 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1904332242_0001 completed successfully
   [druid] 2018-12-04 14:36:03,825 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1269875
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 15:28:27,596 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 15:28:27,598 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 15:28:28,246 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 15:28:28,312 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 15:28:28,338 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 15:28:28,399 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local991972098_0001
   [druid] 2018-12-04 15:28:28,518 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 15:28:28,518 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local991972098_0001
   [druid] 2018-12-04 15:28:28,519 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 15:28:28,524 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 15:28:28,524 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 15:28:28,529 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 15:28:28,558 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 15:28:28,560 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local991972098_0001_m_000000_0
   [druid] 2018-12-04 15:28:28,579 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 15:28:28,584 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 15:28:28,811 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7f34b09c
   [druid] 2018-12-04 15:28:28,818 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 15:28:28,865 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 15:28:28,866 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 15:28:28,866 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 15:28:28,866 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 15:28:28,866 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 15:28:28,870 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 15:28:29,392 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 15:28:29,393 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 15:28:29,393 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 15:28:29,393 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 15:28:29,393 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 15:28:29,425 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 15:28:29,433 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local991972098_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 15:28:29,443 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 15:28:29,444 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local991972098_0001_m_000000_0' done.
   [druid] 2018-12-04 15:28:29,444 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local991972098_0001_m_000000_0
   [druid] 2018-12-04 15:28:29,444 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 15:28:29,445 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 15:28:29,446 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local991972098_0001_r_000000_0
   [druid] 2018-12-04 15:28:29,451 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 15:28:29,451 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 15:28:29,520 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local991972098_0001 running in uber mode : false
   [druid] 2018-12-04 15:28:29,521 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 15:28:29,524 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16937d04
   [druid] 2018-12-04 15:28:29,527 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f8bcec6
   [druid] 2018-12-04 15:28:29,536 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 15:28:29,539 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local991972098_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 15:28:29,567 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local991972098_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 15:28:29,573 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local991972098_0001_m_000000_0
   [druid] 2018-12-04 15:28:29,574 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 15:28:29,576 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 15:28:29,576 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 15:28:29,576 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 15:28:29,587 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 15:28:29,588 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 15:28:29,595 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 15:28:29,597 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 15:28:29,597 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 15:28:29,597 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 15:28:29,598 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 15:28:29,599 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 15:28:29,988 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 15:28:30,324 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local991972098_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 15:28:30,325 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 15:28:30,325 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local991972098_0001_r_000000_0' done.
   [druid] 2018-12-04 15:28:30,325 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local991972098_0001_r_000000_0
   [druid] 2018-12-04 15:28:30,325 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 15:28:30,329 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 15:28:30,523 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 15:28:30,523 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local991972098_0001 completed successfully
   [druid] 2018-12-04 15:28:30,535 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1266811
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 16:40:20,426 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 16:40:20,428 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 16:40:20,997 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 16:40:21,064 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 16:40:21,090 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 16:40:21,158 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1842627804_0001
   [druid] 2018-12-04 16:40:21,276 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 16:40:21,277 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1842627804_0001
   [druid] 2018-12-04 16:40:21,278 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 16:40:21,284 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:40:21,284 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 16:40:21,289 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 16:40:21,325 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 16:40:21,327 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1842627804_0001_m_000000_0
   [druid] 2018-12-04 16:40:21,349 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:40:21,354 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:40:21,559 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@118e4041
   [druid] 2018-12-04 16:40:21,565 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 16:40:21,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 16:40:21,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 16:40:21,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 16:40:21,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 16:40:21,621 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 16:40:21,624 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 16:40:22,175 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 16:40:22,176 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 16:40:22,176 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 16:40:22,176 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 16:40:22,176 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 16:40:22,210 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 16:40:22,216 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1842627804_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:40:22,225 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 16:40:22,226 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1842627804_0001_m_000000_0' done.
   [druid] 2018-12-04 16:40:22,226 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1842627804_0001_m_000000_0
   [druid] 2018-12-04 16:40:22,226 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 16:40:22,227 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 16:40:22,228 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1842627804_0001_r_000000_0
   [druid] 2018-12-04 16:40:22,234 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:40:22,234 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:40:22,279 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1842627804_0001 running in uber mode : false
   [druid] 2018-12-04 16:40:22,280 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 16:40:22,302 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63ccfc03
   [druid] 2018-12-04 16:40:22,305 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@31b8ebfd
   [druid] 2018-12-04 16:40:22,315 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 16:40:22,317 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1842627804_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 16:40:22,341 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1842627804_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 16:40:22,345 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local1842627804_0001_m_000000_0
   [druid] 2018-12-04 16:40:22,346 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 16:40:22,347 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 16:40:22,348 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:40:22,348 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 16:40:22,358 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:40:22,358 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 16:40:22,365 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 16:40:22,366 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 16:40:22,367 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 16:40:22,367 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:40:22,368 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 16:40:22,368 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:40:22,652 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 16:40:23,034 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1842627804_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:40:23,034 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 16:40:23,034 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1842627804_0001_r_000000_0' done.
   [druid] 2018-12-04 16:40:23,034 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1842627804_0001_r_000000_0
   [druid] 2018-12-04 16:40:23,034 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 16:40:23,040 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 16:40:23,283 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 16:40:23,283 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1842627804_0001 completed successfully
   [druid] 2018-12-04 16:40:23,294 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1269843
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 16:41:27,155 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 16:41:27,156 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 16:41:27,744 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 16:41:27,815 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 16:41:27,843 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 16:41:27,906 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local496150007_0001
   [druid] 2018-12-04 16:41:28,020 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 16:41:28,021 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local496150007_0001
   [druid] 2018-12-04 16:41:28,022 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 16:41:28,027 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:41:28,027 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 16:41:28,033 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 16:41:28,069 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 16:41:28,071 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local496150007_0001_m_000000_0
   [druid] 2018-12-04 16:41:28,093 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:41:28,096 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:41:28,170 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b26ba3e
   [druid] 2018-12-04 16:41:28,177 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 16:41:28,222 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 16:41:28,222 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 16:41:28,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 16:41:28,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 16:41:28,223 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 16:41:28,226 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 16:41:28,748 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 16:41:28,750 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 16:41:28,750 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 16:41:28,750 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 16:41:28,750 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 16:41:28,784 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 16:41:28,790 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local496150007_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:41:28,799 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 16:41:28,799 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local496150007_0001_m_000000_0' done.
   [druid] 2018-12-04 16:41:28,799 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local496150007_0001_m_000000_0
   [druid] 2018-12-04 16:41:28,799 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 16:41:28,801 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 16:41:28,801 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local496150007_0001_r_000000_0
   [druid] 2018-12-04 16:41:28,806 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:41:28,807 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:41:28,874 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9dcec4a
   [druid] 2018-12-04 16:41:28,876 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2e36578d
   [druid] 2018-12-04 16:41:28,887 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 16:41:28,888 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local496150007_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 16:41:28,914 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local496150007_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 16:41:28,917 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local496150007_0001_m_000000_0
   [druid] 2018-12-04 16:41:28,919 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 16:41:28,920 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 16:41:28,921 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:41:28,921 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 16:41:28,932 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:41:28,932 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 16:41:28,939 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 16:41:28,940 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 16:41:28,940 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 16:41:28,940 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:41:28,941 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 16:41:28,942 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:41:29,023 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local496150007_0001 running in uber mode : false
   [druid] 2018-12-04 16:41:29,024 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 16:41:29,281 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 16:41:29,770 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local496150007_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:41:29,770 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 16:41:29,771 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local496150007_0001_r_000000_0' done.
   [druid] 2018-12-04 16:41:29,771 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local496150007_0001_r_000000_0
   [druid] 2018-12-04 16:41:29,771 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 16:41:29,775 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 16:41:30,025 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 16:41:30,025 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local496150007_0001 completed successfully
   [druid] 2018-12-04 16:41:30,039 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1266779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=634388480
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 16:56:02,246 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 16:56:02,248 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 16:56:02,851 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 16:56:02,920 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 16:56:02,950 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 16:56:03,012 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1112511500_0001
   [druid] 2018-12-04 16:56:03,125 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 16:56:03,126 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1112511500_0001
   [druid] 2018-12-04 16:56:03,127 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 16:56:03,130 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:56:03,130 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 16:56:03,135 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 16:56:03,168 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 16:56:03,170 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1112511500_0001_m_000000_0
   [druid] 2018-12-04 16:56:03,188 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:56:03,192 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:56:03,262 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16894b5c
   [druid] 2018-12-04 16:56:03,269 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 16:56:03,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 16:56:03,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 16:56:03,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 16:56:03,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 16:56:03,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 16:56:03,320 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 16:56:03,831 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 16:56:03,833 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 16:56:03,833 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 16:56:03,833 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-04 16:56:03,833 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-04 16:56:03,848 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 16:56:03,855 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1112511500_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:56:03,864 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 16:56:03,864 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1112511500_0001_m_000000_0' done.
   [druid] 2018-12-04 16:56:03,864 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1112511500_0001_m_000000_0
   [druid] 2018-12-04 16:56:03,865 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 16:56:03,867 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 16:56:03,867 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1112511500_0001_r_000000_0
   [druid] 2018-12-04 16:56:03,872 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 16:56:03,873 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 16:56:03,942 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@63d98ba4
   [druid] 2018-12-04 16:56:03,944 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@1cb7f9bb
   [druid] 2018-12-04 16:56:03,955 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 16:56:03,957 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1112511500_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 16:56:03,983 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1112511500_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-04 16:56:03,988 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local1112511500_0001_m_000000_0
   [druid] 2018-12-04 16:56:03,989 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-04 16:56:03,990 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 16:56:03,991 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:56:03,991 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 16:56:03,999 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:56:03,999 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-04 16:56:04,002 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 16:56:04,004 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-04 16:56:04,005 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 16:56:04,005 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 16:56:04,005 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-04 16:56:04,006 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 16:56:04,127 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1112511500_0001 running in uber mode : false
   [druid] 2018-12-04 16:56:04,128 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 16:56:04,288 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 16:56:04,652 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1112511500_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 16:56:04,653 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 16:56:04,653 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1112511500_0001_r_000000_0' done.
   [druid] 2018-12-04 16:56:04,653 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1112511500_0001_r_000000_0
   [druid] 2018-12-04 16:56:04,653 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 16:56:04,658 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 16:56:05,130 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 16:56:05,130 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1112511500_0001 completed successfully
   [druid] 2018-12-04 16:56:05,142 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=691190
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 17:19:51,048 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 17:19:51,050 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 17:19:51,625 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 17:19:51,693 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 17:19:51,719 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 17:19:51,783 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local902207528_0001
   [druid] 2018-12-04 17:19:51,901 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 17:19:51,902 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local902207528_0001
   [druid] 2018-12-04 17:19:51,902 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 17:19:51,906 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 17:19:51,906 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 17:19:51,911 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 17:19:51,940 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 17:19:51,941 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local902207528_0001_m_000000_0
   [druid] 2018-12-04 17:19:51,961 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 17:19:51,965 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 17:19:52,230 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@705d0ab2
   [druid] 2018-12-04 17:19:52,237 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 17:19:52,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 17:19:52,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 17:19:52,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 17:19:52,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 17:19:52,284 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 17:19:52,288 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 17:19:52,811 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 17:19:52,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 17:19:52,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 17:19:52,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 17:19:52,813 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 17:19:52,849 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 17:19:52,857 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local902207528_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 17:19:52,865 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 17:19:52,865 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local902207528_0001_m_000000_0' done.
   [druid] 2018-12-04 17:19:52,865 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local902207528_0001_m_000000_0
   [druid] 2018-12-04 17:19:52,865 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 17:19:52,867 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 17:19:52,867 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local902207528_0001_r_000000_0
   [druid] 2018-12-04 17:19:52,872 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 17:19:52,872 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 17:19:52,904 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local902207528_0001 running in uber mode : false
   [druid] 2018-12-04 17:19:52,905 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 17:19:52,941 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7790abb9
   [druid] 2018-12-04 17:19:52,944 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@4da4410a
   [druid] 2018-12-04 17:19:52,953 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 17:19:52,956 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local902207528_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 17:19:52,983 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local902207528_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 17:19:52,990 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local902207528_0001_m_000000_0
   [druid] 2018-12-04 17:19:52,991 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 17:19:52,993 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 17:19:52,994 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 17:19:52,994 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 17:19:53,003 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 17:19:53,003 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 17:19:53,009 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 17:19:53,010 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 17:19:53,010 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 17:19:53,010 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 17:19:53,011 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 17:19:53,011 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 17:19:53,296 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 17:19:53,808 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local902207528_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 17:19:53,808 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 17:19:53,808 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local902207528_0001_r_000000_0' done.
   [druid] 2018-12-04 17:19:53,808 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local902207528_0001_r_000000_0
   [druid] 2018-12-04 17:19:53,808 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 17:19:53,813 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 17:19:53,906 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 17:19:53,906 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local902207528_0001 completed successfully
   [druid] 2018-12-04 17:19:53,919 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1266779
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 18:04:35,662 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 18:04:35,664 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 18:04:36,228 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 18:04:36,295 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 18:04:36,324 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 18:04:36,390 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local466404511_0001
   [druid] 2018-12-04 18:04:36,507 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 18:04:36,508 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local466404511_0001
   [druid] 2018-12-04 18:04:36,508 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 18:04:36,512 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 18:04:36,512 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 18:04:36,517 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 18:04:36,549 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 18:04:36,551 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local466404511_0001_m_000000_0
   [druid] 2018-12-04 18:04:36,570 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 18:04:36,574 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 18:04:36,793 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@221e5249
   [druid] 2018-12-04 18:04:36,800 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 18:04:36,846 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 18:04:36,846 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 18:04:36,846 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 18:04:36,846 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 18:04:36,846 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 18:04:36,849 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 18:04:37,320 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 18:04:37,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 18:04:37,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 18:04:37,321 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 30644; bufvoid = 104857600
   [druid] 2018-12-04 18:04:37,322 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26213456(104853824); length = 941/6553600
   [druid] 2018-12-04 18:04:37,337 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 18:04:37,343 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local466404511_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 18:04:37,353 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 18:04:37,353 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local466404511_0001_m_000000_0' done.
   [druid] 2018-12-04 18:04:37,353 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local466404511_0001_m_000000_0
   [druid] 2018-12-04 18:04:37,353 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 18:04:37,356 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 18:04:37,356 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local466404511_0001_r_000000_0
   [druid] 2018-12-04 18:04:37,362 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 18:04:37,362 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 18:04:37,434 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7ad67c0
   [druid] 2018-12-04 18:04:37,437 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@e767335
   [druid] 2018-12-04 18:04:37,447 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 18:04:37,449 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local466404511_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 18:04:37,475 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local466404511_0001_m_000000_0 decomp: 31118 len: 31122 to MEMORY
   [druid] 2018-12-04 18:04:37,480 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 31118 bytes from map-output for attempt_local466404511_0001_m_000000_0
   [druid] 2018-12-04 18:04:37,482 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 31118, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->31118
   [druid] 2018-12-04 18:04:37,483 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 18:04:37,484 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 18:04:37,484 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 18:04:37,494 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 18:04:37,494 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-04 18:04:37,497 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 31118 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 18:04:37,497 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 31122 bytes from disk
   [druid] 2018-12-04 18:04:37,498 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 18:04:37,498 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 18:04:37,499 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 31044 bytes
   [druid] 2018-12-04 18:04:37,499 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 18:04:37,509 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local466404511_0001 running in uber mode : false
   [druid] 2018-12-04 18:04:37,510 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 18:04:37,760 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 18:04:37,923 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local466404511_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 18:04:37,924 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 18:04:37,924 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local466404511_0001_r_000000_0' done.
   [druid] 2018-12-04 18:04:37,924 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local466404511_0001_r_000000_0
   [druid] 2018-12-04 18:04:37,924 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 18:04:37,928 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 18:04:38,510 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 18:04:38,510 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local466404511_0001 completed successfully
   [druid] 2018-12-04 18:04:38,520 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=62596
		FILE: Number of bytes written=688126
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=236
		Map output bytes=30644
		Map output materialized bytes=31122
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=7
		Reduce shuffle bytes=31122
		Reduce input records=236
		Reduce output records=7
		Spilled Records=472
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 19:06:28,491 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 19:06:28,492 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 19:06:29,110 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 19:06:29,196 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 19:06:29,232 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 19:06:29,308 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local552729291_0001
   [druid] 2018-12-04 19:06:29,441 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 19:06:29,442 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local552729291_0001
   [druid] 2018-12-04 19:06:29,443 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 19:06:29,449 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:06:29,449 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 19:06:29,455 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 19:06:29,488 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 19:06:29,489 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local552729291_0001_m_000000_0
   [druid] 2018-12-04 19:06:29,507 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:06:29,511 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:06:29,707 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16894b5c
   [druid] 2018-12-04 19:06:29,714 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 19:06:29,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 19:06:29,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 19:06:29,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 19:06:29,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 19:06:29,768 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 19:06:29,771 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 19:06:30,218 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,219 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,219 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,219 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,219 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,219 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,220 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,220 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,220 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:06:30,351 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 19:06:30,353 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 19:06:30,354 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 19:06:30,354 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 369512; bufvoid = 104857600
   [druid] 2018-12-04 19:06:30,354 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-04 19:06:30,407 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 19:06:30,413 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local552729291_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:06:30,424 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 19:06:30,425 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local552729291_0001_m_000000_0' done.
   [druid] 2018-12-04 19:06:30,425 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local552729291_0001_m_000000_0
   [druid] 2018-12-04 19:06:30,425 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 19:06:30,427 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 19:06:30,427 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local552729291_0001_r_000000_0
   [druid] 2018-12-04 19:06:30,433 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:06:30,433 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:06:30,445 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local552729291_0001 running in uber mode : false
   [druid] 2018-12-04 19:06:30,446 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 19:06:30,511 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@40dbef79
   [druid] 2018-12-04 19:06:30,515 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@79042d9b
   [druid] 2018-12-04 19:06:30,528 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 19:06:30,530 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local552729291_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 19:06:30,561 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local552729291_0001_m_000000_0 decomp: 375838 len: 375842 to MEMORY
   [druid] 2018-12-04 19:06:30,566 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 375838 bytes from map-output for attempt_local552729291_0001_m_000000_0
   [druid] 2018-12-04 19:06:30,567 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 375838, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->375838
   [druid] 2018-12-04 19:06:30,568 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 19:06:30,569 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:06:30,569 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 19:06:30,579 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:06:30,579 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:06:30,588 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 375838 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 19:06:30,589 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 375842 bytes from disk
   [druid] 2018-12-04 19:06:30,590 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 19:06:30,590 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:06:30,591 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:06:30,591 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:06:30,869 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 19:06:31,236 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local552729291_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:06:31,237 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 19:06:31,237 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local552729291_0001_r_000000_0' done.
   [druid] 2018-12-04 19:06:31,237 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local552729291_0001_r_000000_0
   [druid] 2018-12-04 19:06:31,237 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 19:06:31,243 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 19:06:31,447 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 19:06:31,447 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local552729291_0001 completed successfully
   [druid] 2018-12-04 19:06:31,463 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=752036
		FILE: Number of bytes written=1722326
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=369512
		Map output materialized bytes=375842
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=375842
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 19:07:41,184 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 19:07:41,185 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 19:07:41,769 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 19:07:41,842 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 19:07:41,871 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 19:07:41,937 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local433857848_0001
   [druid] 2018-12-04 19:07:42,058 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 19:07:42,059 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local433857848_0001
   [druid] 2018-12-04 19:07:42,060 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 19:07:42,064 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:07:42,064 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 19:07:42,069 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 19:07:42,100 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 19:07:42,101 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local433857848_0001_m_000000_0
   [druid] 2018-12-04 19:07:42,121 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:07:42,125 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:07:42,199 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@38522b56
   [druid] 2018-12-04 19:07:42,206 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 19:07:42,255 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 19:07:42,255 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 19:07:42,255 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 19:07:42,255 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 19:07:42,255 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 19:07:42,257 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 19:07:42,721 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,722 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:07:42,830 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 19:07:42,831 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 19:07:42,832 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 19:07:42,832 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 369512; bufvoid = 104857600
   [druid] 2018-12-04 19:07:42,832 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-04 19:07:42,881 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 19:07:42,889 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local433857848_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:07:42,899 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 19:07:42,899 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local433857848_0001_m_000000_0' done.
   [druid] 2018-12-04 19:07:42,900 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local433857848_0001_m_000000_0
   [druid] 2018-12-04 19:07:42,900 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 19:07:42,902 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 19:07:42,902 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local433857848_0001_r_000000_0
   [druid] 2018-12-04 19:07:42,907 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:07:42,908 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:07:42,982 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b304c91
   [druid] 2018-12-04 19:07:42,984 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@138a4e0f
   [druid] 2018-12-04 19:07:42,997 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 19:07:42,999 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local433857848_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 19:07:43,032 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local433857848_0001_m_000000_0 decomp: 375838 len: 375842 to MEMORY
   [druid] 2018-12-04 19:07:43,037 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 375838 bytes from map-output for attempt_local433857848_0001_m_000000_0
   [druid] 2018-12-04 19:07:43,038 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 375838, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->375838
   [druid] 2018-12-04 19:07:43,039 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 19:07:43,040 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:07:43,041 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 19:07:43,052 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:07:43,052 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:07:43,060 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local433857848_0001 running in uber mode : false
   [druid] 2018-12-04 19:07:43,064 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 19:07:43,065 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 375838 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 19:07:43,066 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 375842 bytes from disk
   [druid] 2018-12-04 19:07:43,067 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 19:07:43,067 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:07:43,067 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:07:43,068 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:07:43,363 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 19:07:43,746 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local433857848_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:07:43,746 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 19:07:43,746 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local433857848_0001_r_000000_0' done.
   [druid] 2018-12-04 19:07:43,746 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local433857848_0001_r_000000_0
   [druid] 2018-12-04 19:07:43,747 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 19:07:43,752 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 19:07:44,065 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 19:07:44,065 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local433857848_0001 completed successfully
   [druid] 2018-12-04 19:07:44,077 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=752036
		FILE: Number of bytes written=1722326
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=369512
		Map output materialized bytes=375842
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=375842
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=631242752
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 19:24:10,472 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 19:24:10,473 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 19:24:11,093 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 19:24:11,168 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 19:24:11,196 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 19:24:11,256 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1617527751_0001
   [druid] 2018-12-04 19:24:11,387 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 19:24:11,388 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1617527751_0001
   [druid] 2018-12-04 19:24:11,390 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 19:24:11,395 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:24:11,395 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 19:24:11,400 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 19:24:11,432 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 19:24:11,433 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1617527751_0001_m_000000_0
   [druid] 2018-12-04 19:24:11,456 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:24:11,460 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:24:11,532 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@70aa126b
   [druid] 2018-12-04 19:24:11,539 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 19:24:11,589 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 19:24:11,589 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 19:24:11,589 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 19:24:11,589 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 19:24:11,589 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 19:24:11,592 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 19:24:12,019 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,019 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,020 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,021 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 19:24:12,131 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 19:24:12,133 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 19:24:12,133 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 19:24:12,133 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 369512; bufvoid = 104857600
   [druid] 2018-12-04 19:24:12,133 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-04 19:24:12,179 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 19:24:12,187 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1617527751_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:24:12,200 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 19:24:12,200 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1617527751_0001_m_000000_0' done.
   [druid] 2018-12-04 19:24:12,200 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1617527751_0001_m_000000_0
   [druid] 2018-12-04 19:24:12,201 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 19:24:12,203 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 19:24:12,203 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1617527751_0001_r_000000_0
   [druid] 2018-12-04 19:24:12,211 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 19:24:12,211 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 19:24:12,295 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@169bf4c2
   [druid] 2018-12-04 19:24:12,297 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2d279f03
   [druid] 2018-12-04 19:24:12,310 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 19:24:12,312 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1617527751_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 19:24:12,344 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1617527751_0001_m_000000_0 decomp: 375838 len: 375842 to MEMORY
   [druid] 2018-12-04 19:24:12,350 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 375838 bytes from map-output for attempt_local1617527751_0001_m_000000_0
   [druid] 2018-12-04 19:24:12,352 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 375838, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->375838
   [druid] 2018-12-04 19:24:12,353 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 19:24:12,353 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:24:12,353 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 19:24:12,364 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:24:12,364 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:24:12,374 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 375838 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 19:24:12,375 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 375842 bytes from disk
   [druid] 2018-12-04 19:24:12,376 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 19:24:12,376 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 19:24:12,377 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 19:24:12,378 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 19:24:12,391 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1617527751_0001 running in uber mode : false
   [druid] 2018-12-04 19:24:12,392 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 19:24:12,650 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 19:24:12,981 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1617527751_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 19:24:12,982 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 19:24:12,982 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1617527751_0001_r_000000_0' done.
   [druid] 2018-12-04 19:24:12,982 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1617527751_0001_r_000000_0
   [druid] 2018-12-04 19:24:12,982 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 19:24:12,988 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 19:24:13,393 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 19:24:13,393 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1617527751_0001 completed successfully
   [druid] 2018-12-04 19:24:13,406 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=752036
		FILE: Number of bytes written=1725390
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=369512
		Map output materialized bytes=375842
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=375842
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 20:29:09,425 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 20:29:09,427 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 20:29:10,100 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 20:29:10,177 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 20:29:10,206 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 20:29:10,272 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1517351835_0001
   [druid] 2018-12-04 20:29:10,402 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 20:29:10,403 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1517351835_0001
   [druid] 2018-12-04 20:29:10,404 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 20:29:10,409 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:29:10,409 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 20:29:10,415 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 20:29:10,450 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 20:29:10,451 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1517351835_0001_m_000000_0
   [druid] 2018-12-04 20:29:10,473 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:29:10,477 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:29:10,704 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@795c5eab
   [druid] 2018-12-04 20:29:10,711 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 20:29:10,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 20:29:10,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 20:29:10,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 20:29:10,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 20:29:10,769 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 20:29:10,772 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 20:29:11,227 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,227 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,227 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,313 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:29:11,405 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1517351835_0001 running in uber mode : false
   [druid] 2018-12-04 20:29:11,407 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 20:29:11,429 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:29:11,432 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 20:29:11,432 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 20:29:11,432 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 369512; bufvoid = 104857600
   [druid] 2018-12-04 20:29:11,432 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-04 20:29:11,496 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 20:29:11,503 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1517351835_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:29:11,511 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 20:29:11,512 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1517351835_0001_m_000000_0' done.
   [druid] 2018-12-04 20:29:11,512 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1517351835_0001_m_000000_0
   [druid] 2018-12-04 20:29:11,512 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 20:29:11,514 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 20:29:11,514 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1517351835_0001_r_000000_0
   [druid] 2018-12-04 20:29:11,519 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:29:11,520 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:29:11,590 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@56b4e0da
   [druid] 2018-12-04 20:29:11,593 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@21f15c68
   [druid] 2018-12-04 20:29:11,604 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 20:29:11,606 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1517351835_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 20:29:11,636 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1517351835_0001_m_000000_0 decomp: 375838 len: 375842 to MEMORY
   [druid] 2018-12-04 20:29:11,641 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 375838 bytes from map-output for attempt_local1517351835_0001_m_000000_0
   [druid] 2018-12-04 20:29:11,643 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 375838, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->375838
   [druid] 2018-12-04 20:29:11,646 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 20:29:11,646 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 20:29:11,646 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 20:29:11,657 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 20:29:11,658 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 20:29:11,668 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 375838 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 20:29:11,669 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 375842 bytes from disk
   [druid] 2018-12-04 20:29:11,669 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 20:29:11,670 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 20:29:11,671 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 20:29:11,671 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 20:29:11,942 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 20:29:12,286 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1517351835_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:29:12,286 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 20:29:12,287 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1517351835_0001_r_000000_0' done.
   [druid] 2018-12-04 20:29:12,287 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1517351835_0001_r_000000_0
   [druid] 2018-12-04 20:29:12,287 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 20:29:12,291 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 20:29:12,409 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 20:29:12,409 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1517351835_0001 completed successfully
   [druid] 2018-12-04 20:29:12,421 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=752036
		FILE: Number of bytes written=1727294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=369512
		Map output materialized bytes=375842
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=375842
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=633339904
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 20:31:19,556 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 20:31:19,557 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 20:31:20,167 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 20:31:20,246 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 20:31:20,276 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 20:31:20,345 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1655312811_0001
   [druid] 2018-12-04 20:31:20,478 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 20:31:20,479 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1655312811_0001
   [druid] 2018-12-04 20:31:20,480 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 20:31:20,484 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:31:20,485 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 20:31:20,490 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 20:31:20,525 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 20:31:20,526 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1655312811_0001_m_000000_0
   [druid] 2018-12-04 20:31:20,546 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:31:20,549 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:31:20,623 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@320a91aa
   [druid] 2018-12-04 20:31:20,631 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 20:31:20,681 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 20:31:20,681 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 20:31:20,681 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 20:31:20,681 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 20:31:20,681 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 20:31:20,684 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 20:31:21,150 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,150 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,150 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,150 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,150 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,152 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,152 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,152 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,152 [ask Executor #0] INFO  duce.NewMember.NewMemberMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 20:31:21,265 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:31:21,268 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 20:31:21,268 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 20:31:21,268 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 369512; bufvoid = 104857600
   [druid] 2018-12-04 20:31:21,268 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26201752(104807008); length = 12645/6553600
   [druid] 2018-12-04 20:31:21,316 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 20:31:21,320 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1655312811_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:31:21,328 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 20:31:21,328 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1655312811_0001_m_000000_0' done.
   [druid] 2018-12-04 20:31:21,328 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1655312811_0001_m_000000_0
   [druid] 2018-12-04 20:31:21,328 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 20:31:21,331 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 20:31:21,331 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1655312811_0001_r_000000_0
   [druid] 2018-12-04 20:31:21,335 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:31:21,335 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:31:21,409 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@6b304c91
   [druid] 2018-12-04 20:31:21,414 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@138a4e0f
   [druid] 2018-12-04 20:31:21,429 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 20:31:21,431 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1655312811_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 20:31:21,462 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1655312811_0001_m_000000_0 decomp: 375838 len: 375842 to MEMORY
   [druid] 2018-12-04 20:31:21,467 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 375838 bytes from map-output for attempt_local1655312811_0001_m_000000_0
   [druid] 2018-12-04 20:31:21,469 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 375838, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->375838
   [druid] 2018-12-04 20:31:21,470 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 20:31:21,470 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 20:31:21,471 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 20:31:21,481 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 20:31:21,482 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1655312811_0001 running in uber mode : false
   [druid] 2018-12-04 20:31:21,482 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 20:31:21,483 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 20:31:21,491 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 375838 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 20:31:21,492 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 375842 bytes from disk
   [druid] 2018-12-04 20:31:21,492 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 20:31:21,492 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 20:31:21,493 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 375765 bytes
   [druid] 2018-12-04 20:31:21,494 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 20:31:21,767 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 20:31:22,133 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1655312811_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:31:22,133 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 20:31:22,133 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1655312811_0001_r_000000_0' done.
   [druid] 2018-12-04 20:31:22,133 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1655312811_0001_r_000000_0
   [druid] 2018-12-04 20:31:22,133 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 20:31:22,138 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 20:31:22,485 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 20:31:22,485 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1655312811_0001 completed successfully
   [druid] 2018-12-04 20:31:22,502 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=752036
		FILE: Number of bytes written=1727294
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=3162
		Map output bytes=369512
		Map output materialized bytes=375842
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=14
		Reduce shuffle bytes=375842
		Reduce input records=3162
		Reduce output records=14
		Spilled Records=6324
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 20:47:28,204 [main           ] ERROR e.etl.mapreduce.Etl2HdfsDriver {1} - 设置输入输出路径异常
   java.lang.RuntimeException: 输入路径不存在/logs/2018-05-28
	at com.phone.etl.mapreduce.Etl2HdfsDriver.handleInputOutput(Etl2HdfsDriver.java:77)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.run(Etl2HdfsDriver.java:55)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.main(Etl2HdfsDriver.java:31)
[druid] 2018-12-04 20:47:28,242 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 20:47:28,243 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 20:47:28,256 [main           ] ERROR e.etl.mapreduce.Etl2HdfsDriver {1} - 执行etl异常
   org.apache.hadoop.mapred.InvalidJobConfException: Output directory not set.
	at org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:138)
	at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:266)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
	at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.run(Etl2HdfsDriver.java:56)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
	at com.phone.etl.mapreduce.Etl2HdfsDriver.main(Etl2HdfsDriver.java:31)
[druid] 2018-12-04 20:48:06,790 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 20:48:06,791 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 20:48:07,461 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 20:48:07,532 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 20:48:07,564 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 20:48:07,642 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local955486143_0001
   [druid] 2018-12-04 20:48:07,774 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 20:48:07,774 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local955486143_0001
   [druid] 2018-12-04 20:48:07,775 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 20:48:07,780 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:48:07,783 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 20:48:07,833 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 20:48:07,834 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local955486143_0001_m_000000_0
   [druid] 2018-12-04 20:48:07,855 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:48:07,861 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:48:07,934 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@62ce10d6
   [druid] 2018-12-04 20:48:07,938 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/logs/2017-05-28/20170528.log:0+905
   [druid] 2018-12-04 20:48:08,777 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local955486143_0001 running in uber mode : false
   [druid] 2018-12-04 20:48:08,777 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 20:48:08,859 [ask Executor #0] INFO  e.etl.mapreduce.Etl2HdfsMapper {1} - 输入：2过滤：0输出：43
   [druid] 2018-12-04 20:48:08,861 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:48:09,196 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local955486143_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:48:09,204 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:48:09,204 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local955486143_0001_m_000000_0 is allowed to commit now
   [druid] 2018-12-04 20:48:09,216 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local955486143_0001_m_000000_0' to hdfs://Murphy:8020/ods/05/28/_temporary/0/task_local955486143_0001_m_000000
   [druid] 2018-12-04 20:48:09,217 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 20:48:09,218 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local955486143_0001_m_000000_0' done.
   [druid] 2018-12-04 20:48:09,218 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local955486143_0001_m_000000_0
   [druid] 2018-12-04 20:48:09,218 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 20:48:09,782 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 20:48:09,782 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local955486143_0001 completed successfully
   [druid] 2018-12-04 20:48:09,792 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 20
	File System Counters
		FILE: Number of bytes read=165
		FILE: Number of bytes written=277003
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=905
		HDFS: Number of bytes written=15042
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Map-Reduce Framework
		Map input records=2
		Map output records=43
		Input split bytes=112
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=212336640
	File Input Format Counters 
		Bytes Read=905
	File Output Format Counters 
		Bytes Written=15042
   [druid] 2018-12-04 20:49:27,296 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 20:49:27,298 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 20:49:27,942 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 20:49:28,015 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 20:49:28,047 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 20:49:28,114 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1319662496_0001
   [druid] 2018-12-04 20:49:28,248 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 20:49:28,248 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1319662496_0001
   [druid] 2018-12-04 20:49:28,249 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 20:49:28,253 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:49:28,256 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 20:49:28,391 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 20:49:28,391 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1319662496_0001_m_000000_0
   [druid] 2018-12-04 20:49:28,413 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 20:49:28,419 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 20:49:28,503 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@259fa7ae
   [druid] 2018-12-04 20:49:28,508 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/logs/2017-05-29/20170529.log:0+3693
   [druid] 2018-12-04 20:49:29,122 [ask Executor #0] INFO  e.etl.mapreduce.Etl2HdfsMapper {1} - 输入：10过滤：0输出：187
   [druid] 2018-12-04 20:49:29,125 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:49:29,246 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1319662496_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 20:49:29,250 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1319662496_0001 running in uber mode : false
   [druid] 2018-12-04 20:49:29,251 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 20:49:29,255 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 20:49:29,255 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task attempt_local1319662496_0001_m_000000_0 is allowed to commit now
   [druid] 2018-12-04 20:49:29,265 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - Saved output of task 'attempt_local1319662496_0001_m_000000_0' to hdfs://Murphy:8020/ods/05/29/_temporary/0/task_local1319662496_0001_m_000000
   [druid] 2018-12-04 20:49:29,265 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 20:49:29,265 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1319662496_0001_m_000000_0' done.
   [druid] 2018-12-04 20:49:29,266 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1319662496_0001_m_000000_0
   [druid] 2018-12-04 20:49:29,266 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 20:49:30,254 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 20:49:30,254 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1319662496_0001 completed successfully
   [druid] 2018-12-04 20:49:30,263 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 20
	File System Counters
		FILE: Number of bytes read=165
		FILE: Number of bytes written=278463
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=3693
		HDFS: Number of bytes written=91736
		HDFS: Number of read operations=9
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=3
	Map-Reduce Framework
		Map input records=10
		Map output records=187
		Input split bytes=112
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=212336640
	File Input Format Counters 
		Bytes Read=3693
	File Output Format Counters 
		Bytes Written=91736
   [druid] 2018-12-04 21:08:11,456 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:08:11,458 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:08:12,165 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:08:12,244 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:08:12,271 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:08:12,344 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local99143897_0001
   [druid] 2018-12-04 21:08:12,486 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:08:12,487 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local99143897_0001
   [druid] 2018-12-04 21:08:12,488 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:08:12,493 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:08:12,493 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:08:12,499 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:08:12,535 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:08:12,536 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local99143897_0001_m_000000_0
   [druid] 2018-12-04 21:08:12,560 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:08:12,565 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:08:12,779 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@513ac1d3
   [druid] 2018-12-04 21:08:12,787 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:08:12,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:08:12,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:08:12,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:08:12,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:08:12,844 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:08:12,847 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:08:13,297 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,298 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,298 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,298 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,299 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:08:13,303 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:08:13,441 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:08:13,441 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-04 21:08:13,442 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local99143897_0001
   java.lang.Exception: java.lang.NullPointerException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.NullPointerException
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:347)
	at java.io.DataOutputStream.writeUTF(DataOutputStream.java:323)
	at com.phone.analysis.model.BasicDemension.DateDimension.write(DateDimension.java:165)
	at com.phone.analysis.model.key.StatsCommonDimension.write(StatsCommonDimension.java:54)
	at com.phone.analysis.model.key.StatsUserDimension.write(StatsUserDimension.java:48)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1149)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:715)
	at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)
	at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserMapper.map(NewTotalUserMapper.java:84)
	at com.phone.analysis.mapreduce.NewTotalUser.NewTotalUserMapper.map(NewTotalUserMapper.java:28)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[druid] 2018-12-04 21:08:13,490 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local99143897_0001 running in uber mode : false
   [druid] 2018-12-04 21:08:13,491 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 21:08:13,493 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local99143897_0001 failed with state FAILED due to: NA
   [druid] 2018-12-04 21:08:13,498 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 0
   [druid] 2018-12-04 21:09:01,957 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:09:01,958 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:09:02,593 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:09:02,669 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:09:02,699 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:09:02,767 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1362740441_0001
   [druid] 2018-12-04 21:09:02,906 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:09:02,907 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1362740441_0001
   [druid] 2018-12-04 21:09:02,908 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:09:02,914 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:09:02,914 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:09:02,920 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:09:02,951 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:09:02,953 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1362740441_0001_m_000000_0
   [druid] 2018-12-04 21:09:02,976 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:09:02,979 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:09:03,055 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@520baba8
   [druid] 2018-12-04 21:09:03,063 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:09:03,113 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:09:03,113 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:09:03,114 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:09:03,114 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:09:03,114 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:09:03,118 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:09:03,561 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,561 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,561 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,561 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,562 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,562 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,562 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,562 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,562 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:09:03,571 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:09:03,573 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:09:03,573 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:09:03,573 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1500; bufvoid = 104857600
   [druid] 2018-12-04 21:09:03,573 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:09:03,667 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:09:03,667 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (RESET) equator 0 kv 26214396(104857584) kvi 26214300(104857200)
   [druid] 2018-12-04 21:09:03,667 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:09:03,667 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 1500; bufvoid = 104857600
   [druid] 2018-12-04 21:09:03,667 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:09:03,672 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewOutputCollector@1c9d09e5
   java.lang.RuntimeException: java.io.EOFException
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:165)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.compare(MapTask.java:1268)
	at org.apache.hadoop.util.QuickSort.fix(QuickSort.java:35)
	at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:87)
	at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:63)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1600)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1489)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)
	at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:2019)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:797)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.readFields(PlatformDimension.java:92)
	at com.phone.analysis.model.key.StatsCommonDimension.readFields(StatsCommonDimension.java:62)
	at com.phone.analysis.model.key.StatsUserDimension.readFields(StatsUserDimension.java:54)
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:158)
	... 16 more
[druid] 2018-12-04 21:09:03,674 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:09:03,675 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in cleanupJob()
   [druid] 2018-12-04 21:09:03,675 [Thread-3       ] WARN  e.hadoop.mapred.LocalJobRunner {1} - job_local1362740441_0001
   java.lang.Exception: java.lang.RuntimeException: java.io.EOFException
	at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)
Caused by: java.lang.RuntimeException: java.io.EOFException
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:165)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.compare(MapTask.java:1268)
	at org.apache.hadoop.util.QuickSort.fix(QuickSort.java:35)
	at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:87)
	at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:63)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1600)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1489)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at com.phone.analysis.model.BasicDemension.PlatformDimension.readFields(PlatformDimension.java:92)
	at com.phone.analysis.model.key.StatsCommonDimension.readFields(StatsCommonDimension.java:62)
	at com.phone.analysis.model.key.StatsUserDimension.readFields(StatsUserDimension.java:54)
	at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:158)
	... 15 more
[druid] 2018-12-04 21:09:03,909 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1362740441_0001 running in uber mode : false
   [druid] 2018-12-04 21:09:03,910 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 0% reduce 0%
   [druid] 2018-12-04 21:09:03,911 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1362740441_0001 failed with state FAILED due to: NA
   [druid] 2018-12-04 21:09:03,918 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 10
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=1500
		Map output materialized bytes=0
		Input split bytes=106
		Combine input records=0
		Spilled Records=0
		Failed Shuffles=0
		Merged Map outputs=0
	File Input Format Counters 
		Bytes Read=15042
   [druid] 2018-12-04 21:13:05,333 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:13:05,335 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:13:05,965 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:13:06,048 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:13:06,083 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:13:06,161 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local954273225_0001
   [druid] 2018-12-04 21:13:06,301 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:13:06,302 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local954273225_0001
   [druid] 2018-12-04 21:13:06,303 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:13:06,309 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:13:06,309 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:13:06,315 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:13:06,351 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:13:06,352 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local954273225_0001_m_000000_0
   [druid] 2018-12-04 21:13:06,373 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:13:06,379 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:13:06,454 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7f8016d6
   [druid] 2018-12-04 21:13:06,461 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:13:06,509 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:13:06,509 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:13:06,509 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:13:06,509 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:13:06,509 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:13:06,514 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,977 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:13:06,988 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:13:06,990 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:13:06,990 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:13:06,990 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:13:06,990 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:13:07,001 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:13:07,008 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local954273225_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:13:07,015 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:13:07,015 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local954273225_0001_m_000000_0' done.
   [druid] 2018-12-04 21:13:07,015 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local954273225_0001_m_000000_0
   [druid] 2018-12-04 21:13:07,015 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:13:07,018 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:13:07,018 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local954273225_0001_r_000000_0
   [druid] 2018-12-04 21:13:07,024 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:13:07,024 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:13:07,106 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@611114cd
   [druid] 2018-12-04 21:13:07,110 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@315c1733
   [druid] 2018-12-04 21:13:07,127 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:13:07,129 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local954273225_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:13:07,160 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local954273225_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:13:07,165 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local954273225_0001_m_000000_0
   [druid] 2018-12-04 21:13:07,167 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:13:07,168 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:13:07,169 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:13:07,169 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:13:07,179 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:13:07,180 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:13:07,182 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:13:07,183 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:13:07,184 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:13:07,184 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:13:07,185 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:13:07,185 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:13:07,304 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local954273225_0001 running in uber mode : false
   [druid] 2018-12-04 21:13:07,305 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:13:07,478 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:13:07,633 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local954273225_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:13:07,633 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:13:07,634 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local954273225_0001_r_000000_0' done.
   [druid] 2018-12-04 21:13:07,634 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local954273225_0001_r_000000_0
   [druid] 2018-12-04 21:13:07,634 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:13:07,638 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:13:08,307 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:13:08,307 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local954273225_0001 completed successfully
   [druid] 2018-12-04 21:13:08,318 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=610520
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=601882624
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:14:02,262 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:14:02,263 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:14:02,976 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:14:03,054 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:14:03,084 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:14:03,148 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1050986868_0001
   [druid] 2018-12-04 21:14:03,285 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:14:03,286 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1050986868_0001
   [druid] 2018-12-04 21:14:03,287 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:14:03,292 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:14:03,292 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:14:03,297 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:14:03,329 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:14:03,330 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1050986868_0001_m_000000_0
   [druid] 2018-12-04 21:14:03,351 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:14:03,355 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:14:03,431 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@3c6b45a2
   [druid] 2018-12-04 21:14:03,438 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:14:03,487 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:14:03,487 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:14:03,487 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:14:03,487 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:14:03,487 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:14:03,491 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:14:03,952 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,953 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:14:03,966 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:14:03,969 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:14:03,969 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:14:03,969 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:14:03,969 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:14:03,981 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:14:03,986 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1050986868_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:14:03,996 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:14:03,996 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1050986868_0001_m_000000_0' done.
   [druid] 2018-12-04 21:14:03,996 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1050986868_0001_m_000000_0
   [druid] 2018-12-04 21:14:03,996 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:14:03,998 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:14:03,998 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1050986868_0001_r_000000_0
   [druid] 2018-12-04 21:14:04,004 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:14:04,005 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:14:04,083 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@1a4df0ee
   [druid] 2018-12-04 21:14:04,085 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@54ca3542
   [druid] 2018-12-04 21:14:04,098 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:14:04,106 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1050986868_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:14:04,138 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1050986868_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:14:04,144 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local1050986868_0001_m_000000_0
   [druid] 2018-12-04 21:14:04,145 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:14:04,146 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:14:04,147 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:14:04,147 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:14:04,158 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:14:04,158 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:14:04,161 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:14:04,162 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:14:04,162 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:14:04,162 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:14:04,163 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:14:04,164 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:14:04,288 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1050986868_0001 running in uber mode : false
   [druid] 2018-12-04 21:14:04,289 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:14:04,444 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:14:04,557 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1050986868_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:14:04,558 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:14:04,558 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1050986868_0001_r_000000_0' done.
   [druid] 2018-12-04 21:14:04,558 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1050986868_0001_r_000000_0
   [druid] 2018-12-04 21:14:04,558 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:14:04,563 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:14:05,291 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:14:05,291 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1050986868_0001 completed successfully
   [druid] 2018-12-04 21:14:05,304 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=613600
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=5
		Total committed heap usage (bytes)=601358336
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:15:12,036 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:15:12,037 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:15:12,691 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:15:12,798 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:15:12,838 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:15:12,972 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1888207717_0001
   [druid] 2018-12-04 21:15:13,109 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:15:13,110 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1888207717_0001
   [druid] 2018-12-04 21:15:13,111 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:15:13,116 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:13,116 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:15:13,121 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:15:13,152 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:15:13,153 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1888207717_0001_m_000000_0
   [druid] 2018-12-04 21:15:13,175 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:13,180 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:15:13,261 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@5f2d6f66
   [druid] 2018-12-04 21:15:13,268 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:15:13,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:15:13,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:15:13,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:15:13,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:15:13,314 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:15:13,319 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:15:13,761 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,761 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,761 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,762 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,763 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:13,774 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:15:13,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:15:13,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:15:13,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:15:13,775 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:15:13,787 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:15:13,795 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1888207717_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:15:13,805 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:15:13,805 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1888207717_0001_m_000000_0' done.
   [druid] 2018-12-04 21:15:13,805 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1888207717_0001_m_000000_0
   [druid] 2018-12-04 21:15:13,806 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:15:13,807 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:15:13,808 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1888207717_0001_r_000000_0
   [druid] 2018-12-04 21:15:13,814 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:13,814 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:15:13,897 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@9dcec4a
   [druid] 2018-12-04 21:15:13,900 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@2e36578d
   [druid] 2018-12-04 21:15:13,916 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:15:13,918 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1888207717_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:15:13,946 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1888207717_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:15:13,952 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local1888207717_0001_m_000000_0
   [druid] 2018-12-04 21:15:13,954 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:15:13,955 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:15:13,956 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:15:13,956 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:15:13,966 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:15:13,966 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:15:13,969 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:15:13,970 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:15:13,971 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:15:13,971 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:15:13,972 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:15:13,973 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:15:14,112 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1888207717_0001 running in uber mode : false
   [druid] 2018-12-04 21:15:14,113 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:15:14,265 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:15:14,368 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1888207717_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:15:14,369 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:15:14,369 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1888207717_0001_r_000000_0' done.
   [druid] 2018-12-04 21:15:14,369 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1888207717_0001_r_000000_0
   [druid] 2018-12-04 21:15:14,369 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:15:14,374 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:15:15,115 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:15:15,115 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1888207717_0001 completed successfully
   [druid] 2018-12-04 21:15:15,127 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=613784
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=603979776
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:15:35,954 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:15:35,956 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:15:36,582 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:15:36,660 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:15:36,689 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:15:36,757 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1229798009_0001
   [druid] 2018-12-04 21:15:36,891 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:15:36,893 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1229798009_0001
   [druid] 2018-12-04 21:15:36,894 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:15:36,900 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:36,900 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:15:36,905 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:15:36,937 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:15:36,938 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1229798009_0001_m_000000_0
   [druid] 2018-12-04 21:15:36,959 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:36,962 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:15:37,039 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@611c21b3
   [druid] 2018-12-04 21:15:37,046 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:15:37,095 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:15:37,095 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:15:37,095 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:15:37,095 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:15:37,095 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:15:37,099 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:15:37,535 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,536 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,536 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,536 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,536 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,536 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,537 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,537 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,537 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:15:37,548 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:15:37,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:15:37,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:15:37,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:15:37,549 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:15:37,651 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:15:37,657 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1229798009_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:15:37,664 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:15:37,665 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1229798009_0001_m_000000_0' done.
   [druid] 2018-12-04 21:15:37,665 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1229798009_0001_m_000000_0
   [druid] 2018-12-04 21:15:37,665 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:15:37,666 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:15:37,667 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1229798009_0001_r_000000_0
   [druid] 2018-12-04 21:15:37,671 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:15:37,671 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:15:37,754 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@756865c6
   [druid] 2018-12-04 21:15:37,757 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@49df0690
   [druid] 2018-12-04 21:15:37,776 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:15:37,779 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1229798009_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:15:37,815 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1229798009_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:15:37,821 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local1229798009_0001_m_000000_0
   [druid] 2018-12-04 21:15:37,822 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:15:37,823 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:15:37,824 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:15:37,824 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:15:37,836 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:15:37,837 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:15:37,839 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:15:37,839 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:15:37,840 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:15:37,840 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:15:37,842 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:15:37,842 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:15:37,894 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1229798009_0001 running in uber mode : false
   [druid] 2018-12-04 21:15:37,896 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:15:38,136 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:15:38,236 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1229798009_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:15:38,237 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:15:38,237 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1229798009_0001_r_000000_0' done.
   [druid] 2018-12-04 21:15:38,237 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1229798009_0001_r_000000_0
   [druid] 2018-12-04 21:15:38,237 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:15:38,242 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:15:38,897 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:15:38,897 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1229798009_0001 completed successfully
   [druid] 2018-12-04 21:15:38,909 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=613784
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=600309760
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:17:15,242 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:17:15,243 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:17:15,837 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:17:15,912 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:17:15,944 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:17:16,013 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1755103053_0001
   [druid] 2018-12-04 21:17:16,146 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:17:16,146 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1755103053_0001
   [druid] 2018-12-04 21:17:16,147 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:17:16,152 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:16,152 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:17:16,159 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:17:16,189 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:17:16,191 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1755103053_0001_m_000000_0
   [druid] 2018-12-04 21:17:16,211 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:16,215 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:17:16,291 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16894b5c
   [druid] 2018-12-04 21:17:16,298 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:17:16,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:17:16,348 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:17:16,348 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:17:16,348 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:17:16,348 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:17:16,351 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:17:16,799 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,800 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:16,810 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:17:16,812 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:17:16,812 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:17:16,812 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:17:16,812 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:17:16,825 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:17:16,831 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1755103053_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:17:16,839 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:17:16,839 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1755103053_0001_m_000000_0' done.
   [druid] 2018-12-04 21:17:16,839 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1755103053_0001_m_000000_0
   [druid] 2018-12-04 21:17:16,839 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:17:16,841 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:17:16,841 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1755103053_0001_r_000000_0
   [druid] 2018-12-04 21:17:16,849 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:16,849 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:17:16,928 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@4473d0e1
   [druid] 2018-12-04 21:17:16,930 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@359c2777
   [druid] 2018-12-04 21:17:16,942 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:17:16,947 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1755103053_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:17:16,980 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1755103053_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:17:16,986 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local1755103053_0001_m_000000_0
   [druid] 2018-12-04 21:17:16,987 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:17:16,989 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:17:16,989 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:17:16,989 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:17:17,000 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:17:17,001 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:17:17,003 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:17:17,004 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:17:17,005 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:17:17,005 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:17:17,006 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:17:17,007 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:17:17,149 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1755103053_0001 running in uber mode : false
   [druid] 2018-12-04 21:17:17,150 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:17:17,315 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:17:17,443 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1755103053_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:17:17,443 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:17:17,443 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1755103053_0001_r_000000_0' done.
   [druid] 2018-12-04 21:17:17,443 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1755103053_0001_r_000000_0
   [druid] 2018-12-04 21:17:17,443 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:17:17,448 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:17:18,151 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:17:18,151 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1755103053_0001 completed successfully
   [druid] 2018-12-04 21:17:18,163 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=613216
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=597688320
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:17:51,548 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:17:51,549 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:17:52,188 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:17:52,262 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:17:52,294 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:17:52,364 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1475525783_0001
   [druid] 2018-12-04 21:17:52,494 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:17:52,495 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1475525783_0001
   [druid] 2018-12-04 21:17:52,496 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:17:52,500 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:52,501 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:17:52,505 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:17:52,538 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:17:52,540 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1475525783_0001_m_000000_0
   [druid] 2018-12-04 21:17:52,561 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:52,565 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:17:52,641 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@7cde7ebc
   [druid] 2018-12-04 21:17:52,648 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/28/part-m-00000:0+15042
   [druid] 2018-12-04 21:17:52,699 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:17:52,699 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:17:52,699 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:17:52,699 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:17:52,699 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:17:52,703 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:17:53,146 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,146 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,146 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,146 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,146 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,147 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,147 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,147 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,147 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:17:53,159 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:17:53,161 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:17:53,161 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:17:53,161 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 3252; bufvoid = 104857600
   [druid] 2018-12-04 21:17:53,161 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214304(104857216); length = 93/6553600
   [druid] 2018-12-04 21:17:53,173 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:17:53,180 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1475525783_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:17:53,187 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:17:53,187 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1475525783_0001_m_000000_0' done.
   [druid] 2018-12-04 21:17:53,187 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1475525783_0001_m_000000_0
   [druid] 2018-12-04 21:17:53,187 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:17:53,189 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:17:53,190 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1475525783_0001_r_000000_0
   [druid] 2018-12-04 21:17:53,197 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:17:53,197 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:17:53,278 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@56a76c3d
   [druid] 2018-12-04 21:17:53,280 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3c41bda3
   [druid] 2018-12-04 21:17:53,292 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:17:53,294 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1475525783_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:17:53,328 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1475525783_0001_m_000000_0 decomp: 3302 len: 3306 to MEMORY
   [druid] 2018-12-04 21:17:53,333 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 3302 bytes from map-output for attempt_local1475525783_0001_m_000000_0
   [druid] 2018-12-04 21:17:53,335 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 3302, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->3302
   [druid] 2018-12-04 21:17:53,336 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:17:53,336 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:17:53,336 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:17:53,346 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:17:53,346 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:17:53,348 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 3302 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:17:53,349 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 3306 bytes from disk
   [druid] 2018-12-04 21:17:53,350 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:17:53,350 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:17:53,351 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 3225 bytes
   [druid] 2018-12-04 21:17:53,351 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:17:53,497 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1475525783_0001 running in uber mode : false
   [druid] 2018-12-04 21:17:53,498 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:17:53,636 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:17:53,730 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1475525783_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:17:53,730 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:17:53,730 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1475525783_0001_r_000000_0' done.
   [druid] 2018-12-04 21:17:53,730 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1475525783_0001_r_000000_0
   [druid] 2018-12-04 21:17:53,730 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:17:53,735 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:17:54,500 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:17:54,500 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1475525783_0001 completed successfully
   [druid] 2018-12-04 21:17:54,511 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=6962
		FILE: Number of bytes written=613216
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=30084
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=43
		Map output records=24
		Map output bytes=3252
		Map output materialized bytes=3306
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=4
		Reduce shuffle bytes=3306
		Reduce input records=24
		Reduce output records=4
		Spilled Records=48
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=596115456
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=15042
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:20:36,256 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:20:36,257 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:20:36,885 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:20:36,959 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:20:36,990 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:20:37,061 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local1672177752_0001
   [druid] 2018-12-04 21:20:37,191 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:20:37,192 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local1672177752_0001
   [druid] 2018-12-04 21:20:37,193 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:20:37,198 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:20:37,198 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:20:37,204 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:20:37,239 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:20:37,240 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1672177752_0001_m_000000_0
   [druid] 2018-12-04 21:20:37,262 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:20:37,266 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:20:37,339 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@109a569c
   [druid] 2018-12-04 21:20:37,347 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/05/29/part-m-00000:0+91736
   [druid] 2018-12-04 21:20:37,397 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:20:37,397 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:20:37,397 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:20:37,397 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:20:37,398 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:20:37,401 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:20:37,853 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,853 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,853 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,853 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,854 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,854 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,854 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,854 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,854 [ask Executor #0] INFO  ewTotalUser.NewTotalUserMapper {1} - serverTime或者member不能为空
   [druid] 2018-12-04 21:20:37,883 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:20:37,885 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:20:37,885 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:20:37,885 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 8342; bufvoid = 104857600
   [druid] 2018-12-04 21:20:37,885 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26214152(104856608); length = 245/6553600
   [druid] 2018-12-04 21:20:37,897 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:20:37,903 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1672177752_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:20:37,911 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:20:37,911 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1672177752_0001_m_000000_0' done.
   [druid] 2018-12-04 21:20:37,911 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1672177752_0001_m_000000_0
   [druid] 2018-12-04 21:20:37,912 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:20:37,913 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:20:37,914 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local1672177752_0001_r_000000_0
   [druid] 2018-12-04 21:20:37,921 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:20:37,921 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:20:37,998 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@42fada95
   [druid] 2018-12-04 21:20:38,000 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@33084509
   [druid] 2018-12-04 21:20:38,016 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:20:38,017 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local1672177752_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:20:38,046 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local1672177752_0001_m_000000_0 decomp: 8468 len: 8472 to MEMORY
   [druid] 2018-12-04 21:20:38,051 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 8468 bytes from map-output for attempt_local1672177752_0001_m_000000_0
   [druid] 2018-12-04 21:20:38,053 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 8468, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->8468
   [druid] 2018-12-04 21:20:38,054 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:20:38,055 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:20:38,055 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:20:38,065 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:20:38,066 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-04 21:20:38,069 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 8468 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:20:38,070 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 8472 bytes from disk
   [druid] 2018-12-04 21:20:38,070 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:20:38,071 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:20:38,072 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 8391 bytes
   [druid] 2018-12-04 21:20:38,072 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:20:38,195 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1672177752_0001 running in uber mode : false
   [druid] 2018-12-04 21:20:38,196 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:20:38,349 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:20:38,523 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local1672177752_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:20:38,524 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:20:38,524 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local1672177752_0001_r_000000_0' done.
   [druid] 2018-12-04 21:20:38,524 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local1672177752_0001_r_000000_0
   [druid] 2018-12-04 21:20:38,524 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:20:38,530 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:20:39,198 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:20:39,198 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local1672177752_0001 completed successfully
   [druid] 2018-12-04 21:20:39,210 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=17296
		FILE: Number of bytes written=628716
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=183472
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=187
		Map output records=62
		Map output bytes=8342
		Map output materialized bytes=8472
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=6
		Reduce shuffle bytes=8472
		Reduce input records=62
		Reduce output records=6
		Spilled Records=124
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=3
		Total committed heap usage (bytes)=596639744
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=91736
	File Output Format Counters 
		Bytes Written=0
   [druid] 2018-12-04 21:34:03,252 [main           ] INFO  conf.Configuration.deprecation {1} - session.id is deprecated. Instead, use dfs.metrics.session-id
   [druid] 2018-12-04 21:34:03,253 [main           ] INFO  .hadoop.metrics.jvm.JvmMetrics {1} - Initializing JVM Metrics with processName=JobTracker, sessionId=
   [druid] 2018-12-04 21:34:03,935 [main           ] WARN  .mapreduce.JobResourceUploader {1} - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).
   [druid] 2018-12-04 21:34:04,038 [main           ] INFO  duce.lib.input.FileInputFormat {1} - Total input paths to process : 1
   [druid] 2018-12-04 21:34:04,076 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - number of splits:1
   [druid] 2018-12-04 21:34:04,169 [main           ] INFO  .hadoop.mapreduce.JobSubmitter {1} - Submitting tokens for job: job_local130705849_0001
   [druid] 2018-12-04 21:34:04,376 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - The url to track the job: http://localhost:8080/
   [druid] 2018-12-04 21:34:04,378 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Running job: job_local130705849_0001
   [druid] 2018-12-04 21:34:04,378 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter set in config null
   [druid] 2018-12-04 21:34:04,385 [Thread-3       ] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:34:04,385 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
   [druid] 2018-12-04 21:34:04,392 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in setupJob()
   [druid] 2018-12-04 21:34:04,430 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for map tasks
   [druid] 2018-12-04 21:34:04,431 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local130705849_0001_m_000000_0
   [druid] 2018-12-04 21:34:04,455 [ask Executor #0] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:34:04,459 [ask Executor #0] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:34:04,537 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@17f22e10
   [druid] 2018-12-04 21:34:04,544 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Processing split: hdfs://Murphy:8020/ods/11/11/part-m-00000:0+784323
   [druid] 2018-12-04 21:34:04,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - (EQUATOR) 0 kvi 26214396(104857584)
   [druid] 2018-12-04 21:34:04,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - mapreduce.task.io.sort.mb: 100
   [druid] 2018-12-04 21:34:04,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - soft limit at 83886080
   [druid] 2018-12-04 21:34:04,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufvoid = 104857600
   [druid] 2018-12-04 21:34:04,602 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396; length = 6553600
   [druid] 2018-12-04 21:34:04,606 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
   [druid] 2018-12-04 21:34:05,172 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - 
   [druid] 2018-12-04 21:34:05,175 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Starting flush of map output
   [druid] 2018-12-04 21:34:05,175 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Spilling map output
   [druid] 2018-12-04 21:34:05,175 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - bufstart = 0; bufend = 220115; bufvoid = 104857600
   [druid] 2018-12-04 21:34:05,175 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - kvstart = 26214396(104857584); kvend = 26206696(104826784); length = 7701/6553600
   [druid] 2018-12-04 21:34:05,210 [ask Executor #0] INFO  g.apache.hadoop.mapred.MapTask {1} - Finished spill 0
   [druid] 2018-12-04 21:34:05,219 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local130705849_0001_m_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:34:05,231 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - map
   [druid] 2018-12-04 21:34:05,231 [ask Executor #0] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local130705849_0001_m_000000_0' done.
   [druid] 2018-12-04 21:34:05,231 [ask Executor #0] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local130705849_0001_m_000000_0
   [druid] 2018-12-04 21:34:05,232 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - map task executor complete.
   [druid] 2018-12-04 21:34:05,234 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - Waiting for reduce tasks
   [druid] 2018-12-04 21:34:05,234 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Starting task: attempt_local130705849_0001_r_000000_0
   [druid] 2018-12-04 21:34:05,240 [pool-6-thread-1] INFO  lib.output.FileOutputCommitter {1} - File Output Committer Algorithm version is 1
   [druid] 2018-12-04 21:34:05,240 [pool-6-thread-1] INFO  rn.util.ProcfsBasedProcessTree {1} - ProcfsBasedProcessTree currently is supported only on Linux.
   [druid] 2018-12-04 21:34:05,315 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} -  Using ResourceCalculatorProcessTree : org.apache.hadoop.yarn.util.WindowsBasedProcessTree@16937d04
   [druid] 2018-12-04 21:34:05,317 [pool-6-thread-1] INFO  pache.hadoop.mapred.ReduceTask {1} - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@6f8bcec6
   [druid] 2018-12-04 21:34:05,328 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - MergerManager: memoryLimit=1503238528, maxSingleShuffleLimit=375809632, mergeThreshold=992137472, ioSortFactor=10, memToMemMergeOutputsThreshold=10
   [druid] 2018-12-04 21:34:05,330 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - attempt_local130705849_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
   [druid] 2018-12-04 21:34:05,362 [localfetcher#1 ] INFO  educe.task.reduce.LocalFetcher {1} - localfetcher#1 about to shuffle output of map attempt_local130705849_0001_m_000000_0 decomp: 223969 len: 223973 to MEMORY
   [druid] 2018-12-04 21:34:05,367 [localfetcher#1 ] INFO  .task.reduce.InMemoryMapOutput {1} - Read 223969 bytes from map-output for attempt_local130705849_0001_m_000000_0
   [druid] 2018-12-04 21:34:05,368 [localfetcher#1 ] INFO  e.task.reduce.MergeManagerImpl {1} - closeInMemoryFile -> map-output of size: 223969, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->223969
   [druid] 2018-12-04 21:34:05,369 [mpletion Events] INFO  educe.task.reduce.EventFetcher {1} - EventFetcher is interrupted.. Returning
   [druid] 2018-12-04 21:34:05,370 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:34:05,370 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
   [druid] 2018-12-04 21:34:05,380 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local130705849_0001 running in uber mode : false
   [druid] 2018-12-04 21:34:05,380 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:34:05,380 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 21:34:05,381 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 0%
   [druid] 2018-12-04 21:34:05,389 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merged 1 segments, 223969 bytes to disk to satisfy reduce memory limit
   [druid] 2018-12-04 21:34:05,390 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 1 files, 223973 bytes from disk
   [druid] 2018-12-04 21:34:05,391 [pool-6-thread-1] INFO  e.task.reduce.MergeManagerImpl {1} - Merging 0 segments, 0 bytes from memory into reduce
   [druid] 2018-12-04 21:34:05,391 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Merging 1 sorted segments
   [druid] 2018-12-04 21:34:05,392 [pool-6-thread-1] INFO  rg.apache.hadoop.mapred.Merger {1} - Down to the last merge-pass, with 1 segments left of total size: 223886 bytes
   [druid] 2018-12-04 21:34:05,392 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - 1 / 1 copied.
   [druid] 2018-12-04 21:34:05,664 [pool-6-thread-1] INFO  conf.Configuration.deprecation {1} - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
   [druid] 2018-12-04 21:34:05,973 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task:attempt_local130705849_0001_r_000000_0 is done. And is in the process of committing
   [druid] 2018-12-04 21:34:05,974 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce > reduce
   [druid] 2018-12-04 21:34:05,974 [pool-6-thread-1] INFO  org.apache.hadoop.mapred.Task  {1} - Task 'attempt_local130705849_0001_r_000000_0' done.
   [druid] 2018-12-04 21:34:05,974 [pool-6-thread-1] INFO  e.hadoop.mapred.LocalJobRunner {1} - Finishing task: attempt_local130705849_0001_r_000000_0
   [druid] 2018-12-04 21:34:05,974 [Thread-3       ] INFO  e.hadoop.mapred.LocalJobRunner {1} - reduce task executor complete.
   [druid] 2018-12-04 21:34:05,979 [Thread-3       ] WARN  lib.output.FileOutputCommitter {1} - Output Path is null in commitJob()
   [druid] 2018-12-04 21:34:06,382 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} -  map 100% reduce 100%
   [druid] 2018-12-04 21:34:06,382 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Job job_local130705849_0001 completed successfully
   [druid] 2018-12-04 21:34:06,395 [main           ] INFO  rg.apache.hadoop.mapreduce.Job {1} - Counters: 35
	File System Counters
		FILE: Number of bytes read=448298
		FILE: Number of bytes written=1272151
		FILE: Number of read operations=0
		FILE: Number of large read operations=0
		FILE: Number of write operations=0
		HDFS: Number of bytes read=1568646
		HDFS: Number of bytes written=0
		HDFS: Number of read operations=8
		HDFS: Number of large read operations=0
		HDFS: Number of write operations=0
	Map-Reduce Framework
		Map input records=1590
		Map output records=1926
		Map output bytes=220115
		Map output materialized bytes=223973
		Input split bytes=106
		Combine input records=0
		Combine output records=0
		Reduce input groups=13
		Reduce shuffle bytes=223973
		Reduce input records=1926
		Reduce output records=13
		Spilled Records=3852
		Shuffled Maps =1
		Failed Shuffles=0
		Merged Map outputs=1
		GC time elapsed (ms)=4
		Total committed heap usage (bytes)=632291328
	Shuffle Errors
		BAD_ID=0
		CONNECTION=0
		IO_ERROR=0
		WRONG_LENGTH=0
		WRONG_MAP=0
		WRONG_REDUCE=0
	File Input Format Counters 
		Bytes Read=784323
	File Output Format Counters 
		Bytes Written=0
   